---
title: "Submitting an array job with OSG"
date: last-modified
date-format: iso
published-title: "Last updated"
author: Nicholas Ducharme-Barth
engine: knitr
execute:
    eval: false
    echo: true
    output: false
format:
  html:
    embed-resources: true
    toc: true
    toc-location: right
    number-sections: true
    code-overflow: wrap
---

In this example we step through submitting an array job on OSG where we want to run the same job in a number of directories. In this case the job is running a simple [R script](../examples/osg/array_lm/inputs/run_lm_osg_array.r) that reads in the *data.csv* file stored in the directory, fits a linear model, and writes the parameter estimates to a *par.csv*. We specify which directories we want to run jobs in as a part of the job-array using a [text file](../examples/osg/array_lm/inputs/osg_job_directories.txt) to specify the directory path names on OSG.

The *osg/array_lm* example can be set-up either by cloning the repository `git clone https://github.com/MOshima-PIFSC/NSASS-HTC-HPC-Computing.git`, or stepping through the following code:

::: {.callout-important}
# Coding alert!
Note: throughout this tutorial we are using **User.Name** as a stand-in for your actual username and **NMFS/project_name** as a stand-in for your project. In all cases, replace **User.Name** your actual user name and **NMFS/project_name** with your specific project name. ##TODO: project name still needed? 
:::

## Setup data inputs and directories

1. Define a relative path, we are starting from the root directory of this project.

```{r}
#| eval: true
proj_dir = this.path::this.proj()
osg_project = "" ##TDOD Do we need this? 
```

2. Define directory names for each run.
```{r}
#| eval: true
dir_name = paste0("rep_0", 0:9)
```

3. Iterate across directories, create them, and then write a simple *.csv* file into them containing data to fit a linear model.
```{r}
for(i in seq_along(dir_name)){
    
    if(!file.exists(file.path(proj_dir, "example", "OSG", "array_lm", "inputs", dir_name[i], "data.csv")))
    {
        set.seed(i)
        dir.create(file.path(proj_dir, "examples", "OSG", "array_lm", "inputs", dir_name[i]), recursive=TRUE)
        tmp = data.frame(x=1:1000)
        tmp$y = (i + (0.5*i)*tmp$x) + rnorm(1000,0,i)
        write.csv(tmp, file = file.path(proj_dir, "examples", "OSG", "array_lm", "inputs", dir_name[i], "data.csv"))
    }
}
```

4. Write an R script to read in the data, run a linear model, and report back the estimated parameters.
```{r}
if(!file.exists(file.path(proj_dir, "examples", "OSG", "array_lm", "inputs", "run_lm_osg_array.r"))){
    script_lines = c("tmp=read.csv('data.csv')", 
    "fit=lm(y~x,data=tmp)", 
    "out = data.frame(par=unname(fit$coefficients))", 
    "write.csv(out,file='par.csv')"
    )
    writeLines(script_lines, con = file.path(proj_dir, "examples", "OSG", "array_lm", "inputs", "run_lm_osg_array.r"))
}
```

5. Write a text file containing the full path names for where the directories will be on OSG.
```{r}
if(!file.exists(file.path(proj_dir, "examples", "OSG", "array_lm", "inputs", "osg_job_directories.txt"))){
    dir_lines = paste0("array_osg/", dir_name, "/")
    writeLines(dir_lines, con = file.path(proj_dir, "examples", "OSG", "array_lm", "inputs", "osg_job_directories.txt"))
}
```

6. In addtion to the input files, you will need to have 2 additional scripts: a wrapper script (`wrapper.sh`) and a submission script (`submission.sub`), examples of both can be found in `examples/OSG/array_lm/scripts`. To easily upload all the necessary files at once, compress the entire `array_lm/` directory as a tar.gz file `upload.array_lm.tar.gz`. 
```{r}
system(paste0("powershell cd ", file.path(proj_dir, "examples", "OSG", "array_lm"), ";tar -czf upload.array_lm.tar.gz * "))
```

## OSG workflow  

1. Connect to OSG  
As [mentioned](osg_documentation.qmd#transferring-files-via-scp), access to OSG and file transfer is done using a pair of Terminal/PowerShell windows. In your first window, log onto your access point and create a directory for this example.

```{bash}
ssh User.Name@ap21.uc.osg-htc.org
mkdir array_lm
```

2. Transfer files  

We will upload the compressed file`upload.array_lm.tar.gz` into the OSG directory that you just created. The following files should be included:

* all replicate data files to run the linear models on; 
* the r script [run_lm_osg_array.r](/examples/OSG/array_lm/inputs/run_lm_osg_array.r); 
* the text file [osg_job_directories.txt](/examples/OSG/array_lm/inputs/osg_job_directories.txt) with the directory names;
* the wrapper script [wrapper.sh](/examples/OSG/array_lm/scripts/bash/wrapper.sh) which unpacks files, sets up job timing, executes the R script, and packages results;
* the submission script [submission.sub](/examples/OSG/array_lm/scripts/condor_submit/submission.sub); 
* and a bash script [prep.sh](/examples/OSG/array_lm/scripts/bash/prep.sh) that prepares the files to be run on HTCondor, including changing file permissions, making directory structures, and changing dos2unix line endings.
 
In the second terminal, navigate to the directory where the compressed file is on your local computer and run:  
```{bash}
scp upload.array_lm.tar.gz User.Name@ap21.uc.osg-htc.org:/home/User.Name/array_lm 
```

You will be prompted for your passphrase and RSA code before the file transfers. Once the file transfer is complete, go back to the first terminal and you can untar the files by navigating to the `array_lm` directory and running: 

```{bash}
tar - xvf upload.array_lm.tar.gz 
```

3. Prepare scripts  

Still in the first window, change the permissions and line endings for `osg_prep.sh`. Navigate to the `scripts/bash` directory and change the change the line endings for the `prep.sh` script and then execute it to prepare the other scripts as neccessary. 

```{bash}
# navigate to directory
cd scripts/bash
# change permissions to make the file executable 
chmod 777 prep.sh 
# change line endings 
dos2unix prep.sh 
# run script
./prep.sh 
```

4. Submit job  

Once you are ready, you can submit the job by running the command `condor_submit`. 
```{bash}
# navigate out of bash directory
cd ..
# submit job
condor_submit condor_submit/submission.sub
```

While your job is running, you can check on it using the following commands: 

* `condor_q` shows status of all of your jobs: running, idle, or held
* `condor_q -run` shows running jobs only
* `condor_q -hold` shows jobs that are held  

Using these commands you can get the job id number and peek directly at what is happening on the compute node using `condor_ssh_to_the_job <job_id>`. This is most useful for look at longer jobs or to see if intermediate files are being produced correctly.

For more information on tracking and restarting jobs, see [here](). ##TODO: add link, check that the documentation below through log files is included in ss example

Given the complexity of the OSG HTCondor network, there are inevitably going to jobs that have issues (can't complete input/output file transfer to compute nodes or the compute node restarting for a software update) causing them to be held up. There is nothing you can do to prevent this, though typically shorter jobs with smaller files may have less exposure to these issues. Releasing these jobs using `condor_release` restarts their execution giving them an opportunity to try again on different compute node. This usually clears up any issues.

Log files `*.log`, screen output files `*.out`, and error files `*.err` are written into each jobs directory. These will have the prefix `job_*` as specified in `r_and_ss.sub` and are useful for diagnosing any issues. However, once jobs have completed successfully there is no need to keep them and they should be deleted (to save disk space and bandwidth) prior to downloading the results. 

5. Download results  

Once the jobs have completed, you can retrieve the results for further analysis on your local computer. On your local compter create a new directory `outputs` to put all of the downloaded results. Then, in the second terminal window, run: 
```{bash}
scp -r User.Name@ap21.uc.osg-htc.org:/home/User.Name/array_lm/outputs /outputs 
```
Again you will be prompted for your passphrase and RSA code. 