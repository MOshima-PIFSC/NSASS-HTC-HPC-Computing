[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This repository contains examples and documentation for accessing and applying existing research computing resources available to NOAA Fisheries staff. Example code and documentation were presented as a part of the National Stock Assessment Seminar series (slides).\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "assets/web-slides.html",
    "href": "assets/web-slides.html",
    "title": "Seminar slides",
    "section": "",
    "text": "Seminar slides from the NOAA National Stock Assessment Science Seminar Series presentation.\n    View slides in full screen\n       \n      \n    \n  \n\n\n\n Back to top",
    "crumbs": [
      "Seminar slides"
    ]
  },
  {
    "objectID": "assets/web-about-mo.html",
    "href": "assets/web-about-mo.html",
    "title": "Megumi Oshima",
    "section": "",
    "text": "Megumi Oshima has been working at the Pacific Islands Fisheries Science Center since 2021. She works mostly on domestic and territorial bottomfish stocks and is interested in Openscience and creating reproducible and transparent workflows. Before joining PIFSC, she was a graduate student at University of Southern Mississippi where she got her PhD in Coastal Sciences.\n\n\n Back to top",
    "crumbs": [
      "Contact us",
      "Megumi Oshima"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html",
    "href": "assets/osg_documentation.html",
    "title": "Working with Open Science Grid",
    "section": "",
    "text": "The Open Science Grid is an example of an HTC computing environment where jobs are run and scheduled using HTCondor. OSG is free to use for US-based government staff conducting research or education related work, and HTCondor is well suited for running many short, non-sequential, single core jobs. Helpful documentation can be found on their website and the following information is based on this documentation.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your OSG user name.\nReplace osg.project_name your OSG project name.",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#open-science-grid",
    "href": "assets/osg_documentation.html#open-science-grid",
    "title": "Working with Open Science Grid",
    "section": "",
    "text": "The Open Science Grid is an example of an HTC computing environment where jobs are run and scheduled using HTCondor. OSG is free to use for US-based government staff conducting research or education related work, and HTCondor is well suited for running many short, non-sequential, single core jobs. Helpful documentation can be found on their website and the following information is based on this documentation.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your OSG user name.\nReplace osg.project_name your OSG project name.",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#connecting-to-osg-vis-ssh",
    "href": "assets/osg_documentation.html#connecting-to-osg-vis-ssh",
    "title": "Working with Open Science Grid",
    "section": "2 Connecting to OSG vis ssh",
    "text": "2 Connecting to OSG vis ssh\nAccess to OSG and file transfer is done using a pair of Terminal/PowerShell windows. In your first powershell terminal, log onto your access point. You will be prompted to enter your passphrase (password). It will then ask for a verification code generated by the Google Authenticator app.\n\nssh User.Name@ap21.uc.osg-htc.org\n\nIf you see the following, you have connected successfully:\n\n\n                *** Unauthorized use is prohibited. ***\n\n      If you log on to this computer system, you acknowledge your\n  awareness of and concurrence with the OSG Acceptable Use Policy; see\n             https://www.osgconnect.net/aup or /etc/osg/AUP\n\n                              ***\n\n              Cite the OSPool in your publications!\n                https://osg-htc.org/acknowledging\n\n                              ***\n\n               OSPool Office Hours are twice a week:\n    Tues 4-5:30pm ET/1-2:30pm PT, Thurs 11:30am-1pm ET/8:30-10am PT\n             Zoom link: https://osg-htc.org/OfficeHoursZoom\n\n***********************************************************************",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#transferring-files-via-scp",
    "href": "assets/osg_documentation.html#transferring-files-via-scp",
    "title": "Working with Open Science Grid",
    "section": "3 Transferring files via scp",
    "text": "3 Transferring files via scp\n\n3.1 Move a file to OSG\nIf you want to transfer a file (for example, my_file.txt) from your local machine to OSG, navigate to the directory of my_file.txt on your local computer. Open a terminal and use scp and specify the source file name and destination path as shown:\n\n\n## format is scp &lt;source&gt; &lt;destination&gt; \nscp my_file.txt User_Name@ap21.uc.osg-htc.org:/home/User_Name/\n\nNote, you do not need to login to OSG first, you will be prompted to enter your passphrase and then a verification code before the file transfers. And each time you transfer a file you will need to re-enter the information.\n\n\n3.2 Move multiple files at once\nTo transfer many files more efficiently, you can compress your files into tarballs (.tar.gz file). For example, to upload all of the input files for running an array of linear model jobs at once, in R, run:\n\nsystem(paste0(\"powershell cd \",file.path(\"examples\", \"osg\", \"array_lm\"),\";tar -czf inputs.tar.gz inputs\"))\n\nto create a tarball called inputs.tar.gz. To do this in a terminal, open a terminal window in the directory above the files to compress (e.g. examples/osg/array_lm) and run:\n\ntar -czf inputs.tar.gz inputs\n\n\n\n3.3 Move files back to local machine\nTo transfer files from OSG back to your local machine you can use the same command as above but swaping the source and destination paths. From the same terminal as before (or in the directory to put the file), use:\n\n\nscp User_Name@ap21.uc.osg-htc.org:/home/User_Name/my_file.txt ./\n\nwhere ./ is the current location on your local computer or you can specify the path relative to where you are (e.g. ./path/to/files).",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#transferring-large-files-to-osdf",
    "href": "assets/osg_documentation.html#transferring-large-files-to-osdf",
    "title": "Working with Open Science Grid",
    "section": "4 Transferring large files to OSDF",
    "text": "4 Transferring large files to OSDF\nFor large input and output files, including containers, it is recommended to use the Open Science Data Federation (OSDF). To see exactly which OSDF origins to use see this guidance on Where to Put Your Files. For this example, we will use the access point of ap21.uc.osg-htc.org. If you are working with a container, you can upload it to your files and transfer it using the following commands.\nFrom a terminal in the directory where the container files (linux.def and linux.sif) are stored on your local computer, run:\n\nscp linux.def User.Name@ap21.uc.osg-htc.org:/home/User.Name\nscp linux.sif User.Name@ap21.uc.osg-htc.org:/home/User.Name\n\nAgain you will be prompted for your passphrase and RSA code each time. Then, in a second terminal that is logged into OSG, move the container to the OSDF location.\n\nmv linux.sif /ospool/ap21/data/User.Name\n\nTo test the container, in that same OSG terminal, run:\n\n\napptainer shell /ospool/ap21/data/User.Name/linux.sif\n\n#once it opens you can try running R and making sure the packages you need are there\nR\nlibrary(r4ss)\nq()",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/presentation.html#what",
    "href": "assets/presentation.html#what",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "What?",
    "text": "What?\n\n\n\nResearch computing is the collection of computing, software, storage resources and services that allows for data analysis at scale.\n\n\n\n\nIn our particular case we are interested in leverging research computing to augment stock assessment worflows.\n\n\n\nRun more/bigger models in less time"
  },
  {
    "objectID": "assets/presentation.html#why",
    "href": "assets/presentation.html#why",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Why?",
    "text": "Why?\n\n\n\n\n\nImprove efficiency by running 10s - 1000s of models ‘simultaneously’.\n\n\n\n\n  \n\n2021 Southwest Pacific Ocean swordfish stock assessment\n\n\n\n\n9,300 model runs totalling ~46 months of computation time."
  },
  {
    "objectID": "assets/presentation.html#why-1",
    "href": "assets/presentation.html#why-1",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Why?",
    "text": "Why?\n\n\n\nEfficiency\n\n\n\n\n\n\nKnowledge acquisition\n\n\n\n\n\n\nAutomation, transparency, reproducibility & portability\n\n\n\n\n\n\nMulti-model inference\n\n\n\n\n\n\nSoftware containers\n\n\n\n\n\n\n\n\nBetter science"
  },
  {
    "objectID": "assets/presentation.html#how-1",
    "href": "assets/presentation.html#how-1",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "How?",
    "text": "How?\n\n\nHigh-throughput computing (HTC)\n\nHigh-performance computing (HPC)\n\n \n\nPhoto credit: NOAA\n\n\nOpenScienceGrid (OSG): OSPool\n\n\nNOAA Hera"
  },
  {
    "objectID": "assets/presentation.html#how-2",
    "href": "assets/presentation.html#how-2",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "How?",
    "text": "How?\n\n\nOpenScienceGrid (OSG)\n\n\n\nUses HTCondor distributed computing network (no shared file system between compute nodes) to implement HTC workflows\n\n\n\n\nFree to use for US based researchers affiliated with academic/government organization and using OSG for research/education efforts\n\n\n\n\nShould not be used to analyze protected data\n\n\n\n\nNOAA Hera\n\n\n\nUses Slurm to schedule HPC (or HTC) workflows\n\n\n\n\nShared file system between compute nodes\n\n\n\n\nNOAA resource so no restrictions on acceptable use/analyzing protected data if working on mission related tasks\n\n\n\n\nAllocation determines access\n\n\n\n\n\n\nBoth use software containers"
  },
  {
    "objectID": "assets/presentation.html#software-containers",
    "href": "assets/presentation.html#software-containers",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Software containers",
    "text": "Software containers\n\n\nMany may already be using containers such as GitHub Codespaces or Posit Workbench in existing cloud-based workflows\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplication: set up identical, custom software environments on OSG and Hera\n\n\n\n\nApplication: use to “version” analyses by “freezing” packages/libraries"
  },
  {
    "objectID": "assets/presentation.html#software-containers-1",
    "href": "assets/presentation.html#software-containers-1",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Software containers",
    "text": "Software containers\n\n\nApptainer\n\n\nSecure, portable and reproducible software container for Linux operating systems\n\n\n\n\nEasy to use\n\n\n\n\nDoesn’t require root privileges to build\n\n\n\n\nPlays nice with existing containers (e.g., Docker)"
  },
  {
    "objectID": "assets/presentation.html#apptainer",
    "href": "assets/presentation.html#apptainer",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Apptainer",
    "text": "Apptainer\nLet’s look at an example (linux-r4ss-v4.def):\n\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    TZ=Etc/UTC && \\\n    ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && \\\n    echo $TZ &gt; /etc/timezone\n    apt update -y\n    apt install -y \\\n        tzdata \\\n        curl \\\n        dos2unix\n\n    apt-get update -y\n    apt-get install -y \\\n            build-essential \\\n            cmake \\\n            g++ \\\n            libssl-dev \\\n            libssh2-1-dev \\\n            libcurl4-openssl-dev \\\n            libfontconfig1-dev \\\n            libxml2-dev \\\n            libgit2-dev \\\n            wget \\\n            tar \\\n            coreutils \\\n            gzip \\\n            findutils \\\n            sed \\\n            gdebi-core \\\n            locales \\\n            nano\n    \n    locale-gen en_US.UTF-8\n\n    export R_VERSION=4.4.0\n    curl -O https://cdn.rstudio.com/r/ubuntu-2004/pkgs/r-${R_VERSION}_1_amd64.deb\n    gdebi -n r-${R_VERSION}_1_amd64.deb\n\n    ln -s /opt/R/${R_VERSION}/bin/R /usr/local/bin/R\n    ln -s /opt/R/${R_VERSION}/bin/Rscript /usr/local/bin/Rscript\n\n    R -e \"install.packages('remotes', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"install.packages('data.table', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"install.packages('magrittr', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"install.packages('mvtnorm', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"remotes::install_github('r4ss/r4ss')\"\n    R -e \"remotes::install_github('PIFSCstockassessments/ss3diags')\"\n\n    NOW=`date`\n    echo 'export build_date=$NOW' &gt;&gt; $SINGULARITY_ENVIRONMENT\n\n    mkdir -p /ss_exe\n    curl -L -o /ss_exe/ss3_linux https://github.com/nmfs-ost/ss3-source-code/releases/download/v3.30.22.1/ss3_linux\n    chmod 755 /ss_exe/ss3_linux\n\n%environment\n    export PATH=/ss_exe:$PATH\n    \n%labels\n    Author nicholas.ducharme-barth@noaa.gov\n    Version v0.0.4\n\n%help\n    This is a Linux (Ubuntu 20.04) container containing Stock Synthesis (version 3.30.22.1), R (version 4.4.0) and the R packages r4ss, ss3diags, data.table, magrittr, and mvtnorm."
  },
  {
    "objectID": "assets/presentation.html#apptainer-1",
    "href": "assets/presentation.html#apptainer-1",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Apptainer",
    "text": "Apptainer\nLet’s look at an example (linux-r4ss-v4.def):\nBuild on Linux system with Apptainer installed.\n\napptainer build linux-r4ss-v4.sif linux-r4ss-v4.def"
  },
  {
    "objectID": "assets/presentation.html#lets-walk-through-an-example",
    "href": "assets/presentation.html#lets-walk-through-an-example",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Let’s walk through an example",
    "text": "Let’s walk through an example\n\nIn this case we will use Hera to conduct a quick retrospective analysis on all models in the SS3 testing suite.\n\n\nMore complete documentation of this example can be found on the GitHub website."
  },
  {
    "objectID": "assets/presentation.html#workflow",
    "href": "assets/presentation.html#workflow",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Workflow",
    "text": "Workflow\n\nCreate container\n\n\n\nCreate scripts\n\n\n\n\nTransfer files\n\n\n\n\nSubmit jobs\n\n\n\n\nTransfer files back to local machine"
  },
  {
    "objectID": "assets/presentation.html#workflow-1",
    "href": "assets/presentation.html#workflow-1",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogin node\n\n\n\nCompute node: 1\n\n\n\nCompute node: 2\n\n\n\nCompute node: 3\n\n\n\nShared storage\n\n\n\nIDE\n\n\n\nTerminal A: ssh\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE: Develop and make job script files\n\n\n\n\n\n\nNational Stock Assessment Science Seminar"
  },
  {
    "objectID": "assets/ss3-hera.html",
    "href": "assets/ss3-hera.html",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "",
    "text": "Similar to the array_lm example, this example also sets up running an array job on Hera. As before, we will use a *.txt to indicate which directories we want to run jobs in as a part of our array.\nThere are a few main differences that serve to illustrate useful modifications to the workflow:\nThe hera/ss3 example can be set-up either by cloning the repository git clone https://github.com/MOshima-PIFSC/NSASS-HTC-HPC-Computing.git, or stepping through the following code:",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#build-software-container",
    "href": "assets/ss3-hera.html#build-software-container",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "1 Build software container",
    "text": "1 Build software container\nSoftware containers allow for portable, reproducible research by allowing researchers to set-up a software environment to their exact spefications and being able to run it on any Linux system. The Apptainer container system is widely used across HPC/HTC systems, and make it easy to build a container from a definition file. Running a job within a container means that you are able to replicate an identical software environment in any location with Apptainer installed, no matter the native operating system, software and installed packages. The Apptainer container can be built from any Linux machine with Apptainer installed, including the Open Science Grid (OSG) access points. Here we walk through the steps needed to build a Linux (Ubuntu 20.04) container containing Stock Synthesis (version 3.30.22.1), R (version 4.4.0) and the R packages r4ss, ss3diags, data.table, magrittr, and mvtnorm from a definition file, linux-r4ss-v4.def. In this case we will show the steps needed to build the container using the OSG access point as our Linux virtual machine (VM), though many may not be needed if working from an alternative Linux VM.\n\n\n\n\n\n\nCoding alert!\n\n\n\nNote: you will have to change apXX to match your OSG access point (e.g., ap20 or ap21).\n\n\nThe first step is to log onto your OSG access point via ssh using a Terminal/PowerShell window and make a directory to build your container in this case singularity1.\n\nssh User.Name@apXX.uc.osg-htc.org\nmkdir -p singularity/linux_r4ss\n\nUsing a second Terminal/PowerShell window, navigate to the directory that you cloned the NSASS-HTC-HPC-Computing repo into and upload the definition file (linux-r4ss-v4.def) to the directory you just created on OSG.\n\nscp apptainer/linux-r4ss-v4.def User.Name@apXX.uc.osg-htc.org:/home/User.Name/singularity/linux_r4ss\n\nBack in your first Terminal/PowerShell window manoeuvre into the directory, and build the container2. The second line of code is what builds the Singularity Image File (.sif) and takes two arguments: the name of the output .sif file and the input definition file (.def).\n\ncd singularity/linux_r4ss\napptainer build linux-r4ss-v4.sif linux-r4ss-v4.def\n\nUsing the second Terminal/PowerShell window, download the Singularity Image File (.sif) so that it can be uploaded for use on the NOAA Hera HPC system.\n\nscp User.Name@apXX.uc.osg-htc.org:/home/User.Name/singularity/linux_r4ss/linux-r4ss-v4.sif apptainer/",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#setup-data-inputs-and-directories",
    "href": "assets/ss3-hera.html#setup-data-inputs-and-directories",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "2 Setup data inputs and directories",
    "text": "2 Setup data inputs and directories\nGiven that our example is to run a 4-year retrospective analysis for each of the SS3 test models, the next step is downloading the SS3 test models from the nmfs-stock-synthesis/test-models Github repo. Once you’ve downloaded the test models, copy the models/ directory into a new example directory ss3/inputs/ within the NSASS-HTC-HPC-Computing/examples/hera/ directory on your machine. If you cloned the NSASS-HTC-HPC-Computing repo, the SS3 test models will already be in the correct location.\nFor the sake of example the job array will be set-up to run each retrospective peel (e.g., -0 years, -1 year, … , -4 years of data) as individual jobs in the job array. This is more efficient in a true HTC environment such as OSG however on Hera it could make more sense to bundle the initial model run and subsequent retrospective peels as a single job. We will store the results of each retrospective peel in its own directory. The directories on Hera will be listed in a text file, and we will use this text file to launch jobs on Hera (as a part of the job array) in each of the named directories.\nLet us define that text file using R.\n\nDefine a relative path, we are starting from the root directory of this project.\n\n\n\nShow code used to define relative paths.\nproj_dir = this.path::this.proj()\nhera_project = \"NMFS/project_name/User.Name/\"\n\n\n\nWrite a text file containing the full path names for where the directories will be on Hera.\n\n\n\nShow code used to define job directory structure.\ntest_models=list.dirs(paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\"),recursive=FALSE,full.names=FALSE)\nretro_peels=0:4\n\n# replace '-' with '_' in model names since we will use '-' as a delimiter\n    if(length(grep(\"-\",test_models,fixed=TRUE))&gt;0){\n        test_models_new = gsub(\"-\",\"_\",test_models)\n        rename_models_idx = grep(\"-\",test_models,fixed=TRUE)\n        for(i in seq_along(rename_models_idx)){\n            # create new dir\n            dir.create(paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models_new[rename_models_idx[i]]),recursive=TRUE)\n            # copy files\n            file.copy(paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models[rename_models_idx[i]],\"/\",list.files(paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models[rename_models_idx[i]]),full.names=FALSE,recursive=FALSE)),paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models_new[rename_models_idx[i]]))\n            # delete old dir\n            # file.remove(paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models[rename_models_idx[i]],\"/\"))\n            shell(paste0(\"powershell rm -r \",proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models[rename_models_idx[i]],\"/\"))\n        }\n        test_models = test_models_new\n    }\n    \n\n# define scenarios\nscenario_df = expand.grid(model=test_models,peel=retro_peels)\nscenario_df$run_id = 1:nrow(scenario_df)\nscenario_df = scenario_df[,c(3,1,2)]\nscenario_df$run_id = ifelse(scenario_df$run_id&lt;10,paste0(0,scenario_df$run_id),as.character(scenario_df$run_id))\n\n# write text file\nhera_dir_lines = paste0(\"/scratch1/\", hera_project, \"examples/ss3/output/\", apply(scenario_df,1,paste0,collapse=\"-\"), \"/\")\nwriteLines(hera_dir_lines, con=paste0(proj_dir, \"/examples/hera/ss3/inputs/hera_job_directories.txt\"))",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#prepare-job-scripts",
    "href": "assets/ss3-hera.html#prepare-job-scripts",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "3 Prepare job scripts",
    "text": "3 Prepare job scripts\nDuring benchmark testing, issues were identified when trying to apply an HTC workflow to Hera, and a seperate workflow was developed which may be more Hera/Slurm appropriate. This takes advantage of the gnu parallel utility to run batches of jobs on distinct compute nodes. In this particular example 90 models will be run across 3 nodes each using 30 CPUs, and we will set the maximum run time to 1 hour. This reserves the entire node for computations thus reducing the competition for resources for any one job3. In order to execute this workflow, instructions are coordinated using four nested scripts:\n\nparallel-submit.sh: This script prepares files for Slurm job execution, makes the directory structure specified by hera_job_directories.txt, specifies the job requirements and submits the parallel jobs.\nparallel-job-exec.sh: This is a script that defines variables to be passed to the software container and a second bash script wrapper-r.sh.\nwrapper-r.sh: This wrapper script controls file input/output to and from the R script ss3-example-calcs.r, executes the R script, conducts job timing and tidies up the job working directory.\nss3-example-calcs.r: This is the actual computation script which modifies the SS3 input files as needed, executes the appropriate SS3 model run and conducts any needed post-processing of the output within R.\n\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn parallel-submit.sh you will need to change the following before you upload and run the script:\n\nLine 20-22: change account project_name to the name of your project. If you are using the NOAA htc4sa project, it would be htc4sa.\n\n\n\n\nFrom within R, compress the ss3/inputs/ and ss3/slurm_scripts/ directories as a tar.gz file upload.example-ss3.tar.gz. This simplifies the number of steps needed for file transfers.\n\n\nshell(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"hera\", \"ss3\"), \";tar -czf upload.example-ss3.tar.gz inputs/ slurm_scripts/\"))",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#hera-workflow",
    "href": "assets/ss3-hera.html#hera-workflow",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "4 Hera workflow",
    "text": "4 Hera workflow\n\nConnect to Hera\n\nOpen a PowerShell terminal and connect to Hera. This terminal will be your remote workstation, call it Terminal A. You will be prompted for your RSA passcode, which is your password followed by the 8-digit code from the authenticator app.\n\nssh -m hmac-sha2-256-etm@openssh.com User.Name@hera-rsa.boulder.rdhpcs.noaa.gov -p22\n\n\nCreate directories\n\nIn Terminal A navigate to the project directory on scratch1 and create some directories. If using a shared directory such as htc4sa/, make a directory to save your work within this directory (.e.g., User.Name/). Change your working directory to this directory, and make a directory for the current project examples/ss3/4.\n\n# navigate to project directory\ncd /scratch1/NMFS/project_name/\n# create new directory\nmkdir User.Name/\n# navigate into new directory\ncd User.Name/\n# create directory for SLURM scripts and logs\nmkdir -p examples/ss3/\n\n\nTransfer files\n\nOpen a second PowerShell terminal in the NSASS-HTC-HPC-Computing directory on your machine. This will be your local workstation, call it Terminal B. Use this terminal window to upload via scp the needed files (examples/hera/ss3/upload.example-ss3.tar.gz and apptainer/linux-r4ss-v4.sif) to Hera. The upload.example-ss3.tar.gz will be uploaded to your directory within the project directory on scratch1. Make sure your VPN is active when attempting to upload using the DTN. You will be prompted for your RSA passcode after each scp command. Note that you will need to specify the MAC protocol needed for the scp file transfer similar to what was done for the initial ssh connection using scp -o MACs=hmac-sha2-256-etm@openssh.com.\n\nscp -o MACs=hmac-sha2-256-etm@openssh.com examples/hera/ss3/upload.example-ss3.tar.gz User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/examples/ss3/\nscp -o MACs=hmac-sha2-256-etm@openssh.com apptainer/linux-r4ss-v4.sif User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/examples/ss3/\n\n\nPrepare files and submit job on Hera\n\nIn Terminal A, un-tar upload.example-ss3.tar.gz, change the permissions/line endings for slurm_scripts/parallel-submit.sh and execute the script.\n\ntar -xzf upload.example-ss3.tar.gz\nchmod 777 slurm_scripts/parallel-submit.sh\ndos2unix slurm_scripts/parallel-submit.sh\n./slurm_scripts/parallel-submit.sh\n\nAfter job submission you can check on job status using squeue -u $USER or you can use the following for more detailed information.\n\n# count the number of output files (End.tar.gz) that have been produced\nfind . -type f -name End.tar.gz -exec echo . \\; | wc -l\n# list the size and location of all of the End.tar.gz files\nfind . -type f -name End.tar.gz -exec du -ch {} +\n\n\nDownload jobs and clean-up workspace\n\nOnce all jobs are completed (or the job has hit its time limit), use your Terminal B to download your jobs.\n\nscp -o MACs=hmac-sha2-256-etm@openssh.com -r User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/examples/ss3/output/ examples/hera/ss3/\n\nLastly in Terminal A, clean-up the /scratch1/NMFS/project_name/User.Name/ directory since it is a shared space.\n\n\n\n\n\n\nWarning!\n\n\n\nMake sure that you have verified that your jobs completed successfully and that all results have been downloaded before cleaning-up the directory.\n\n\n\n# move back up a level in the directory structure\ncd ..\n# delete the ss3/ directory\nrm -r ss3/",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#process-results",
    "href": "assets/ss3-hera.html#process-results",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "5 Process results",
    "text": "5 Process results\nAfter results are downloaded they can be processed in R to extract the model run times, time series of estimated biomass for each model run, and Mohn’s rho across retrospective peels for a given model ‘family’.\n\n\nShow output processing code\n# iterate over output files and extract quantities\nlibrary(data.table)\nlibrary(magrittr)\nlibrary(r4ss)\n\noutput_dirs = list.dirs(paste0(proj_dir,\"/examples/hera/ss3/output/\"),recursive=FALSE,full.names=FALSE)\nssb_dt.list = comptime_dt.list = as.list(rep(NA,length(output_dirs)))\nss_output_list =  as.list(rep(NA,length(output_dirs)))\nnames(ss_output_list) = output_dirs\n\nfor(i in seq_along(output_dirs)){\n    tmp_model = strsplit(output_dirs[i],\"-\")[[1]][2]\n    tmp_peel = as.numeric(strsplit(output_dirs[i],\"-\")[[1]][3])\n    tmp_index = as.numeric(strsplit(output_dirs[i],\"-\")[[1]][1])\n\n    # check if the End.tar.gz file got created\n    if(file.exists(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/End.tar.gz\")))\n    {\n        # get snapshot of original files in the directory\n        tmp_orig_files = list.files(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/\"))\n\n        # un-tar if the End.tar.gz file gets made\n        shell(paste0(\"powershell cd \", paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/\"), \";tar -xzf End.tar.gz\"))\n\n        # check if runtime.txt was produced and extract output\n        if(file.exists(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/runtime.txt\"))){\n            tmp_time = readLines(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/runtime.txt\")) %&gt;%\n            gsub(\".*?([0-9]+).*\", \"\\\\1\", .) %&gt;%\n            as.numeric(.) %&gt;%\n            as.data.table(.) %&gt;%\n            setnames(.,\".\",\"time\")\n            comptime_dt.list[[i]] = data.table(id = output_dirs[i])    \n            comptime_dt.list[[i]]$index = tmp_index\n            comptime_dt.list[[i]]$model = tmp_model\n            comptime_dt.list[[i]]$peel = tmp_peel\n            comptime_dt.list[[i]]$hera_start = as.POSIXct(tmp_time$time[1],origin=\"1970-01-01\")\n            comptime_dt.list[[i]]$hera_end = as.POSIXct(tmp_time$time[2],origin=\"1970-01-01\")\n            comptime_dt.list[[i]]$hera_runtime = tmp_time$time[3]/60\n\n            # clean-up\n            rm(list=c(\"tmp_time\"))\n        }\n\n        # if \"ss_report.RData\" is produced put it into the storage list\n        if(file.exists(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/ss_report.RData\"))){\n            load(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/ss_report.RData\"))\n            ss_output_list[[i]] = ss_report\n\n            ssb_dt.list[[i]] = ss_report$derived_quants %&gt;%\n                as.data.table(.) %&gt;%\n                .[Label %in% paste0(\"SSB_\", ss_report$startyr:ss_report$endyr)] %&gt;%\n                .[,id := output_dirs[i]] %&gt;%\n                .[,sbo:=Value/subset(ss_report$derived_quants,Label==\"SSB_Virgin\")$Value] %&gt;%\n                .[,yr:=sapply(Label,function(x)as.numeric(strsplit(x,\"_\")[[1]][2]))] %&gt;%\n                .[,.(id,yr,sbo)]\n            # clean-up\n                rm(list=c(\"ss_report\"))\n        }    \n\n        # clean-up\n        file.remove(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/\",setdiff(list.files(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/\")),tmp_orig_files)))\n        rm(list=c(\"tmp_orig_files\"))\n    } else {\n        comptime_dt.list[[i]] = data.table(id=output_dirs[i],index=tmp_index,model=tmp_model,peel=tmp_peel,hera_start=NA,hera_end=NA,hera_runtime=NA)\n        ssb_dt.list[[i]] = data.table(id=output_dirs[i],yr=2023,sbo=NA)\n    }\n\n    # clean-up\n    rm(list=c(\"tmp_model\",\"tmp_peel\",\"tmp_index\"))\n}\n\ncomptime_dt = rbindlist(na.omit(comptime_dt.list))\nssb_dt = rbindlist(ssb_dt.list) %&gt;% merge(comptime_dt[,.(id,index,model,peel)],.,by=\"id\")\nss_output_list = na.omit(ss_output_list)\n\n# adjust times to account for the fact that model 79 did not finish within the 1 hour allocation\ncomptime_dt$hera_start[79] = comptime_dt$hera_start[80]\ncomptime_dt$hera_end[79] = comptime_dt$hera_start[79] + 60^2\ncomptime_dt$hera_runtime[79] = 60\n\n# save\nfwrite(comptime_dt,file=paste0(proj_dir,\"/examples/hera/ss3/output/comptime_dt.csv\"))\nfwrite(ssb_dt,file=paste0(proj_dir,\"/examples/hera/ss3/output/ssb_dt.csv\"))\n\n# calculate Mohn's rho\nunique_models = unique(comptime_dt$model)\nretro_dt.list = as.list(rep(NA,length(unique_models)))\n\nfor(i in seq_along(unique_models)){\n    tmp_model = unique_models[i]\n\n    retro_dt.list[[i]] = data.table(model=tmp_model)\n    retro_dt.list[[i]]$type = c(\"SBO\")\n    retro_dt.list[[i]]$rho = NA\n\n    if(uniqueN(na.omit(ssb_dt[model==tmp_model])$peel)==5){\n        tmp_dt = ssb_dt[model==tmp_model]\n        base_dt = tmp_dt[peel==0]\n        year_vec = max(base_dt$yr) - 1:4\n        bias_vec = rep(NA,length(year_vec))\n        # calc Mohn's rho for runs where all models completed\n        for(j in 1:4){\n            bias_vec[j] = (ssb_dt[model==tmp_model&peel==j&yr==year_vec[j]]$sbo - base_dt[yr==year_vec[j]]$sbo)/base_dt[yr==year_vec[j]]$sbo\n        }\n        retro_dt.list[[i]]$rho = mean(bias_vec)\n        rm(list=c(\"tmp_dt\",\"base_dt\",\"year_vec\",\"bias_vec\"))\n    } \n    \n    rm(list=c(\"tmp_model\"))\n}\n\nretro_dt = rbindlist(retro_dt.list)\nfwrite(retro_dt,file=paste0(proj_dir,\"/examples/hera/ss3/output/retro_dt.csv\"))\n\n\n\n5.1 Job runtime\nThe 90 jobs run on Hera completed 4.15 hours of calculations (2.77 minutes per job) in an elapsed time of 1 hour.\nExcluding the job that timed out at the 1-hour limit the 89 jobs run on Hera completed 3.15 hours of calculations (2.12 minutes per job) in an elapsed time of 14.48 minutes or \\(\\sim\\) 13 times faster (Figure 1).\n\n\nShow plotting code\nlibrary(ggplot2)\n\ncomptime_dt_minus %&gt;%\n.[,.(id,hera_start,hera_end)] %&gt;%\nmelt(.,id.vars=\"id\") %&gt;%\n.[,variable:=ifelse(variable%in%c(\"hera_start\"),\"start\",\"end\")] %&gt;%\ndcast(.,id~variable) %&gt;%\n.[order(start)] %&gt;%\nggplot() +\nxlab(\"Time (GMT)\") +\nylab(\"Job\") +\ngeom_segment(aes(x=start,xend=end,y=id,yend=id),color=\"#003087\",alpha=0.5,linewidth=2) +\ntheme(panel.background = element_rect(fill = \"transparent\", color = \"black\", linetype = \"solid\"),\n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),  \n            strip.background =element_rect(fill=\"transparent\"),\n            legend.key = element_rect(fill = \"transparent\"),\n            axis.text.y=element_blank(),\n            axis.ticks.y=element_blank())\n\n\n\n\n\n\n\n\nFigure 1: Start and stop time for jobs run on Hera, excluding the job that timed out.",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#example-results",
    "href": "assets/ss3-hera.html#example-results",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "6 Example results",
    "text": "6 Example results\n\n6.1 Retrospectives\nRetrospective plots of static biomass depletion for the SS3 test models are shown in Figure 2.\n\n\nShow plotting code\ntext_dt.list = as.list(rep(NA,uniqueN(ssb_dt$model)))\nfor(i in seq_along(text_dt.list)){\n    tmp_dt = ssb_dt[model==unique(ssb_dt$model)[i]]\n    tmp_min_yr = min(tmp_dt$yr)\n    text_dt.list[[i]] = data.table(model=unique(ssb_dt$model)[i],yr=tmp_min_yr,sbo=0.2,rho=round(retro_dt[model==unique(ssb_dt$model)[i]]$rho,digits=2))\n}\ntext_dt = rbindlist(text_dt.list)\n\n\nssb_dt %&gt;%\n            ggplot() +\n            facet_wrap(~model,scales=\"free_x\") +\n            xlab(\"Year\") +\n            ylab(expression(SB/SB[0])) +\n            ylim(0,NA) +\n            geom_hline(yintercept=0) +\n            geom_path(aes(x=yr,y=sbo,color=as.character(peel),group=id)) +\n            geom_text(data=text_dt,aes(x=yr,y=sbo,label=rho),size=3,hjust = 0) +\n            viridis::scale_color_viridis(\"Peel\",begin = 0.1,end = 0.8,direction = 1,option = \"H\",discrete=TRUE) +\n            viridis::scale_fill_viridis(\"Peel\",begin = 0.1,end = 0.8,direction = 1,option = \"H\",discrete=TRUE) +\n            theme(panel.background = element_rect(fill = \"transparent\", color = \"black\", linetype = \"solid\"),\n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),\n            strip.background =element_rect(fill=\"transparent\"),\n            legend.key = element_rect(fill = \"transparent\"))\n\n\n\n\n\n\n\n\nFigure 2: Depletion estimates across retrospective peels from the Stock Synthesis testing model suite examples. Mohn’s rho values are printed in each panel.",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#footnotes",
    "href": "assets/ss3-hera.html#footnotes",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis directory can be named anything that you like, in this case singularity is a legacy name from an earlier version of the code written before Singularity changed its name to Apptainer.↩︎\nThis may take ~10-15 minutes depending on how long it takes to install R packages.↩︎\nWhile this workflow leads to better scheduling and makes HTC applications possible on Hera it may not be computationally efficient if users do not make use of all CPUs on a given node. For example, given that an entire compute node is requested, the user’s allocation on Hera will be billed for use of all CPUs on that node even if not all are in use.↩︎\nNote that this path should match the path defined in hera_job_directories.txt.↩︎",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html",
    "href": "assets/hera_documentation.html",
    "title": "Working with NOAA HPC Hera",
    "section": "",
    "text": "Hera is one of NOAA’s high performance computing resources available to NMFS scientists where jobs are run and scheduled with SLURM. In order to access the system, you must first have a RDHPCS user account and request access to a project. For all of the examples in this documentation, we will be working in the htc4sa project. For complete documentation about Hera and other RDHPCS resources, see the NOAA RDHPCS website.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your RDHPCS user name.\nReplace bastion with either boulder or princeton depending on which bastion you wish to login in through.\nReplace project_name with your RDHPCS project name.",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html#hera",
    "href": "assets/hera_documentation.html#hera",
    "title": "Working with NOAA HPC Hera",
    "section": "",
    "text": "Hera is one of NOAA’s high performance computing resources available to NMFS scientists where jobs are run and scheduled with SLURM. In order to access the system, you must first have a RDHPCS user account and request access to a project. For all of the examples in this documentation, we will be working in the htc4sa project. For complete documentation about Hera and other RDHPCS resources, see the NOAA RDHPCS website.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your RDHPCS user name.\nReplace bastion with either boulder or princeton depending on which bastion you wish to login in through.\nReplace project_name with your RDHPCS project name.",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html#connecting-to-hera-via-ssh",
    "href": "assets/hera_documentation.html#connecting-to-hera-via-ssh",
    "title": "Working with NOAA HPC Hera",
    "section": "2 Connecting to Hera via ssh",
    "text": "2 Connecting to Hera via ssh\nOpen a terminal window (.e.g, command prompt or PowerShell), then open an ssh tunnel to Hera by using the following command:\n\nssh -m hmac-sha2-256-etm@openssh.com User.Name@hera-rsa.bastion.rdhpcs.noaa.gov -p22\n\nNote that the flag -p22 specifies opening port 22 and is optional. When prompted, put in your password followed by the RSA authentication code:\n\nXXXXXXXXRSACODE\n\nIf you see the following then you have connected successfully:\n\n________________________________________________________\n|                                                        |\n|                                                        |\n|  Welcome to the Hera High Performance Computing system |\n|                                                        |\n|        This system is located in Fairmont, WV          |\n|                                                        |\n|          Please Submit Helpdesk Requests to:           |\n|               rdhpcs.hera.help@noaa.gov                |\n|                                                        |\n|________________________________________________________|\n\nNote that you will also see the following terminal output upon connecting:\n\nLocal port XXXXX forwarded to remote host.\nRemote port YYYYY forwarded to local host.\n\nWhere XXXXX and YYYYY are your 4-5 digit port forwarding numbers. Make note of what your local port number (XXXXX) is since this will be used for scp file transfer using an ssh tunnel.",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html#transferring-files-tofrom-hera",
    "href": "assets/hera_documentation.html#transferring-files-tofrom-hera",
    "title": "Working with NOAA HPC Hera",
    "section": "3 Transferring files to/from Hera",
    "text": "3 Transferring files to/from Hera\n\n3.1 via scp using data transfer node (DTN)\nInformation in this section is largely based on this wiki (CAC login required). File transfer using a DTN is only possible from machines within the noaa.gov domain (VPN ok), and can only transfer files to the scratch directory. However, this is ok since it is recommended that input/output data files are stored in the scratch directory. Using a DTN is faster than using the ssh tunnel.\n\n\n\n\n\n\nNote\n\n\n\nThe scratch1/NMFS/project_name/ directory is a shared space (shared access and shared disk space) for all users of a project. Rather than dumping files into the root project directory (e.g., project_name/) it is better to place them in your own sub-directory. Since this sub-directory structure does not already exist you will need to log into Hera and create it:\n\nmkdir scratch1/NMFS/project_name/User.Name/\n\nThis will avoid over-writing other users’ work.\n\n\nFile transfer takes place via scp and you need to specify the source file (and path to the file if your terminal is not in that directory) and destination path as shown:\n\n## format is scp &lt;source&gt; &lt;destination&gt; \nscp /path/to/local/file User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/\n\nIf trying to transfer from a machine not within the NOAA domain (or on the VPN) you can use scp with an untrusted DTN:\n\nscp /path/to/local/file User.Name@udtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/data_untrusted/User.Name/\n\nNote that dtn-hera was changed to udtn-hera. Since files cannot stay in the data_untrusted directory we will need to log in to Hera and move it to the correct project directory on scratch. Once logged in, moving the files can be done with rsync (CAC login required), and then they can be deleted from data_untrusted by:\n\nrsync -axv /scratch1/data_untrusted/User.Name/file /scratch1/NMFS/project_name/User.Name/\nrm /scratch1/data_untrusted/User.Name/file\n\nFiles can be downloaded back to your local machine using scp from either the trusted (dtn) or untrusted (udtn) DTN:\n\nscp User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/file /path/to/local/file \n\n\n\n3.2 via scp using an ssh tunnel\nInformation in this section is largely based on this wiki (CAC login required). File transfer using an ssh tunnel is possible from all locations. Using the ssh tunnel for file transfer requires a two-step process with two active terminal windows (we suggest using PowerShell):\n\nIn the first terminal window, open an ssh connection to Hera with port forwarding:\n\n\nssh -m hmac-sha2-256-etm@openssh.com -LXXXXX:localhost:XXXXX User.Name@hera-rsa.bastion.rdhpcs.noaa.gov\n\nReplace XXXXX with your local port number.\n\nIn the second terminal window you can check to see if the tunnel was properly created:\n\n\nssh -p XXXXX User.Name@localhost\n\nif you get prompted for your password then success! Press CONTROL+C to ignore this prompt. Staying within this 2nd terminal window use scp to transfer your file:\n\nscp -P XXXXX /path/to/local/file User.Name@localhost:/home/User.Name/\n\nThis will copy your file from your local machine into your home directory on Hera. Simply append additional directory structure to /home/User.Name/ to copy it into a sub-directory that you have created on Hera (e.g., /home/User.Name/sub/dir/). To transfer files into your scratch directory, specify the proper destination path (e.g., /scratch1/NMFS/project_name/User.Name/).\nTo download a file from Hera using scp and the ssh tunnel use the following:\n\nscp -P XXXXX User.Name@localhost:/home/User.Name/file_name /path/to/local/file",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/array_osg.html",
    "href": "assets/array_osg.html",
    "title": "Submitting an array job with OSG",
    "section": "",
    "text": "In this example we step through submitting an array job on OSG where we want to run the same job in a number of directories. In this case the job is running a simple R script that reads in the data.csv file stored in the directory, fits a linear model, and writes the paramter estimates to a par.csv. We specify which directories we want to run jobs in as a part of the job-array using a text file to specify the directory path names on OSG.\nThe osg/array_lm example can be set-up either by cloning the repository git clone https://github.com/MOshima-PIFSC/NSASS-HTC-HPC-Computing.git, or stepping through the following code:",
    "crumbs": [
      "Documentation",
      "Submitting an array job with OSG"
    ]
  },
  {
    "objectID": "assets/array_osg.html#setup-data-inputs-and-directories",
    "href": "assets/array_osg.html#setup-data-inputs-and-directories",
    "title": "Submitting an array job with OSG",
    "section": "1 Setup data inputs and directories",
    "text": "1 Setup data inputs and directories\n\nDefine a relative path, we are starting from the root directory of this project.\n\n\nproj_dir = this.path::this.proj()\nosg_project = \"\" ##TDOD Do we need this? \n\n\nDefine directory names for each run.\n\n\ndir_name = paste0(\"rep_0\", 0:9)\n\n\nIterate across directories, create them, and then write a simple .csv file into them containing data to fit a linear model.\n\n\nfor(i in seq_along(dir_name)){\n    \n    if(!file.exists(file.path(proj_dir, \"example\", \"OSG\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\")))\n    {\n        set.seed(i)\n        dir.create(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", dir_name[i]), recursive=TRUE)\n        tmp = data.frame(x=1:1000)\n        tmp$y = (i + (0.5*i)*tmp$x) + rnorm(1000,0,i)\n        write.csv(tmp, file = file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\"))\n    }\n}\n\n\nWrite an R script to read in the data, run a linear model, and report back the estimated parameters.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"run_lm_osg_array.r\"))){\n    script_lines = c(\"tmp=read.csv('data.csv')\", \n    \"fit=lm(y~x,data=tmp)\", \n    \"out = data.frame(par=unname(fit$coefficients))\", \n    \"write.csv(out,file='par.csv')\"\n    )\n    writeLines(script_lines, con = file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"run_lm_osg_array.r\"))\n}\n\n\nWrite a text file containing the full path names for where the directories will be on OSG.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"osg_job_directories.txt\"))){\n    dir_lines = paste0(\"array_osg/\", dir_name, \"/\")\n    writeLines(dir_lines, con = file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"osg_job_directories.txt\"))\n}\n\n\nIn addtion to the input files, you will need to have 2 additional scripts: a wrapper script (wrapper.sh) and a submission script (submission.sub). To easily upload all the necessary files at once, compress the entire array_lm/ directory as a tar.gz file upload.array_lm.tar.gz.\n\n\nshell(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"OSG\"), \";tar -czf upload.array_lm.tar.gz array_lm \"))",
    "crumbs": [
      "Documentation",
      "Submitting an array job with OSG"
    ]
  },
  {
    "objectID": "assets/array_osg.html#osg-workflow",
    "href": "assets/array_osg.html#osg-workflow",
    "title": "Submitting an array job with OSG",
    "section": "2 OSG workflow",
    "text": "2 OSG workflow\nAs before, access to OSG and file transfer is done using a pair of Terminal/PowerShell windows. In your first window, log onto your access point and create directory for this example.\n\nssh nicholas.ducharmebarth@ap21.uc.osg-htc.org\nmkdir r_and_ss\n\nIn the second window, upload the files contained in example/r_and_ss/osg/ into the OSG directory that you just created. These files include:\n\nthe base SS files to be modified (as a compressed file) Start.tar.gz;\nthe text file osg_dir.txt that was created in the above R code chunk;\nthe R script r_and_ss.r for the job (manipulating SS input files, running SS, and bootstrapping the results to generate uncertainty around management quantities);\nthe HTCondor submit script r_and_ss.sub;\nthe bash wrapper script osg_wrapper_r_and_ss.sh executing the condor job (unpacks files, sets up job timing, executes R script, and packages results);\nand a bash script osg_prep.sh preparing the before-listed files for being run on HTCondor (change file permissions, make directory structure, and change dos2unix line endings).\n\n\nscp -r example/r_and_ss/osg/* nicholas.ducharmebarth@ap21.uc.osg-htc.org:/home/nicholas.ducharmebarth/r_and_ss\n\nIn the first window, change the permissions and line endings for osg_prep.sh. Since the OSG access point does not have the dos2unix utility installed we can quickly jump into the Singularity container that we created to access dos2unix, change the line endings and execute the script. Use condor_submit to submit your HTCondor job.",
    "crumbs": [
      "Documentation",
      "Submitting an array job with OSG"
    ]
  },
  {
    "objectID": "assets/web-about-nd.html",
    "href": "assets/web-about-nd.html",
    "title": "Nicholas Ducharme-Barth",
    "section": "",
    "text": "Nicholas Ducharme-Barth joined the Pacific Islands Fisheries Science Center in 2021. Previously, Nicholas worked at the Pacific Community (SPC) conducting pelagic stock assessments for the Western and Central Pacific Fisheries Commission (WCPFC). He received his B.S. in Mathematics from the College of William & Mary and his Ph. D. in Fisheries and Aquatic Sciences from the University of Florida.\n\n\n Back to top",
    "crumbs": [
      "Contact us",
      "Nicholas Ducharme-Barth"
    ]
  },
  {
    "objectID": "assets/array_hera.html",
    "href": "assets/array_hera.html",
    "title": "Submitting an array job with Hera",
    "section": "",
    "text": "In this example we step through submitting an array job on Hera where we want to run the same job in a number of directories. In this case the job is running a simple R script that reads in the data.csv file stored in the directory, fits a linear model, and writes the paramter estimates to a par.csv. We specify which directories we want to run jobs in as a part of the job-array using a text file to specify the directory path names on Hera.\nThe hera/array_lm example can be set-up either by cloning the repository git clone https://github.com/MOshima-PIFSC/NSASS-HTC-HPC-Computing.git, or stepping through the following code:",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  },
  {
    "objectID": "assets/array_hera.html#setup-data-inputs-and-directories",
    "href": "assets/array_hera.html#setup-data-inputs-and-directories",
    "title": "Submitting an array job with Hera",
    "section": "1 Setup data inputs and directories",
    "text": "1 Setup data inputs and directories\n\nDefine a relative path, we are starting from the root directory of this project.\n\n\nproj_dir = this.path::this.proj()\nhera_project = \"NMFS/project_name/User.Name/\"\n\n\nDefine directory names for each run.\n\n\ndir_name = paste0(\"rep_0\", 0:9)\n\n\nIterate across directories, create them, and then write a simple .csv file into them containing data to fit a linear model.\n\n\nfor(i in seq_along(dir_name)){\n    \n    if(!file.exists(file.path(proj_dir, \"example\", \"hera\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\")))\n    {\n        set.seed(i)\n        dir.create(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", dir_name[i]), recursive=TRUE)\n        tmp = data.frame(x=1:1000)\n        tmp$y = (i + (0.5*i)*tmp$x) + rnorm(1000,0,i)\n        write.csv(tmp, file = file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\"))\n    }\n}\n\n\nWrite an R script to read in the data, run a linear model, and report back the estimated parameters.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"run_lm_hera_array.r\"))){\n    script_lines = c(\"tmp=read.csv('data.csv')\", \n    \"fit=lm(y~x,data=tmp)\", \n    \"out = data.frame(par=unname(fit$coefficients))\", \n    \"write.csv(out,file='par.csv')\"\n    )\n    writeLines(script_lines, con = file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"run_lm_hera_array.r\"))\n}\n\n\nWrite a text file containing the full path names for where the directories will be on Hera.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"hera_job_directories.txt\"))){\n    dir_lines = paste0(\"/scratch1/\", hera_project, \"array_hera/\", dir_name, \"/\")\n    writeLines(dir_lines, con = file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"hera_job_directories.txt\"))\n}\n\n\nCompress the entire array_lm/ directory as a tar.gz file upload.array_lm.tar.gz. This simplifies the number of steps needed for file transfers.\n\n\nshell(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"hera\"), \";tar -czf upload.array_lm.tar.gz array_lm \"))",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  },
  {
    "objectID": "assets/array_hera.html#hera-workflow",
    "href": "assets/array_hera.html#hera-workflow",
    "title": "Submitting an array job with Hera",
    "section": "2 Hera workflow",
    "text": "2 Hera workflow\n\nConnect to Hera\n\nOpen a PowerShell terminal and connect to Hera. This terminal will be your remote workstation, call it Terminal A. You will be prompted for your RSA passcode, which is your password followed by the 8-digit code from the authenticator app.\n\nssh -m hmac-sha2-256-etm@openssh.com User.Name@hera-rsa.boulder.rdhpcs.noaa.gov -p22\n\n\nCreate directories\n\nIn Terminal A navigate to the project directory on scratch1 and create some directories. If using a shared directory such as htc4sa/, make a directory to save your work within this directory (.e.g., User.Name/). Change your working directory to this directory. We will upload our SLURM submit script into submit_scripts/ and write our SLURM log files to logs/.\n\n# navigate to project directory\ncd /scratch1/NMFS/project_name/\n# create new directory\nmkdir User.Name/\n# navigate into new directory\ncd User.Name/\n# create directory for SLURM scripts and logs\nmkdir submit_scripts/\nmkdir logs/\n\n\nTransfer files\n\nOpen a second PowerShell terminal in the NSASS-HTC-HPC-Computing directory on your machine. This will be your local workstation, call it Terminal B. Use this terminal window to upload via scp the needed files (examples/hera/upload.array_lm.tar.gz and examples/hera/array_lm/slurm_scripts/submit_array_lm.sh) to Hera. The upload.array_lm.tar.gz will be uploaded to your directory within the project directory on scratch1 and the submit script submit_array_lm.sh will be uploaded to the submit_scripts directory. Make sure your VPN is active when attempting to upload using the DTN. You will be prompted for your RSA passcode after each scp command.\n\nscp examples/hera/upload.array_lm.tar.gz User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name\n\n\n\n\n\n\n\nTroubleshooting Tip\n\n\n\nIf you are getting an error Corrupted MAC on input when uploading files, add -o MACs=hmac-sha2-512 between scp and the name of the file to upload.\n\n\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn submit_array_lm.sh you will need to change the following before you upload and run the script:\n\nLine 5: change account project_name to the name of your project. If you are using the NOAA htc4sa project, it would be htc4sa.\nLine 9: /scratch1/NMFS/project_name/User.Name/logs/ to the location you created for logs/ above.\nLine 10: Change to your email address.\nLine 24: Make sure that this line points to the location on Hera that you uploaded hera_job_directories.txt to. hera_job_directories.txt is located in the array_lm/inputs directory that was uploaded as a part of upload.array_lm.tar.gz.\nLine 37: Make sure that this line points to the location on Hera that you uploaded run_lm_hera_array.r to. run_lm_hera_array.r is located in the array_lm/inputs directory that was uploaded as a part of upload.array_lm.tar.gz.\n\n\n\n\nscp examples/hera/array_lm/slurm_scripts/submit_array_lm.sh User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/submit_scripts/\n\n\nUn-tar files\n\nBack in Terminal A untar the uploaded directory.\n\ntar -xzf upload.array_lm.tar.gz\n\n\nPrep files to be read on Hera\n\nMake sure files that will be read/executed have unix line endings:\n\ndos2unix inputs/run_lm_hera_array.r inputs/hera_job_directories.txt slurm_scripts/submit_array_lm.sh\n\n\nSubmit job\n\nNow you are ready to submit the SLURM submission script submit_array_lm.sh.\n\nsbatch slurm_scripts/submit_array_lm.sh\n\nAs part of this job script, it cleans the directories specified in hera_job_directories.txt of the input file data.csv. The output is a compressed tar.gz containing the output produced by run_lm_hera_array.r (par.csv) and runtime.txt which logs the job start, end, and runtime. You can check your job status using squeue but this job should complete in a few seconds.\n\nsqueue -u User.Name\n\n\nDownload results\n\nBefore sending back the results you need to compress array_lm.\n\ntar -czf download.array_lm.tar.gz array_lm\n\nMoving back to Terminal B you can download the results, but first you create a directory for it to be downloaded into. This can be done in R:\n\ndir.create(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\"), recursive=TRUE, showWarnings = FALSE)\n\nNow you can use scp in Terminal B to download download.array_lm.tar.gz into examples/hera/array_lm/output\n\nscp User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/download.array_lm.tar.gz examples/hera/array_lm/output/\n\nMove into examples/hera/array_lm/output/ and untar downloaded results.\n\n# navigate to output directory\ncd examples/hera/array_lm/output/\n# untar results\ntar -xzf download.array_lm.tar.gz",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  },
  {
    "objectID": "assets/array_hera.html#process-the-output",
    "href": "assets/array_hera.html#process-the-output",
    "title": "Submitting an array job with Hera",
    "section": "3 Process the output",
    "text": "3 Process the output\nIn R, iterate through the sub-directories of the input and output data to extract the results of the linear model fits, and the model run time information.\n\n\nShow code\nlibrary(data.table)\nlibrary(magrittr)\n\ninput_data.list = as.list(rep(NA,10))\noutput_data.list = as.list(rep(NA,10))\nruntime_data.list = as.list(rep(NA,10))\n\n##TODO: change paths here to match directory names\nfor(i in seq_along(dir_name)){\n    # get input data\n        input_data.list[[i]] = fread(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", dir_name[i],\"data.csv\")) %&gt;%\n            .[,.(x,y)] %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            .[,.(model,x,y)]\n    \n    # untar results\n        system(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\", dir_name[i],\"/\"), \";tar -xzf output.tar.gz\"))\n\n    # get output\n        output_data.list[[i]] = fread(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\", dir_name[i],\"par.csv\")) %&gt;%\n            .[,.(par)] %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            .[,.(model,par)] %&gt;%\n            melt(.,id.vars=\"model\") %&gt;%\n            .[,variable:=c(\"intercept\",\"slope\")] %&gt;%\n            dcast(.,model ~ variable) %&gt;%\n            merge(.,input_data.list[[i]][,.(model,x)],by=\"model\") %&gt;%\n            .[,pred_y := intercept+slope*x] %&gt;%\n            .[,.(model,x,pred_y)]\n    # get time\n        runtime_data.list[[i]] = readLines(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\", dir_name[i],\"runtime.txt\")) %&gt;%\n            gsub(\".*?([0-9]+).*\", \"\\\\1\", .) %&gt;%\n            as.numeric(.) %&gt;%\n            as.data.table(.) %&gt;%\n            setnames(.,\".\",\"time\") %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            melt(.,id.vars=\"model\") %&gt;%\n            .[,variable:=c(\"start\",\"end\",\"runtime\")] %&gt;%\n            dcast(.,model ~ variable) %&gt;%\n            .[,.(model,start,end,runtime)]\n}\n\ninput_data = rbindlist(input_data.list)\noutput_data = rbindlist(output_data.list)\nruntime_data = rbindlist(runtime_data.list)\n\n\nThe jobs started execution at 2023-05-31 00:49:56 and all finished by 2023-05-31 00:49:57 for an elapsed runtime of 1 seconds and a total computation time of 5 seconds. Use of Hera resulted in a job completing 5\\(\\times\\) faster. Figure 1 shows the simulated data and estimated linear fits for each model run in the job-array.\n\n\nShow code\nlibrary(ggplot2)\ninput_data %&gt;%\nggplot() +\ngeom_point(aes(x=x,y=y,fill=model),alpha=0.05,size=5,shape=21) +\ngeom_line(data=output_data,aes(x=x,y=pred_y,color=model),linewidth=2)\n\n\n\n\n\n\n\n\nFigure 1: Linear model fits from the 10 models run.",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  }
]