[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This repository contains examples and documentation for accessing and applying existing research computing resources available to NOAA Fisheries staff. Example code and documentation were presented as a part of the National Stock Assessment Seminar series (slides). A recording of the seminar can be found on the NOAA Library YouTube channel (here).\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "assets/array_hera.html",
    "href": "assets/array_hera.html",
    "title": "Submitting an array job with Hera",
    "section": "",
    "text": "In this example we step through submitting an array job on Hera where we want to run the same job in a number of directories. In this case the job is running a simple R script that reads in the data.csv file stored in the directory, fits a linear model, and writes the parameter estimates to a par.csv. We specify which directories we want to run jobs in as a part of the job-array using a text file to specify the directory path names on Hera.\nThe hera/array_lm example can be set-up either by cloning the repository git clone https://github.com/MOshima-PIFSC/NSASS-HTC-HPC-Computing.git, or stepping through the following code:",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  },
  {
    "objectID": "assets/array_hera.html#setup-data-inputs-and-directories",
    "href": "assets/array_hera.html#setup-data-inputs-and-directories",
    "title": "Submitting an array job with Hera",
    "section": "1 Setup data inputs and directories",
    "text": "1 Setup data inputs and directories\n\nDefine a relative path, we are starting from the root directory of this project.\n\n\nproj_dir = this.path::this.proj()\nhera_project = \"NMFS/project_name/User.Name/\"\n\n\nDefine directory names for each run.\n\n\ndir_name = paste0(\"rep_0\", 0:9)\n\n\nIterate across directories, create them, and then write a simple .csv file into them containing data to fit a linear model.\n\n\nfor(i in seq_along(dir_name)){\n    \n    if(!file.exists(file.path(proj_dir, \"example\", \"hera\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\")))\n    {\n        set.seed(i)\n        dir.create(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", dir_name[i]), recursive=TRUE)\n        tmp = data.frame(x=1:1000)\n        tmp$y = (i + (0.5*i)*tmp$x) + rnorm(1000,0,i)\n        write.csv(tmp, file = file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\"))\n    }\n}\n\n\nWrite an R script to read in the data, run a linear model, and report back the estimated parameters.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"run_lm_hera_array.r\"))){\n    script_lines = c(\"tmp=read.csv('data.csv')\", \n    \"fit=lm(y~x,data=tmp)\", \n    \"out = data.frame(par=unname(fit$coefficients))\", \n    \"write.csv(out,file='par.csv')\"\n    )\n    writeLines(script_lines, con = file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"run_lm_hera_array.r\"))\n}\n\n\nWrite a text file containing the full path names for where the directories will be on Hera.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"hera_job_directories.txt\"))){\n    dir_lines = paste0(\"/scratch1/\", hera_project, \"array_lm/inputs/\", dir_name, \"/\")\n    writeLines(dir_lines, con = file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"hera_job_directories.txt\"))\n}\n\n\nCompress files and prepare for transfer\nCompress the array_lm/inputs directory as a tar.gz file upload.array_lm.tar.gz. This simplifies the number of steps needed for file transfers.\n\nWe will also need to transfer the submission script, but before that, you will need to specify a few things in the script.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn submit_array_lm.sh you will need to change the following before you upload and run the script:\n\nLine 5: change account project_name to the name of your project. If you are using the NOAA htc4sa project, it would be htc4sa.\nLine 9: /scratch1/NMFS/project_name/User.Name/logs/ to the location you created for logs/ above.\nLine 10: Change to your email address.\nLine 24: Make sure that this line points to the location on Hera that you uploaded hera_job_directories.txt to. hera_job_directories.txt is located in the array_lm/inputs directory that was uploaded as a part of upload.array_lm.tar.gz.\nLine 37: Make sure that this line points to the location on Hera that you uploaded run_lm_hera_array.r to. run_lm_hera_array.r is located in the array_lm/inputs directory that was uploaded as a part of upload.array_lm.tar.gz.\n\n\n\n\nsystem(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"hera\", \"array_lm\"), \";tar -czf upload.array_lm.tar.gz inputs \"))",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  },
  {
    "objectID": "assets/array_hera.html#hera-workflow",
    "href": "assets/array_hera.html#hera-workflow",
    "title": "Submitting an array job with Hera",
    "section": "2 Hera workflow",
    "text": "2 Hera workflow\n\nConnect to Hera\n\nOpen a PowerShell terminal and connect to Hera. This terminal will be your remote workstation, call it Terminal A. You will be prompted for your RSA passcode, which is your password followed by the 8-digit code from the authenticator app.\n\nssh -m hmac-sha2-256-etm@openssh.com User.Name@hera-rsa.boulder.rdhpcs.noaa.gov -p22\n\n\nCreate directories\n\nIn Terminal A navigate to the project directory on scratch1 and create some directories. If using a shared directory such as htc4sa/, make a directory to save your work within this directory (.e.g., User.Name/). Change your working directory to this directory. We will upload our SLURM submit script into submit_scripts/ and write our SLURM log files to logs/.\n\n# navigate to project directory\ncd /scratch1/NMFS/project_name/\n# create new directory\nmkdir User.Name/\n# navigate into new directory\ncd User.Name/\n# create directory for SLURM scripts and logs\nmkdir submit_scripts/\nmkdir logs/\n\n\nTransfer files\n\nOpen a second PowerShell terminal in the NSASS-HTC-HPC-Computing directory on your machine. This will be your local workstation, call it Terminal B. Use this terminal window to upload via scp the needed files (examples/hera/array_lm/upload.array_lm.tar.gz and examples/hera/array_lm/slurm_scripts/submit_array_lm.sh) to Hera. The upload.array_lm.tar.gz will be uploaded to your directory within the project directory on scratch1 and the submit script submit_array_lm.sh will be uploaded to the submit_scripts directory. Make sure your VPN is active when attempting to upload using the DTN. You will be prompted for your RSA passcode after each scp command.\n\n# upload inputs\nscp -o MACs=hmac-sha2-512 examples/hera/array_lm/upload.array_lm.tar.gz User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name\n\n# upload submission script \nscp -o MACs=hmac-sha2-512 examples/hera/array_lm/slurm_scripts/submit_array_lm.sh User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/submit_scripts\n\n\n\n\n\n\n\nTroubleshooting Tip\n\n\n\nIf you are getting an error Corrupted MAC on input when uploading files, add -o MACs=hmac-sha2-512 between scp and the name of the file to upload.\n\n\n\nUn-tar files\n\nBack in Terminal A untar the inputs directory.\n\n# untar files\ntar -xzf upload.array_lm.tar.gz\n\n\nPrep files to be read on Hera\n\nMake sure files that will be read/executed have unix line endings:\n\ndos2unix inputs/run_lm_hera_array.r inputs/hera_job_directories.txt submit_scripts/submit_array_lm.sh\n\n\nSubmit job\n\nNow you are ready to submit the SLURM submission script submit_array_lm.sh.\n\nsbatch submit_scripts/submit_array_lm.sh\n\nAs part of this job script, it cleans the directories specified in hera_job_directories.txt of the input file data.csv. The output is a compressed tar.gz containing the output produced by run_lm_hera_array.r (par.csv) and runtime.txt which logs the job start, end, and runtime. You can check your job status using squeue but this job should complete in a few seconds.\n\nsqueue -u User.Name\n\n\nDownload results\n\nBefore sending back the results you need to compress all of the outputs (stored in the inputs directory).\n\ncd inputs\ntar -czf download.array_lm.tar.gz ./*\n\nMoving back to Terminal B you can download the results, but first you create a directory for it to be downloaded into. This can be done in R:\n\ndir.create(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\"), recursive=TRUE, showWarnings = FALSE)\n\nNow you can use scp in Terminal B to download download.array_lm.tar.gz into examples/hera/array_lm/output\n\nscp -o MACs=hmac-sha2-512 User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/inputs/download.array_lm.tar.gz examples/hera/array_lm/output/\n\nMove into examples/hera/array_lm/output/ and untar downloaded results.\n\n# navigate to output directory\ncd examples/hera/array_lm/output/\n# untar results\ntar -xzf download.array_lm.tar.gz",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  },
  {
    "objectID": "assets/array_hera.html#process-the-output",
    "href": "assets/array_hera.html#process-the-output",
    "title": "Submitting an array job with Hera",
    "section": "3 Process the output",
    "text": "3 Process the output\nIn R, iterate through the sub-directories of the input and output data to extract the results of the linear model fits, and the model run time information.\n\n\nShow code\nlibrary(data.table)\nlibrary(magrittr)\n\ninput_data.list = as.list(rep(NA,10))\noutput_data.list = as.list(rep(NA,10))\nruntime_data.list = as.list(rep(NA,10))\n\n##TODO: change paths here to match directory names\nfor(i in seq_along(dir_name)){\n    # get input data\n        input_data.list[[i]] = fread(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", dir_name[i],\"data.csv\")) %&gt;%\n            .[,.(x,y)] %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            .[,.(model,x,y)]\n    \n    # untar results\n        system(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\", dir_name[i],\"/\"), \";tar -xzf output.tar.gz\"))\n\n    # get output\n        output_data.list[[i]] = fread(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\", dir_name[i],\"par.csv\")) %&gt;%\n            .[,.(par)] %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            .[,.(model,par)] %&gt;%\n            melt(.,id.vars=\"model\") %&gt;%\n            .[,variable:=c(\"intercept\",\"slope\")] %&gt;%\n            dcast(.,model ~ variable) %&gt;%\n            merge(.,input_data.list[[i]][,.(model,x)],by=\"model\") %&gt;%\n            .[,pred_y := intercept+slope*x] %&gt;%\n            .[,.(model,x,pred_y)]\n    # get time\n        runtime_data.list[[i]] = readLines(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\", dir_name[i],\"runtime.txt\")) %&gt;%\n            gsub(\".*?([0-9]+).*\", \"\\\\1\", .) %&gt;%\n            as.numeric(.) %&gt;%\n            as.data.table(.) %&gt;%\n            setnames(.,\".\",\"time\") %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            melt(.,id.vars=\"model\") %&gt;%\n            .[,variable:=c(\"start\",\"end\",\"runtime\")] %&gt;%\n            dcast(.,model ~ variable) %&gt;%\n            .[,.(model,start,end,runtime)]\n}\n\ninput_data = rbindlist(input_data.list)\noutput_data = rbindlist(output_data.list)\nruntime_data = rbindlist(runtime_data.list)\n\n\nThe jobs started execution at 2023-05-31 00:49:56 and all finished by 2023-05-31 00:49:57 for an elapsed runtime of 1 seconds and a total computation time of 5 seconds. Use of Hera resulted in a job completing 5\\(\\times\\) faster. Figure 1 shows the simulated data and estimated linear fits for each model run in the job-array.\n\n\nShow code\nlibrary(ggplot2)\ninput_data %&gt;%\nggplot() +\ngeom_point(aes(x=x,y=y,fill=model),alpha=0.05,size=5,shape=21) +\ngeom_line(data=output_data,aes(x=x,y=pred_y,color=model),linewidth=2)\n\n\n\n\n\n\n\n\nFigure 1: Linear model fits from the 10 models run.",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  },
  {
    "objectID": "assets/web-about-nd.html",
    "href": "assets/web-about-nd.html",
    "title": "Nicholas Ducharme-Barth",
    "section": "",
    "text": "Nicholas Ducharme-Barth joined the Pacific Islands Fisheries Science Center in 2021. Previously, Nicholas worked at the Pacific Community (SPC) conducting pelagic stock assessments for the Western and Central Pacific Fisheries Commission (WCPFC). He received his B.S. in Mathematics from the College of William & Mary and his Ph. D. in Fisheries and Aquatic Sciences from the University of Florida.\n\n\n Back to top",
    "crumbs": [
      "Contact us",
      "Nicholas Ducharme-Barth"
    ]
  },
  {
    "objectID": "assets/array_osg.html",
    "href": "assets/array_osg.html",
    "title": "Submitting an array job with OSG",
    "section": "",
    "text": "In this example we step through submitting an array job on OSG where we want to run the same job in a number of directories. In this case the job is running a simple R script that reads in the data.csv file stored in the directory, fits a linear model, and writes the parameter estimates to a par.csv. We specify which directories we want to run jobs in as a part of the job-array using a text file to specify the directory path names on OSG.\nThe osg/array_lm example can be set-up either by cloning the repository git clone https://github.com/MOshima-PIFSC/NSASS-HTC-HPC-Computing.git, or stepping through the following code:",
    "crumbs": [
      "Documentation",
      "Submitting an array job with OSG"
    ]
  },
  {
    "objectID": "assets/array_osg.html#setup-data-inputs-and-directories",
    "href": "assets/array_osg.html#setup-data-inputs-and-directories",
    "title": "Submitting an array job with OSG",
    "section": "1 Setup data inputs and directories",
    "text": "1 Setup data inputs and directories\n\nDefine a relative path, we are starting from the root directory of this project.\n\n\nproj_dir = this.path::this.proj() \n\n\nDefine directory names for each run.\n\n\ndir_name = paste0(\"rep_0\", 0:9)\n\n\nIterate across directories, create them, and then write a simple .csv file into them containing data to fit a linear model.\n\n\nfor(i in seq_along(dir_name)){\n    \n    if(!file.exists(file.path(proj_dir, \"example\", \"OSG\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\")))\n    {\n        set.seed(i)\n        dir.create(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", dir_name[i]), recursive=TRUE)\n        tmp = data.frame(x=1:1000)\n        tmp$y = (i + (0.5*i)*tmp$x) + rnorm(1000,0,i)\n        write.csv(tmp, file = file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\"))\n    }\n}\n\n\nWrite an R script to read in the data, run a linear model, and report back the estimated parameters.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"run_lm_osg_array.r\"))){\n    script_lines = c(\"tmp=read.csv('data.csv')\", \n    \"fit=lm(y~x,data=tmp)\", \n    \"out = data.frame(par=unname(fit$coefficients))\", \n    \"write.csv(out,file='par.csv')\"\n    )\n    writeLines(script_lines, con = file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"run_lm_osg_array.r\"))\n}\n\n\nWrite a text file containing the full path names for where the directories will be on OSG.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"osg_job_directories.txt\"))){\n    dir_lines = paste0(\"./../../inputs/\", dir_name, \"/\")\n    writeLines(dir_lines, con = file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"osg_job_directories.txt\"))\n}\n\n\nIn addtion to the input files, you will need to have 2 additional scripts: a wrapper script (wrapper.sh) and a submission script (submission.sub), examples of both can be found in examples/OSG/array_lm/scripts. To easily upload all the necessary files at once, compress the entire array_lm/ directory as a tar.gz file upload.array_lm.tar.gz.\n\n\nsystem(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\"), \";tar -czf upload.array_lm.tar.gz * \"))",
    "crumbs": [
      "Documentation",
      "Submitting an array job with OSG"
    ]
  },
  {
    "objectID": "assets/array_osg.html#osg-workflow",
    "href": "assets/array_osg.html#osg-workflow",
    "title": "Submitting an array job with OSG",
    "section": "2 OSG workflow",
    "text": "2 OSG workflow\n\nConnect to OSG\nAs mentioned, access to OSG and file transfer is done using a pair of Terminal/PowerShell windows, we will call them Terminal A and Terminal B. In Terminal A, log onto your access point and create a directory for this example.\n\n\nssh User.Name@ap21.uc.osg-htc.org\nmkdir array_lm\n\n\nTransfer files\n\nWe will upload the compressed fileupload.array_lm.tar.gz into the OSG directory that you just created. The following files should be included:\n\nall replicate data files to run the linear models on;\nthe r script run_lm_osg_array.r;\nthe text file osg_job_directories.txt with the directory names;\nthe wrapper script wrapper.sh which unpacks files, sets up job timing, executes the R script, and packages results;\nthe submission script submission.sub;\nand a bash script prep.sh that prepares the files to be run on HTCondor, including changing file permissions, making directory structures, and changing dos2unix line endings.\n\nFor this example, we are using the container that was built in the Hera SS3 example. If you are unsure of how to build a container or access it in OSG please refer back to that example.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the submission script submission.sub you will need to change the following before you upload and run the script:\n\nLine 7: Change User.Name to your user name.\nLine 15: Change User.Name to your user name and linux-r4ss-v4.sif to the name of your container file. For more information on building containers, see Running an array of SS3 jobs on Hera.\nLine 26: Change the project name from osg.project_name to the name of your OSG project.\nLines 30 and 37: Change User.Name in the file path to your user name.\nNOTE There cannot be an empty last line in submission.sub. Make sure the script is 34 lines. If in doubt, backspace up to the last letter on the last line.\n\n\n\nIn Terminal B, navigate to the directory where the compressed file is on your local computer and run:\n\nscp upload.array_lm.tar.gz User.Name@ap21.uc.osg-htc.org:/home/User.Name/array_lm \n\nYou will be prompted for your passphrase and RSA code before the file transfers. Once the file transfer is complete, go back to Terminal A and you can untar the files by navigating to the array_lm directory and running:\n\ntar - xvf upload.array_lm.tar.gz \n\n\nPrepare scripts\n\nStill in Terminal A, change the permissions and line endings for osg_prep.sh. Navigate to the scripts/bash directory and change the change the line endings for the prep.sh script and then execute it to prepare the other scripts as neccessary.\n\n# navigate to directory\ncd scripts/bash\n# change permissions to make the file executable \nchmod 777 prep.sh \n# change line endings \ndos2unix prep.sh \n# run script\n./prep.sh \n\n\nSubmit job\n\nOnce you are ready, you can submit the job by running the command condor_submit.\n\n# navigate out of bash directory and into condor_submit directory\ncd ../condor_submit\n# submit job\ncondor_submit submission.sub\n\nWhile your job is running, you can check on it using the following commands:\n\ncondor_q shows status of all of your jobs: running, idle, or held\ncondor_q -run shows running jobs only\ncondor_q -hold shows jobs that are held\n\nUsing these commands you can get the job id number and peek directly at what is happening on the compute node using condor_ssh_to_the_job &lt;job_id&gt;. This is most useful for look at longer jobs or to see if intermediate files are being produced correctly.\nFor more information on tracking and restarting jobs, see here. ##TODO: add link, check that the documentation below through log files is included in ss example\n\nDownload results\n\nOnce the jobs have completed, you can retrieve the results for further analysis on your local computer. The easiest way to do this is to compress all of the directories in array_lm/inputs. In Terminal A, logged into OSG, run:\n\ntar -czf download.array_lm.tar.gz ./rep_*\n\nWe use the wildcard character * to indicate that we want to include everything with the name starting with rep_. This will give us all of the directory folders. Then on your local compter create a new directory outputs to put all of the downloaded results. In Terminal B, navigate to the outputs folder and run:\n\n# navigate to outputs folder, assuming you are in array_lm/\ncd outputs\n# download all files from array_lm/inputs on OSG into current directory\nscp -r User.Name@ap21.uc.osg-htc.org:/home/User.Name/array_lm/inputs/download.array_lm.tar.gz ./ \n\nAgain you will be prompted for your passphrase and RSA code before any files can transfer.\nYou can then unzip the files by running:\n\n# untar results\ntar -xzf download.array_lm.tar.gz\n\nin Terminal B.",
    "crumbs": [
      "Documentation",
      "Submitting an array job with OSG"
    ]
  },
  {
    "objectID": "assets/array_osg.html#process-the-output",
    "href": "assets/array_osg.html#process-the-output",
    "title": "Submitting an array job with OSG",
    "section": "3 Process the output",
    "text": "3 Process the output\nIn R, iterate through the sub-directories of the input and output data to extract the results of the linear model fits, and the model run time information.\n\n\nShow code\nlibrary(data.table)\nlibrary(magrittr)\n\ninput_data.list = as.list(rep(NA,10))\noutput_data.list = as.list(rep(NA,10))\nruntime_data.list = as.list(rep(NA,10))\n\nfor(i in seq_along(dir_name)){\n    # get input data\n        input_data.list[[i]] = fread(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", dir_name[i],\"data.csv\")) %&gt;%\n            .[,.(x,y)] %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            .[,.(model,x,y)]\n    \n    # untar results\n        system(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"outputs\", dir_name[i],\"/\"), \";tar -xzf End.tar.gz\"))\n\n    # get output\n        output_data.list[[i]] = fread(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"outputs\", dir_name[i],\"par.csv\")) %&gt;%\n            .[,.(par)] %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            .[,.(model,par)] %&gt;%\n            melt(.,id.vars=\"model\") %&gt;%\n            .[,variable:=c(\"intercept\",\"slope\")] %&gt;%\n            dcast(.,model ~ variable) %&gt;%\n            merge(.,input_data.list[[i]][,.(model,x)],by=\"model\") %&gt;%\n            .[,pred_y := intercept+slope*x] %&gt;%\n            .[,.(model,x,pred_y)]\n    # get time\n        runtime_data.list[[i]] = readLines(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"outputs\", dir_name[i],\"runtime.txt\")) %&gt;%\n            gsub(\".*?([0-9]+).*\", \"\\\\1\", .) %&gt;%\n            as.numeric(.) %&gt;%\n            as.data.table(.) %&gt;%\n            setnames(.,\".\",\"time\") %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            melt(.,id.vars=\"model\") %&gt;%\n            .[,variable:=c(\"start\",\"end\",\"runtime\")] %&gt;%\n            dcast(.,model ~ variable) %&gt;%\n            .[,.(model,start,end,runtime)]\n}\n\ninput_data = rbindlist(input_data.list)\noutput_data = rbindlist(output_data.list)\nruntime_data = rbindlist(runtime_data.list)\n\n\nThe jobs started execution at 2024-11-02 00:50:03 and all finished by 2024-11-02 00:50:31 for an elapsed runtime of 28 seconds and a total computation time of 6 seconds. Use of Hera resulted in a job completing 0.21\\(\\times\\) faster. Figure 1 shows the simulated data and estimated linear fits for each model run in the job-array.\n\n\nShow code\nlibrary(ggplot2)\ninput_data %&gt;%\nggplot() +\ngeom_point(aes(x=x,y=y,fill=model),alpha=0.05,size=5,shape=21) +\ngeom_line(data=output_data,aes(x=x,y=pred_y,color=model),linewidth=2)\n\n\n\n\n\n\n\n\nFigure 1: Linear model fits from the 10 models run.",
    "crumbs": [
      "Documentation",
      "Submitting an array job with OSG"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html",
    "href": "assets/hera_documentation.html",
    "title": "Working with NOAA HPC Hera",
    "section": "",
    "text": "Hera is one of NOAA’s high performance computing resources available to NMFS scientists where jobs are run and scheduled with SLURM. In order to access the system, you must first have a RDHPCS user account and request access to a project. For all of the examples in this documentation, we will be working in the htc4sa project. For complete documentation about Hera and other RDHPCS resources, see the NOAA RDHPCS website.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your RDHPCS user name.\nReplace bastion with either boulder or princeton depending on which bastion you wish to login in through.\nReplace project_name with your RDHPCS project name.",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html#hera",
    "href": "assets/hera_documentation.html#hera",
    "title": "Working with NOAA HPC Hera",
    "section": "",
    "text": "Hera is one of NOAA’s high performance computing resources available to NMFS scientists where jobs are run and scheduled with SLURM. In order to access the system, you must first have a RDHPCS user account and request access to a project. For all of the examples in this documentation, we will be working in the htc4sa project. For complete documentation about Hera and other RDHPCS resources, see the NOAA RDHPCS website.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your RDHPCS user name.\nReplace bastion with either boulder or princeton depending on which bastion you wish to login in through.\nReplace project_name with your RDHPCS project name.",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html#connecting-to-hera-via-ssh",
    "href": "assets/hera_documentation.html#connecting-to-hera-via-ssh",
    "title": "Working with NOAA HPC Hera",
    "section": "2 Connecting to Hera via ssh",
    "text": "2 Connecting to Hera via ssh\nOpen a terminal window (.e.g, command prompt or PowerShell), then open an ssh tunnel to Hera by using the following command:\n\nssh -m hmac-sha2-256-etm@openssh.com User.Name@hera-rsa.bastion.rdhpcs.noaa.gov -p22\n\nNote that the flag -p22 specifies opening port 22 and is optional. When prompted, put in your password followed by the RSA authentication code:\n\nXXXXXXXXRSACODE\n\nIf you see the following then you have connected successfully:\n\n________________________________________________________\n|                                                        |\n|                                                        |\n|  Welcome to the Hera High Performance Computing system |\n|                                                        |\n|        This system is located in Fairmont, WV          |\n|                                                        |\n|          Please Submit Helpdesk Requests to:           |\n|               rdhpcs.hera.help@noaa.gov                |\n|                                                        |\n|________________________________________________________|\n\nNote that you will also see the following terminal output upon connecting:\n\nLocal port XXXXX forwarded to remote host.\nRemote port YYYYY forwarded to local host.\n\nWhere XXXXX and YYYYY are your 4-5 digit port forwarding numbers. Make note of what your local port number (XXXXX) is since this will be used for scp file transfer using an ssh tunnel.",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html#transferring-files-tofrom-hera",
    "href": "assets/hera_documentation.html#transferring-files-tofrom-hera",
    "title": "Working with NOAA HPC Hera",
    "section": "3 Transferring files to/from Hera",
    "text": "3 Transferring files to/from Hera\n\n3.1 via scp using data transfer node (DTN)\nInformation in this section is largely based on this wiki. File transfer using a DTN is only possible from machines within the noaa.gov domain (VPN ok), and can only transfer files to the scratch directory. However, this is ok since it is recommended that input/output data files are stored in the scratch directory. Using a DTN is faster than using the ssh tunnel.\n\n\n\n\n\n\nNote\n\n\n\nThe scratch1/NMFS/project_name/ directory is a shared space (shared access and shared disk space) for all users of a project. Rather than dumping files into the root project directory (e.g., project_name/) it is better to place them in your own sub-directory. Since this sub-directory structure does not already exist you will need to log into Hera and create it:\n\nmkdir scratch1/NMFS/project_name/User.Name/\n\nThis will avoid over-writing other users’ work.\n\n\nFile transfer takes place via scp and you need to specify the source file (and path to the file if your terminal is not in that directory) and destination path as shown:\n\n## format is scp &lt;source&gt; &lt;destination&gt; \nscp /path/to/local/file User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/\n\nIf trying to transfer from a machine not within the NOAA domain (or on the VPN) you can use scp with an untrusted DTN:\n\nscp /path/to/local/file User.Name@udtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/data_untrusted/User.Name/\n\nNote that dtn-hera was changed to udtn-hera. Since files cannot stay in the data_untrusted directory we will need to log in to Hera and move it to the correct project directory on scratch. Once logged in, moving the files can be done with rsync, and then they can be deleted from data_untrusted by:\n\nrsync -axv /scratch1/data_untrusted/User.Name/file /scratch1/NMFS/project_name/User.Name/\nrm /scratch1/data_untrusted/User.Name/file\n\nFiles can be downloaded back to your local machine using scp from either the trusted (dtn) or untrusted (udtn) DTN:\n\nscp User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/file /path/to/local/file \n\n\n\n3.2 via scp using an ssh tunnel\nInformation in this section is largely based on this wiki. File transfer using an ssh tunnel is possible from all locations. Using the ssh tunnel for file transfer requires a two-step process with two active terminal windows (we suggest using PowerShell):\n\nIn the first terminal window, open an ssh connection to Hera with port forwarding:\n\n\nssh -m hmac-sha2-256-etm@openssh.com -LXXXXX:localhost:XXXXX User.Name@hera-rsa.bastion.rdhpcs.noaa.gov\n\nReplace XXXXX with your local port number.\n\nIn the second terminal window you can check to see if the tunnel was properly created:\n\n\nssh -p XXXXX User.Name@localhost\n\nif you get prompted for your password then success! Press CONTROL+C to ignore this prompt. Staying within this 2nd terminal window use scp to transfer your file:\n\nscp -P XXXXX /path/to/local/file User.Name@localhost:/home/User.Name/\n\nThis will copy your file from your local machine into your home directory on Hera. Simply append additional directory structure to /home/User.Name/ to copy it into a sub-directory that you have created on Hera (e.g., /home/User.Name/sub/dir/). To transfer files into your scratch directory, specify the proper destination path (e.g., /scratch1/NMFS/project_name/User.Name/).\nTo download a file from Hera using scp and the ssh tunnel use the following:\n\nscp -P XXXXX User.Name@localhost:/home/User.Name/file_name /path/to/local/file",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html",
    "href": "assets/ss3-hera.html",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "",
    "text": "Similar to the array_lm example, this example also sets up running an array job on Hera. As before, we will use a *.txt to indicate which directories we want to run jobs in as a part of our array.\nThere are a few main differences that serve to illustrate useful modifications to the workflow:\nThe hera/ss3 example can be set-up either by cloning the repository git clone https://github.com/MOshima-PIFSC/NSASS-HTC-HPC-Computing.git, or stepping through the following code:",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#build-software-container",
    "href": "assets/ss3-hera.html#build-software-container",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "1 Build software container",
    "text": "1 Build software container\nSoftware containers allow for portable, reproducible research by allowing researchers to set-up a software environment to their exact spefications and can run it on any Linux system. The Apptainer container system is widely used across HPC/HTC systems, and makes it easy to build a container from a definition file. Running a job within a container means that you are able to replicate an identical software environment in any location with Apptainer installed, no matter the native operating system, software and installed packages. The Apptainer container can be built from any Linux machine with Apptainer installed, including the Open Science Grid (OSG) access points. Here we walk through the steps needed to build a Linux (Ubuntu 20.04) container containing Stock Synthesis (version 3.30.22.1), R (version 4.4.0) and the R packages r4ss, ss3diags, data.table, magrittr, and mvtnorm from a definition file, linux-r4ss-v4.def. In this case we will show the steps needed to build the container using the OSG access point as our Linux virtual machine (VM), though this may not be needed if working from an alternative Linux VM.\n\n\n\n\n\n\nCoding alert!\n\n\n\nNote: you will have to change apXX to match your OSG access point (e.g., ap20 or ap21).\n\n\nThe first step is to log onto your OSG access point via ssh using a Terminal/PowerShell window and make a directory to build your container in. In this case, we are creating the directory singularity1.\n\nssh User.Name@apXX.uc.osg-htc.org\nmkdir -p singularity/linux_r4ss\n\nUsing a second Terminal/PowerShell window, navigate to the directory that you cloned the NSASS-HTC-HPC-Computing repo into and upload the definition file (linux-r4ss-v4.def) to the directory you just created on OSG.\n\nscp apptainer/linux-r4ss-v4.def User.Name@apXX.uc.osg-htc.org:/home/User.Name/singularity/linux_r4ss\n\nBack in your first Terminal/PowerShell window manoeuvre into the directory, and build the container2. The second line of code is what builds the Singularity Image File (.sif) and takes two arguments: the name of the output .sif file and the input definition file (.def).\n\ncd singularity/linux_r4ss\napptainer build linux-r4ss-v4.sif linux-r4ss-v4.def\n\nUsing the second Terminal/PowerShell window, download the Singularity Image File (.sif) so that it can be uploaded for use on the NOAA Hera HPC system.\n\nscp User.Name@apXX.uc.osg-htc.org:/home/User.Name/singularity/linux_r4ss/linux-r4ss-v4.sif apptainer/",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#setup-data-inputs-and-directories",
    "href": "assets/ss3-hera.html#setup-data-inputs-and-directories",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "2 Setup data inputs and directories",
    "text": "2 Setup data inputs and directories\nGiven that our example is to run a 4-year retrospective analysis for each of the SS3 test models, the next step is downloading the SS3 test models from the nmfs-stock-synthesis/test-models Github repo. Once you’ve downloaded the test models, copy the models/ directory into a new example directory ss3/inputs/ within the NSASS-HTC-HPC-Computing/examples/hera/ directory on your machine. If you cloned the NSASS-HTC-HPC-Computing repo, the SS3 test models will already be in the correct location.\nFor the sake of example the job array will be set-up to run each retrospective peel (e.g., -0 years, -1 year, … , -4 years of data) as individual jobs in the job array. This is more efficient in a true HTC environment such as OSG however on Hera it could make more sense to bundle the initial model run and subsequent retrospective peels as a single job. We will store the results of each retrospective peel in its own directory. The directories on Hera will be listed in a text file, and we will use this text file to launch jobs on Hera (as a part of the job array) in each of the named directories.\nLet us define that text file using R.\n\nDefine a relative path, we are starting from the root directory of this project.\n\n\n\nShow code used to define relative paths.\nproj_dir = this.path::this.proj()\nhera_project = \"NMFS/project_name/User.Name/\"\n\n\n\nWrite a text file containing the full path names for where the directories will be on Hera.\n\n\n\nShow code used to define job directory structure.\ntest_models=list.dirs(paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\"),recursive=FALSE,full.names=FALSE)\nretro_peels=0:4\n\n# replace '-' with '_' in model names since we will use '-' as a delimiter\n    if(length(grep(\"-\",test_models,fixed=TRUE))&gt;0){\n        test_models_new = gsub(\"-\",\"_\",test_models)\n        rename_models_idx = grep(\"-\",test_models,fixed=TRUE)\n        for(i in seq_along(rename_models_idx)){\n            # create new dir\n            dir.create(paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models_new[rename_models_idx[i]]),recursive=TRUE)\n            # copy files\n            file.copy(paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models[rename_models_idx[i]],\"/\",list.files(paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models[rename_models_idx[i]]),full.names=FALSE,recursive=FALSE)),paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models_new[rename_models_idx[i]]))\n            # delete old dir\n            # file.remove(paste0(proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models[rename_models_idx[i]],\"/\"))\n            shell(paste0(\"powershell rm -r \",proj_dir,\"/examples/hera/ss3/inputs/models/\",test_models[rename_models_idx[i]],\"/\"))\n        }\n        test_models = test_models_new\n    }\n    \n\n# define scenarios\nscenario_df = expand.grid(model=test_models,peel=retro_peels)\nscenario_df$run_id = 1:nrow(scenario_df)\nscenario_df = scenario_df[,c(3,1,2)]\nscenario_df$run_id = ifelse(scenario_df$run_id&lt;10,paste0(0,scenario_df$run_id),as.character(scenario_df$run_id))\n\n# write text file\nhera_dir_lines = paste0(\"/scratch1/\", hera_project, \"examples/ss3/output/\", apply(scenario_df,1,paste0,collapse=\"-\"), \"/\")\nwriteLines(hera_dir_lines, con=paste0(proj_dir, \"/examples/hera/ss3/inputs/hera_job_directories.txt\"))",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#prepare-job-scripts",
    "href": "assets/ss3-hera.html#prepare-job-scripts",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "3 Prepare job scripts",
    "text": "3 Prepare job scripts\nDuring benchmark testing, issues were identified when trying to apply an HTC workflow to Hera, and a seperate workflow was developed which may be more Hera/Slurm appropriate. This takes advantage of the gnu parallel utility to run batches of jobs on distinct compute nodes. In this particular example 90 models will be run across 3 nodes each using 30 CPUs, and we will set the maximum run time to 1 hour. This reserves the entire node for computations thus reducing the competition for resources for any one job3. In order to execute this workflow, instructions are coordinated using four nested scripts:\n\nparallel-submit.sh: This script prepares files for Slurm job execution, makes the directory structure specified by hera_job_directories.txt, specifies the job requirements and submits the parallel jobs.\nparallel-job-exec.sh: This is a script that defines variables to be passed to the software container and a second bash script wrapper-r.sh.\nwrapper-r.sh: This wrapper script controls file input/output to and from the R script ss3-example-calcs.r, executes the R script, conducts job timing and tidies up the job working directory.\nss3-example-calcs.r: This is the actual computation script which modifies the SS3 input files as needed, executes the appropriate SS3 model run and conducts any needed post-processing of the output within R.\n\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn parallel-submit.sh you will need to change the following before you upload and run the script:\n\nLine 20-22: change account project_name to the name of your project. If you are using the NOAA htc4sa project, it would be htc4sa.\n\n\n\n\nFrom within R, compress the ss3/inputs/ and ss3/slurm_scripts/ directories as a tar.gz file upload.example-ss3.tar.gz. This simplifies the number of steps needed for file transfers.\n\n\nshell(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"hera\", \"ss3\"), \";tar -czf upload.example-ss3.tar.gz inputs/ slurm_scripts/\"))",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#hera-workflow",
    "href": "assets/ss3-hera.html#hera-workflow",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "4 Hera workflow",
    "text": "4 Hera workflow\n\nConnect to Hera\n\nOpen a PowerShell terminal and connect to Hera. This terminal will be your remote workstation, call it Terminal A. You will be prompted for your RSA passcode, which is your password followed by the 8-digit code from the authenticator app.\n\nssh -m hmac-sha2-256-etm@openssh.com User.Name@hera-rsa.boulder.rdhpcs.noaa.gov -p22\n\n\nCreate directories\n\nIn Terminal A navigate to the project directory on scratch1 and create some directories. If using a shared directory such as htc4sa/, make a directory to save your work within this directory (.e.g., User.Name/). Change your working directory to this directory, and make a directory for the current project examples/ss3/4.\n\n# navigate to project directory\ncd /scratch1/NMFS/project_name/\n# create new directory\nmkdir User.Name/\n# navigate into new directory\ncd User.Name/\n# create directory for SLURM scripts and logs\nmkdir -p examples/ss3/\n\n\nTransfer files\n\nOpen a second PowerShell terminal in the NSASS-HTC-HPC-Computing directory on your machine. This will be your local workstation, call it Terminal B. Use this terminal window to upload via scp the needed files (examples/hera/ss3/upload.example-ss3.tar.gz and apptainer/linux-r4ss-v4.sif) to Hera. The upload.example-ss3.tar.gz will be uploaded to your directory within the project directory on scratch1. Make sure your VPN is active when attempting to upload using the DTN. You will be prompted for your RSA passcode after each scp command. Note that you will need to specify the MAC protocol needed for the scp file transfer similar to what was done for the initial ssh connection using scp -o MACs=hmac-sha2-256-etm@openssh.com.\n\nscp -o MACs=hmac-sha2-256-etm@openssh.com examples/hera/ss3/upload.example-ss3.tar.gz User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/examples/ss3/\nscp -o MACs=hmac-sha2-256-etm@openssh.com apptainer/linux-r4ss-v4.sif User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/examples/ss3/\n\n\nPrepare files and submit job on Hera\n\nIn Terminal A, un-tar upload.example-ss3.tar.gz, change the permissions/line endings for slurm_scripts/parallel-submit.sh and execute the script.\n\ntar -xzf upload.example-ss3.tar.gz\nchmod 777 slurm_scripts/parallel-submit.sh\ndos2unix slurm_scripts/parallel-submit.sh\n./slurm_scripts/parallel-submit.sh\n\nAfter job submission you can check on job status using squeue -u $USER or you can use the following for more detailed information.\n\n# count the number of output files (End.tar.gz) that have been produced\nfind . -type f -name End.tar.gz -exec echo . \\; | wc -l\n# list the size and location of all of the End.tar.gz files\nfind . -type f -name End.tar.gz -exec du -ch {} +\n\n\nDownload jobs and clean-up workspace\n\nOnce all jobs are completed (or the job has hit its time limit), use your Terminal B to download your jobs.\n\nscp -o MACs=hmac-sha2-256-etm@openssh.com -r User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/examples/ss3/output/ examples/hera/ss3/\n\nLastly in Terminal A, clean-up the /scratch1/NMFS/project_name/User.Name/ directory since it is a shared space.\n\n\n\n\n\n\nWarning!\n\n\n\nMake sure that you have verified that your jobs completed successfully and that all results have been downloaded before cleaning-up the directory.\n\n\n\n# move back up a level in the directory structure\ncd ..\n# delete the ss3/ directory\nrm -r ss3/",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#process-results",
    "href": "assets/ss3-hera.html#process-results",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "5 Process results",
    "text": "5 Process results\nAfter results are downloaded they can be processed in R to extract the model run times, time series of estimated biomass for each model run, and Mohn’s rho across retrospective peels for a given model ‘family’.\n\n\nShow output processing code\n# iterate over output files and extract quantities\nlibrary(data.table)\nlibrary(magrittr)\nlibrary(r4ss)\n\noutput_dirs = list.dirs(paste0(proj_dir,\"/examples/hera/ss3/output/\"),recursive=FALSE,full.names=FALSE)\nssb_dt.list = comptime_dt.list = as.list(rep(NA,length(output_dirs)))\nss_output_list =  as.list(rep(NA,length(output_dirs)))\nnames(ss_output_list) = output_dirs\n\nfor(i in seq_along(output_dirs)){\n    tmp_model = strsplit(output_dirs[i],\"-\")[[1]][2]\n    tmp_peel = as.numeric(strsplit(output_dirs[i],\"-\")[[1]][3])\n    tmp_index = as.numeric(strsplit(output_dirs[i],\"-\")[[1]][1])\n\n    # check if the End.tar.gz file got created\n    if(file.exists(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/End.tar.gz\")))\n    {\n        # get snapshot of original files in the directory\n        tmp_orig_files = list.files(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/\"))\n\n        # un-tar if the End.tar.gz file gets made\n        shell(paste0(\"powershell cd \", paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/\"), \";tar -xzf End.tar.gz\"))\n\n        # check if runtime.txt was produced and extract output\n        if(file.exists(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/runtime.txt\"))){\n            tmp_time = readLines(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/runtime.txt\")) %&gt;%\n            gsub(\".*?([0-9]+).*\", \"\\\\1\", .) %&gt;%\n            as.numeric(.) %&gt;%\n            as.data.table(.) %&gt;%\n            setnames(.,\".\",\"time\")\n            comptime_dt.list[[i]] = data.table(id = output_dirs[i])    \n            comptime_dt.list[[i]]$index = tmp_index\n            comptime_dt.list[[i]]$model = tmp_model\n            comptime_dt.list[[i]]$peel = tmp_peel\n            comptime_dt.list[[i]]$hera_start = as.POSIXct(tmp_time$time[1],origin=\"1970-01-01\")\n            comptime_dt.list[[i]]$hera_end = as.POSIXct(tmp_time$time[2],origin=\"1970-01-01\")\n            comptime_dt.list[[i]]$hera_runtime = tmp_time$time[3]/60\n\n            # clean-up\n            rm(list=c(\"tmp_time\"))\n        }\n\n        # if \"ss_report.RData\" is produced put it into the storage list\n        if(file.exists(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/ss_report.RData\"))){\n            load(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/ss_report.RData\"))\n            ss_output_list[[i]] = ss_report\n\n            ssb_dt.list[[i]] = ss_report$derived_quants %&gt;%\n                as.data.table(.) %&gt;%\n                .[Label %in% paste0(\"SSB_\", ss_report$startyr:ss_report$endyr)] %&gt;%\n                .[,id := output_dirs[i]] %&gt;%\n                .[,sbo:=Value/subset(ss_report$derived_quants,Label==\"SSB_Virgin\")$Value] %&gt;%\n                .[,yr:=sapply(Label,function(x)as.numeric(strsplit(x,\"_\")[[1]][2]))] %&gt;%\n                .[,.(id,yr,sbo)]\n            # clean-up\n                rm(list=c(\"ss_report\"))\n        }    \n\n        # clean-up\n        file.remove(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/\",setdiff(list.files(paste0(proj_dir,\"/examples/hera/ss3/output/\",output_dirs[i],\"/\")),tmp_orig_files)))\n        rm(list=c(\"tmp_orig_files\"))\n    } else {\n        comptime_dt.list[[i]] = data.table(id=output_dirs[i],index=tmp_index,model=tmp_model,peel=tmp_peel,hera_start=NA,hera_end=NA,hera_runtime=NA)\n        ssb_dt.list[[i]] = data.table(id=output_dirs[i],yr=2023,sbo=NA)\n    }\n\n    # clean-up\n    rm(list=c(\"tmp_model\",\"tmp_peel\",\"tmp_index\"))\n}\n\ncomptime_dt = rbindlist(na.omit(comptime_dt.list))\nssb_dt = rbindlist(ssb_dt.list) %&gt;% merge(comptime_dt[,.(id,index,model,peel)],.,by=\"id\")\nss_output_list = na.omit(ss_output_list)\n\n# adjust times to account for the fact that model 79 did not finish within the 1 hour allocation\ncomptime_dt$hera_start[79] = comptime_dt$hera_start[80]\ncomptime_dt$hera_end[79] = comptime_dt$hera_start[79] + 60^2\ncomptime_dt$hera_runtime[79] = 60\n\n# save\nfwrite(comptime_dt,file=paste0(proj_dir,\"/examples/hera/ss3/output/comptime_dt.csv\"))\nfwrite(ssb_dt,file=paste0(proj_dir,\"/examples/hera/ss3/output/ssb_dt.csv\"))\n\n# calculate Mohn's rho\nunique_models = unique(comptime_dt$model)\nretro_dt.list = as.list(rep(NA,length(unique_models)))\n\nfor(i in seq_along(unique_models)){\n    tmp_model = unique_models[i]\n\n    retro_dt.list[[i]] = data.table(model=tmp_model)\n    retro_dt.list[[i]]$type = c(\"SBO\")\n    retro_dt.list[[i]]$rho = NA\n\n    if(uniqueN(na.omit(ssb_dt[model==tmp_model])$peel)==5){\n        tmp_dt = ssb_dt[model==tmp_model]\n        base_dt = tmp_dt[peel==0]\n        year_vec = max(base_dt$yr) - 1:4\n        bias_vec = rep(NA,length(year_vec))\n        # calc Mohn's rho for runs where all models completed\n        for(j in 1:4){\n            bias_vec[j] = (ssb_dt[model==tmp_model&peel==j&yr==year_vec[j]]$sbo - base_dt[yr==year_vec[j]]$sbo)/base_dt[yr==year_vec[j]]$sbo\n        }\n        retro_dt.list[[i]]$rho = mean(bias_vec)\n        rm(list=c(\"tmp_dt\",\"base_dt\",\"year_vec\",\"bias_vec\"))\n    } \n    \n    rm(list=c(\"tmp_model\"))\n}\n\nretro_dt = rbindlist(retro_dt.list)\nfwrite(retro_dt,file=paste0(proj_dir,\"/examples/hera/ss3/output/retro_dt.csv\"))\n\n\n\n5.1 Job runtime\nThe 90 jobs run on Hera completed 4.15 hours of calculations (2.77 minutes per job) in an elapsed time of 1 hour.\nExcluding the job that timed out at the 1-hour limit the 89 jobs run on Hera completed 3.15 hours of calculations (2.12 minutes per job) in an elapsed time of 14.48 minutes or \\(\\sim\\) 13 times faster (Figure 1).\n\n\nShow plotting code\nlibrary(ggplot2)\n\np = comptime_dt_minus %&gt;%\n.[,.(id,hera_start,hera_end)] %&gt;%\nmelt(.,id.vars=\"id\") %&gt;%\n.[,variable:=ifelse(variable%in%c(\"hera_start\"),\"start\",\"end\")] %&gt;%\ndcast(.,id~variable) %&gt;%\n.[order(start)] %&gt;%\nggplot() +\nxlab(\"Time (GMT)\") +\nylab(\"Job\") +\ngeom_segment(aes(x=start,xend=end,y=id,yend=id),color=\"#003087\",alpha=0.5,linewidth=2) +\ntheme(panel.background = element_rect(fill = \"transparent\", color = \"black\", linetype = \"solid\"),\n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),  \n            strip.background =element_rect(fill=\"transparent\"),\n            legend.key = element_rect(fill = \"transparent\"),\n            axis.text.y=element_blank(),\n            axis.ticks.y=element_blank())\np\n\n# save plot\nggsave(\n  \"hera-ss3-elapsed.png\",\n  plot = p,\n  device = \"png\",\n  path = paste0(proj_dir,\"/assets/static/\"),\n  width = 8,\n  height = 4.5,\n  units = c(\"in\"),\n  dpi = 300,\n  bg = \"transparent\")\n\n\n\n\n\n\n\n\nFigure 1: Start and stop time for jobs run on Hera, excluding the job that timed out.",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#example-results",
    "href": "assets/ss3-hera.html#example-results",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "6 Example results",
    "text": "6 Example results\n\n6.1 Retrospectives\nRetrospective plots of static biomass depletion for the SS3 test models are shown in Figure 2.\n\n\nShow plotting code\ntext_dt.list = as.list(rep(NA,uniqueN(ssb_dt$model)))\nfor(i in seq_along(text_dt.list)){\n    tmp_dt = ssb_dt[model==unique(ssb_dt$model)[i]]\n    tmp_min_yr = min(tmp_dt$yr)\n    text_dt.list[[i]] = data.table(model=unique(ssb_dt$model)[i],yr=tmp_min_yr,sbo=0.2,rho=round(retro_dt[model==unique(ssb_dt$model)[i]]$rho,digits=2))\n}\ntext_dt = rbindlist(text_dt.list)\n\n\np = ssb_dt %&gt;%\n            ggplot() +\n            facet_wrap(~model,scales=\"free_x\") +\n            xlab(\"Year\") +\n            ylab(expression(SB/SB[0])) +\n            ylim(0,NA) +\n            geom_hline(yintercept=0) +\n            geom_path(aes(x=yr,y=sbo,color=as.character(peel),group=id)) +\n            geom_text(data=text_dt,aes(x=yr,y=sbo,label=rho),size=3,hjust = 0) +\n            viridis::scale_color_viridis(\"Peel\",begin = 0.1,end = 0.8,direction = 1,option = \"H\",discrete=TRUE) +\n            viridis::scale_fill_viridis(\"Peel\",begin = 0.1,end = 0.8,direction = 1,option = \"H\",discrete=TRUE) +\n            theme(panel.background = element_rect(fill = \"transparent\", color = \"black\", linetype = \"solid\"),\n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),\n            strip.background =element_rect(fill=\"transparent\"),\n            legend.key = element_rect(fill = \"transparent\"))\np\n\n# save plot\nggsave(\n  \"hera-ss3-retro.png\",\n  plot = p,\n  device = \"png\",\n  path = paste0(proj_dir,\"/assets/static/\"),\n  width = 8,\n  height = 4.5,\n  units = c(\"in\"),\n  dpi = 300,\n  bg = \"transparent\")\n\n\n\n\n\n\n\n\nFigure 2: Depletion estimates across retrospective peels from the Stock Synthesis testing model suite examples. Mohn’s rho values are printed in each panel.",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/ss3-hera.html#footnotes",
    "href": "assets/ss3-hera.html#footnotes",
    "title": "Running an array of SS3 jobs on Hera",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis directory can be named anything that you like, in this case singularity is a legacy name from an earlier version of the code written before Singularity changed its name to Apptainer.↩︎\nThis may take ~10-15 minutes depending on how long it takes to install R packages.↩︎\nWhile this workflow leads to better scheduling and makes HTC applications possible on Hera it may not be computationally efficient if users do not make use of all CPUs on a given node. For example, given that an entire compute node is requested, the user’s allocation on Hera will be billed for use of all CPUs on that node even if not all are in use.↩︎\nNote that this path should match the path defined in hera_job_directories.txt.↩︎",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on Hera"
    ]
  },
  {
    "objectID": "assets/presentation.html#section",
    "href": "assets/presentation.html#section",
    "title": "HTC/HPC for Stock Assessments",
    "section": "",
    "text": "Operationalizing available research computing resources for stock assessment\nNicholas Ducharme-Barth & Megumi Oshima\n2024-11-05"
  },
  {
    "objectID": "assets/presentation.html#what",
    "href": "assets/presentation.html#what",
    "title": "HTC/HPC for Stock Assessments",
    "section": "What?",
    "text": "What?\n\n\n\nResearch computing is the collection of computing, software, storage resources and services that allows for data analysis at scale.\n\n\n\n\n\nIn our particular case we are interested in leverging research computing to augment stock assessment worflows.\n\n\n\nRun more/bigger models in less time"
  },
  {
    "objectID": "assets/presentation.html#why",
    "href": "assets/presentation.html#why",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Why?",
    "text": "Why?\n\n\n\nImprove efficiency by running 10s - 1000s of models ‘simultaneously’.\n\n\n\n\n  \n\n2021 Southwest Pacific Ocean swordfish stock assessment\n\n\n\n\n9,300 model runs totalling ~46 months of computation time."
  },
  {
    "objectID": "assets/presentation.html#why-1",
    "href": "assets/presentation.html#why-1",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Why?",
    "text": "Why?\n\n\n\n\nEfficiency\n\n\n\n\n\n\nKnowledge acquisition\n\n\n\n\n\n\nAutomation, transparency, reproducibility & portability\n\n\n\n\n\n\nMulti-model inference\n\n\n\n\n\n\nSoftware containers\n\n\n\n\n\n\n\n\nBetter science"
  },
  {
    "objectID": "assets/presentation.html#how",
    "href": "assets/presentation.html#how",
    "title": "HTC/HPC for Stock Assessments",
    "section": "How?",
    "text": "How?\n\n\n\nHigh-throughput computing (HTC)\n\n\n\nSet-up to handle running many jobs simultaneously\n\n\n\n\nIdeal for running short, small, independent (embarrassingly parallel) jobs.\n\n\n\n\n\n\nHigh-performance computing (HPC)\n\n\n\nCan handle HTC workflows (in theory)\n\n\n\n\nCan also handle long running, large, multi-processor jobs (true parallel processing)\n\n\n\n\n\n\n\n\n2024 North Pacific shortfin mako shark assessment: Used HTC resources to complete ~4 months months of Bayesian simulation-estimation evaluations (18,000 model runs using RStan) in ~3 hours (1027 \\(\\times\\) faster) during a working group meeting.\n\n\n\n\n\n\nExample: Fitting large spatiotemporal model in R using TMB required 128 CPUs & 1TB RAM."
  },
  {
    "objectID": "assets/presentation.html#how-1",
    "href": "assets/presentation.html#how-1",
    "title": "HTC/HPC for Stock Assessments",
    "section": "How?",
    "text": "How?\n\n\n\nAvailable resources\n\n\n\n\n\n\nHigh-throughput computing (HTC)\n\n\n\nHigh-performance computing (HPC)\n\n\n\n\n\n\n\n\nPhoto credit: NOAA\n\n\n\n\nOpenScienceGrid (OSG): OSPool\n\n\n\n\nNOAA Hera"
  },
  {
    "objectID": "assets/presentation.html#how-2",
    "href": "assets/presentation.html#how-2",
    "title": "HTC/HPC for Stock Assessments",
    "section": "How?",
    "text": "How?\n\n\nOpenScienceGrid (OSG)\n\n\n\nUses HTCondor distributed computing network (no shared file system between compute nodes) to implement HTC workflows\n\n\n\n\nFree to use for US based researchers affiliated with academic/government organization and using OSG for research/education efforts\n\n\n\n\nShould not be used to analyze protected data\n\n\n\n\nNOAA Hera\n\n\n\nUses Slurm to schedule HPC (or HTC) workflows\n\n\n\n\nShared file system between compute nodes\n\n\n\n\nAllocation determines access\n\n\n\n\nNOAA resource so no restrictions on acceptable use/analyzing protected data if working on mission related tasks\n\n\n\n\n\n\nBoth use software containers"
  },
  {
    "objectID": "assets/presentation.html#software-containers",
    "href": "assets/presentation.html#software-containers",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Software containers",
    "text": "Software containers\n\n\nMany may already be using containers in existing cloud-based workspaces such as GitHub Codespaces or Posit Workbench\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplication: set up identical, custom software environments on OSG and Hera\n\n\n\n\nApplication: use to “version” analyses by “freezing” packages/libraries"
  },
  {
    "objectID": "assets/presentation.html#software-containers-1",
    "href": "assets/presentation.html#software-containers-1",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Software containers",
    "text": "Software containers\n\n\nApptainer\n\n\nSecure, portable and reproducible software container for Linux operating systems\n\n\n\n\nEasy to use\n\n\n\n\nDoesn’t require root privileges to build making it ideal for HTC/HPC environments\n\n\n\n\nPlays nice with existing containers (e.g., Docker)"
  },
  {
    "objectID": "assets/presentation.html#apptainer-1",
    "href": "assets/presentation.html#apptainer-1",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Apptainer",
    "text": "Apptainer\nLet’s look at an example (linux-r4ss-v4.def):\n\n\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    TZ=Etc/UTC && \\\n    ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && \\\n    echo $TZ &gt; /etc/timezone\n    apt update -y\n    apt install -y \\\n        tzdata \\\n        curl \\\n        dos2unix\n\n    apt-get update -y\n    apt-get install -y \\\n            build-essential \\\n            cmake \\\n            g++ \\\n            libssl-dev \\\n            libssh2-1-dev \\\n            libcurl4-openssl-dev \\\n            libfontconfig1-dev \\\n            libxml2-dev \\\n            libgit2-dev \\\n            wget \\\n            tar \\\n            coreutils \\\n            gzip \\\n            findutils \\\n            sed \\\n            gdebi-core \\\n            locales \\\n            nano\n    \n    locale-gen en_US.UTF-8\n\n    export R_VERSION=4.4.0\n    curl -O https://cdn.rstudio.com/r/ubuntu-2004/pkgs/r-${R_VERSION}_1_amd64.deb\n    gdebi -n r-${R_VERSION}_1_amd64.deb\n\n    ln -s /opt/R/${R_VERSION}/bin/R /usr/local/bin/R\n    ln -s /opt/R/${R_VERSION}/bin/Rscript /usr/local/bin/Rscript\n\n    R -e \"install.packages('remotes', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"install.packages('data.table', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"install.packages('magrittr', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"install.packages('mvtnorm', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"remotes::install_github('r4ss/r4ss')\"\n    R -e \"remotes::install_github('PIFSCstockassessments/ss3diags')\"\n\n    NOW=`date`\n    echo 'export build_date=$NOW' &gt;&gt; $SINGULARITY_ENVIRONMENT\n\n    mkdir -p /ss_exe\n    curl -L -o /ss_exe/ss3_linux https://github.com/nmfs-ost/ss3-source-code/releases/download/v3.30.22.1/ss3_linux\n    chmod 755 /ss_exe/ss3_linux\n\n%environment\n    export PATH=/ss_exe:$PATH\n    \n%labels\n    Author nicholas.ducharme-barth@noaa.gov\n    Version v0.0.4\n\n%help\n    This is a Linux (Ubuntu 20.04) container containing Stock Synthesis (version 3.30.22.1), R (version 4.4.0) and the R packages r4ss, ss3diags, data.table, magrittr, and mvtnorm."
  },
  {
    "objectID": "assets/presentation.html#apptainer-2",
    "href": "assets/presentation.html#apptainer-2",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Apptainer",
    "text": "Apptainer\nLet’s look at an example (linux-r4ss-v4.def):\nBuild on Linux system with Apptainer installed.\n\napptainer build linux-r4ss-v4.sif linux-r4ss-v4.def"
  },
  {
    "objectID": "assets/presentation.html#lets-walk-through-an-example",
    "href": "assets/presentation.html#lets-walk-through-an-example",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Let’s walk through an example",
    "text": "Let’s walk through an example\n\nIn this case we will use NOAA Hera to conduct a quick retrospective analysis of all models in the Stock Synthesis (SS3) testing suite.\n\n\n\n\n\n\nMore complete documentation of this example can be found on our GitHub website."
  },
  {
    "objectID": "assets/presentation.html#workflow",
    "href": "assets/presentation.html#workflow",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow",
    "text": "Workflow\n\nCreate container\n\n\n\nCreate files/scripts\n\n\n\n\nUpload files\n\n\n\n\nSubmit jobs\n\n\n\n\nDownload files back to local machine"
  },
  {
    "objectID": "assets/presentation.html#workflow---create-filesscripts",
    "href": "assets/presentation.html#workflow---create-filesscripts",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Create files/scripts",
    "text": "Workflow - Create files/scripts\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE: Develop and make job files/scripts\n\n\n\n\n\n\n\n\n\nImportant\n\n\nNote that you will need to replace User.Name with your actual NOAA RDHPCS user name and project_name with your specific Hera project name in the following code."
  },
  {
    "objectID": "assets/presentation.html#workflow---create-filesscripts-1",
    "href": "assets/presentation.html#workflow---create-filesscripts-1",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Create files/scripts",
    "text": "Workflow - Create files/scripts\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0. Text file specifying directories to run jobs in.\n\n\nhera_job_directories.txt\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/01-BigSkate_2019-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/02-Empirical_Wtatage_Age_Selex-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/03-growth_timevary-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/04-Hake_2018-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/05-Hake_2019_semi_parametric_selex-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/06-platoons-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/07-Sablefish2015-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/08-Simple-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/09-Simple_Lorenzen_tv_trend-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/10-Simple_NoCPUE-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/11-Simple_with_Discard-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/12-Simple_with_DM_sizefreq-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/13-Spinydogfish_2011-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/14-tagging_mirrored_sel-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/15-three_area_nomove-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/16-two_morph_seas_areas-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/17-vermillion_snapper-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/18-vermillion_snapper_F4-0/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/19-BigSkate_2019-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/20-Empirical_Wtatage_Age_Selex-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/21-growth_timevary-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/22-Hake_2018-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/23-Hake_2019_semi_parametric_selex-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/24-platoons-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/25-Sablefish2015-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/26-Simple-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/27-Simple_Lorenzen_tv_trend-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/28-Simple_NoCPUE-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/29-Simple_with_Discard-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/30-Simple_with_DM_sizefreq-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/31-Spinydogfish_2011-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/32-tagging_mirrored_sel-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/33-three_area_nomove-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/34-two_morph_seas_areas-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/35-vermillion_snapper-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/36-vermillion_snapper_F4-1/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/37-BigSkate_2019-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/38-Empirical_Wtatage_Age_Selex-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/39-growth_timevary-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/40-Hake_2018-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/41-Hake_2019_semi_parametric_selex-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/42-platoons-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/43-Sablefish2015-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/44-Simple-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/45-Simple_Lorenzen_tv_trend-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/46-Simple_NoCPUE-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/47-Simple_with_Discard-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/48-Simple_with_DM_sizefreq-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/49-Spinydogfish_2011-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/50-tagging_mirrored_sel-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/51-three_area_nomove-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/52-two_morph_seas_areas-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/53-vermillion_snapper-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/54-vermillion_snapper_F4-2/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/55-BigSkate_2019-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/56-Empirical_Wtatage_Age_Selex-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/57-growth_timevary-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/58-Hake_2018-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/59-Hake_2019_semi_parametric_selex-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/60-platoons-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/61-Sablefish2015-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/62-Simple-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/63-Simple_Lorenzen_tv_trend-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/64-Simple_NoCPUE-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/65-Simple_with_Discard-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/66-Simple_with_DM_sizefreq-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/67-Spinydogfish_2011-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/68-tagging_mirrored_sel-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/69-three_area_nomove-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/70-two_morph_seas_areas-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/71-vermillion_snapper-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/72-vermillion_snapper_F4-3/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/73-BigSkate_2019-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/74-Empirical_Wtatage_Age_Selex-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/75-growth_timevary-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/76-Hake_2018-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/77-Hake_2019_semi_parametric_selex-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/78-platoons-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/79-Sablefish2015-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/80-Simple-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/81-Simple_Lorenzen_tv_trend-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/82-Simple_NoCPUE-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/83-Simple_with_Discard-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/84-Simple_with_DM_sizefreq-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/85-Spinydogfish_2011-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/86-tagging_mirrored_sel-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/87-three_area_nomove-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/88-two_morph_seas_areas-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/89-vermillion_snapper-4/\n/scratch1/NMFS/project_name/User.Name/examples/ss3/output/90-vermillion_snapper_F4-4/"
  },
  {
    "objectID": "assets/presentation.html#workflow---create-filesscripts-2",
    "href": "assets/presentation.html#workflow---create-filesscripts-2",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Create files/scripts",
    "text": "Workflow - Create files/scripts\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Prepare files for Slurm job execution, specify job requirements and submit the parallel jobs.\n\n\nparallel-submit.sh\n#!/bin/bash\n\n# prep files for slurm job execution\nmkdir ./logs\ndos2unix ./inputs/hera_job_directories.txt\ndos2unix ./slurm_scripts/parallel-job-exec.sh\nchmod 777 ./slurm_scripts/parallel-job-exec.sh\n\n# make directory structure\n# recursively (mkdir -p) makes a new directory for each line in hera_job_directories.txt\nxargs -d '\\n' mkdir -p -- &lt; ./inputs/hera_job_directories.txt\n\n# Slurm job submission variables\n# -A project name\n# -t time requested (minutes)\n# -q queue type: batch (billed allocation) or windfall\n# -N nodes requested (leave at 1 since requesting additional nodes with each line)\n# -j number of jobs to run in parallel per node (restricted by number of total CPUs and available RAM)\n# seq job ids to run on each node (this can be greater than -j but only -j will be run at a time so the more job ids assigned to the node the longer they will wait to be executed)\nsbatch -A project_name -t 60 -q batch -N 1 --wrap 'set -x; parallel -j 30 -S `scontrol show hostnames \"$SLURM_JOB_NODELIST\"|paste -sd,` `pwd`/slurm_scripts/parallel-job-exec.sh `pwd` ::: `seq 0 29`; report-mem'\nsbatch -A project_name -t 60 -q batch -N 1 --wrap 'set -x; parallel -j 30 -S `scontrol show hostnames \"$SLURM_JOB_NODELIST\"|paste -sd,` `pwd`/slurm_scripts/parallel-job-exec.sh `pwd` ::: `seq 30 59`; report-mem'\nsbatch -A project_name -t 60 -q batch -N 1 --wrap 'set -x; parallel -j 30 -S `scontrol show hostnames \"$SLURM_JOB_NODELIST\"|paste -sd,` `pwd`/slurm_scripts/parallel-job-exec.sh `pwd` ::: `seq 60 89`; report-mem'"
  },
  {
    "objectID": "assets/presentation.html#workflow---create-filesscripts-3",
    "href": "assets/presentation.html#workflow---create-filesscripts-3",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Create files/scripts",
    "text": "Workflow - Create files/scripts\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Define variables to be passed to the software container and a bash wrapper script.\n\n\nparallel-job-exec.sh\n#!/bin/bash\n# read directories from a file list\n\npwd; hostname; date\n\ncd $1\nexport SLURM_ARRAY_TASK_ID=$2\n\necho $SLURM_ARRAY_TASK_ID\n\n# define current directory\ncwd=$(pwd)\n\n# define paths for singularity container\nsingularity_container=${cwd}/linux-r4ss-v4.sif\n\n# define variables and paths here to avoid hard coding insider the wrapper script\njob_wrapper_script=${cwd}/slurm_scripts/wrapper-r.sh\ndir_file=${cwd}/inputs/hera_job_directories.txt\nr_script=${cwd}/slurm_scripts/ss3-example-calcs.r\ninput_data_path=${cwd}/inputs/models/\nr_script_name=ss3-example-calcs.r\n\n# change permissions on scripts to allow it to run\nchmod 777 $job_wrapper_script\ndos2unix $job_wrapper_script\nchmod 777 $r_script\ndos2unix $r_script\n\n# run bash wrapper script within singularity environment\nsingularity exec $singularity_container $job_wrapper_script $SLURM_ARRAY_TASK_ID $dir_file $r_script $input_data_path $r_script_name &gt;& logs/out-parallel.$SLURM_ARRAY_TASK_ID"
  },
  {
    "objectID": "assets/presentation.html#workflow---create-filesscripts-4",
    "href": "assets/presentation.html#workflow---create-filesscripts-4",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Create files/scripts",
    "text": "Workflow - Create files/scripts\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Control file I/O to the R script, execute the R script, conduct job timing and package outputs.\n\n\nwrapper-r.sh\n#!/bin/bash\necho \"Running on host `hostname`\"\n\n# rename variables passed into the script\nslurm_array_task_id=$1\ndir_file=$2\n\n# create an array with all data directories\nline_index=$(($slurm_array_task_id+1))\necho ${line_index}\necho $dir_file\nrep_dir=$(sed -n ${line_index}p $dir_file) \necho $rep_dir\n\n# change to target directory\ncd ${rep_dir}\n\n# make working directory\nmkdir -p working/\ncd working/\n\n# copy files to working/\ncp $3 .\n\n# define variables for R script\ninput_data_path=$4\n\n# begin calcs\nstart=`date +%s`\nRscript $5 $rep_dir $input_data_path \n\n# end of calcs book-keeping\nend=`date +%s`\nruntime=$((end-start))\necho $runtime\necho Start $start &gt;  runtime.txt\necho End $end &gt;&gt; runtime.txt\necho Runtime $runtime &gt;&gt; runtime.txt\n\n# Create empty file so that it does not mess up when repacking tar\ntouch End.tar.gz\n# only pack up certain items\ntar -czf End.tar.gz ss_report.RData runtime.txt \n# move tar out of working/\ncd ..\nmv working/End.tar.gz .\n# delete working/\nrm -r working/"
  },
  {
    "objectID": "assets/presentation.html#workflow---create-filesscripts-5",
    "href": "assets/presentation.html#workflow---create-filesscripts-5",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Create files/scripts",
    "text": "Workflow - Create files/scripts\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Calculation script modifies SS3 input files, executes SS3 model run and conducts post-processing of output within R.\n\n\nss3-example-calcs.r\n# where is the job executing\n    print(getwd())\n\n# load packages\n    library(r4ss)\n\n# get args from bash environment\n    args = commandArgs(trailingOnly = TRUE)\n    print(args)\n\n# get scenario\n    scenario = tail(strsplit(args[1],\"/\")[[1]],n=1)\n    model = strsplit(scenario,\"-\")[[1]][2]\n    peel = as.numeric(strsplit(scenario,\"-\")[[1]][3])\n\n# copy model files\n    model_files = list.files(paste0(args[2],model,\"/\"),full.names=TRUE)\n    file.copy(from=model_files,to=getwd())\n\n# modify starter\n    tmp_starter = SS_readstarter()\n    tmp_starter$retro_yr = -peel\n\n# write files\n    SS_writestarter(tmp_starter, overwrite = TRUE)\n\n# run stock synthesis\n    run(exe=\"ss3_linux\")\n\n# extract model output\n    ss_report = try(SS_output(dir=getwd()),silent=TRUE) \n\n# save output\n    save(ss_report,file=\"ss_report.RData\")"
  },
  {
    "objectID": "assets/presentation.html#workflow---upload-files",
    "href": "assets/presentation.html#workflow---upload-files",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Upload files",
    "text": "Workflow - Upload files\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/"
  },
  {
    "objectID": "assets/presentation.html#workflow---upload-files-1",
    "href": "assets/presentation.html#workflow---upload-files-1",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Upload files",
    "text": "Workflow - Upload files\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal A\nssh -m hmac-sha2-256-etm@openssh.com User.Name@hera-rsa.boulder.rdhpcs.noaa.gov -p22"
  },
  {
    "objectID": "assets/presentation.html#workflow---upload-files-2",
    "href": "assets/presentation.html#workflow---upload-files-2",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Upload files",
    "text": "Workflow - Upload files\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal A\n# navigate to project directory\ncd /scratch1/NMFS/project_name/\n# create new directory\nmkdir User.Name/\n# navigate into new directory\ncd User.Name/\n# create directory for SLURM scripts and logs\nmkdir -p examples/ss3/"
  },
  {
    "objectID": "assets/presentation.html#workflow---upload-files-3",
    "href": "assets/presentation.html#workflow---upload-files-3",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Upload files",
    "text": "Workflow - Upload files\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal B\nscp -o MACs=hmac-sha2-256-etm@openssh.com examples/hera/ss3/upload.example-ss3.tar.gz User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/examples/ss3/\nscp -o MACs=hmac-sha2-256-etm@openssh.com apptainer/linux-r4ss-v4.sif User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/examples/ss3/"
  },
  {
    "objectID": "assets/presentation.html#workflow---submit-jobs",
    "href": "assets/presentation.html#workflow---submit-jobs",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Submit jobs",
    "text": "Workflow - Submit jobs\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal A\nchmod 777 slurm_scripts/parallel-submit.sh\ndos2unix slurm_scripts/parallel-submit.sh\n./slurm_scripts/parallel-submit.sh"
  },
  {
    "objectID": "assets/presentation.html#workflow---download-results",
    "href": "assets/presentation.html#workflow---download-results",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - Download results",
    "text": "Workflow - Download results\n\n\n\n\n\n\n\n\n\n\n\n\nHera\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nLogin node\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nscratch1/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal B\nscp -o MACs=hmac-sha2-256-etm@openssh.com -r User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/examples/ss3/output/ examples/hera/ss3/"
  },
  {
    "objectID": "assets/presentation.html#workflow---osg",
    "href": "assets/presentation.html#workflow---osg",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Workflow - OSG",
    "text": "Workflow - OSG\n\n\n\n\n\n\n\n\n\n\n\n\nOSPool n\n\n\n\n\n\n\n\nOSPool 2\n\n\n\n\n\n\n\nOSPool 1\n\n\n\n\n\n\n\nOSG\n\n\n\n\n\n\n\nLocal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDE\n\n\n\n\n\n\n\nTerminal A: ssh\n\n\n\n\n\n\n\nTerminal B: scp\n\n\n\n\n\n\n\nAccess point\n\n\n\n\n\n\n\nFile storage\n\n\n\n\n\n\n\nCompute node: 1\n\n\n\n\n\n\n\nCompute node: 2\n\n\n\n\n\n\n\nFile storage: 1\n\n\n\n\n\n\n\nFile storage: 2\n\n\n\n\n\n\n\nCompute node: 3\n\n\n\n\n\n\n\nCompute node: 4\n\n\n\n\n\n\n\nFile storage: 3\n\n\n\n\n\n\n\nFile storage: 4\n\n\n\n\n\n\n\nCompute node: n\n\n\n\n\n\n\n\nCompute node: 90\n\n\n\n\n\n\n\nFile storage: n\n\n\n\n\n\n\n\nFile storage: 90\n\n\n\n\n\n\n\n\n\n\n\n\nDocumentation of this example using OSG can be found on our GitHub website.\n\n\n\nBiggest difference: no shared file system between compute nodes"
  },
  {
    "objectID": "assets/presentation.html#example-results",
    "href": "assets/presentation.html#example-results",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Example results",
    "text": "Example results\n\n\nThe example ran 90 jobs (18 test models \\(\\times\\) 5 runs each; base + 4 peels), with only one job ‘timing out’ at 1-hour limit.\n\\(~\\) \\(~\\)\n\nExcluding the job that timed out the 89 jobs run on Hera completed 3.15 hours of calculations (2.12 minutes per job) in an elapsed time of 14.48 minutes or \\(\\sim\\) 13 \\(\\times\\) faster.\n\n\n\n\n\nDepletion estimates across retrospective peels from the SS3 testing model suite examples. Mohn’s \\(\\rho\\) values are printed in each panel.\n\n\n\n\n\nStart and stop time for jobs run on Hera, excluding the job that timed out.\n\n\n\n\nIs this ‘time-savings’ worth it?"
  },
  {
    "objectID": "assets/presentation.html#benchmark-testing",
    "href": "assets/presentation.html#benchmark-testing",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Benchmark testing",
    "text": "Benchmark testing\n\nRun the same large job array on both OSG and Hera\n\nBuild Apptainer container to replicate an identical software environment on both OSG and Hera\n\nMake sure we can run SS3 and R with non-default packages"
  },
  {
    "objectID": "assets/presentation.html#benchmark-testing-1",
    "href": "assets/presentation.html#benchmark-testing-1",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Benchmark testing",
    "text": "Benchmark testing\n\n\nUsing a baseline SS3 file, run 500 alternative models with different fixed parameter values of natural mortality and steepness (uses SS3 and R; r4ss)\n\n\n\n\nUse the delta-MVLN approach to generate uncertainty in predictions so that models can be combined in an ensemble (uses R; ss3diags)\n\n\n\n\n\nRun 5 retrospective peels (\\(t-1\\) to \\(t-5\\)) for each of the 500 models and calculate Mohn’s \\(\\rho\\)\n\nNote: Retrospective peels were treated as separate jobs for the purpose of the benchmark testing giving 3000 unique jobs.\n\n\n\n\n\n\nStock status plots from the model ensemble: A spawning biomass relative to the spawning biomass at MSY, and B fishing mortality relative to the fishing mortality at MSY. The median is showed in the solid line, darker band shows the 50th percentile and lighter band the 80th percentile.\n\n\n\n\n\n\nMohn’s \\(\\rho\\) for alternative parameter combinations of natural mortality (M) and steepness (h). The solid black lines denote the original M (vertical line) and steepness (horizontal line)."
  },
  {
    "objectID": "assets/presentation.html#benchmark-testing---osg",
    "href": "assets/presentation.html#benchmark-testing---osg",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Benchmark testing - OSG",
    "text": "Benchmark testing - OSG\n\n\nAll 3000 jobs hit the queue at the same time from a single array job submission and began executing almost immediately\n\n\nSmall fraction of jobs had bad file transfer and had to be relaunched\n\n\n\nTotal computation time was \\(\\sim\\) 25 days, but elapsed time was \\(\\sim\\) 3.5 hours (166 \\(\\times\\) faster)\n\n\n\n\nStart and stop time for jobs run on OpenScienceGrid (OSG)."
  },
  {
    "objectID": "assets/presentation.html#benchmark-testing---hera-as-htc",
    "href": "assets/presentation.html#benchmark-testing---hera-as-htc",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Benchmark testing - Hera (as HTC)",
    "text": "Benchmark testing - Hera (as HTC)\n\n\nQueue and job_array_id limits required multiple (staggered) job submissions\n\n\n\n\nLarge proportion of jobs suffered from resource competition during memory/disk intensive portion of SS3 calculations (SD calcs) and produced incomplete outputs.\n\n\n\nNote SS3 does not appear to crash/trigger an error when it runs out of memory/disk but instead writes out available output which could appear complete.\n\n\n\n\n\nStart and stop time for jobs run on NOAA Hera."
  },
  {
    "objectID": "assets/presentation.html#benchmark-testing---hera-parallel",
    "href": "assets/presentation.html#benchmark-testing---hera-parallel",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Benchmark testing - Hera (parallel)",
    "text": "Benchmark testing - Hera (parallel)\n\n\nThe Hera workflow was re-configured to run parallel jobs within 15 compute nodes using the gnu parallel utility1.\n\n\n\n\n\nAll jobs hit the queue at the same time and executed as resources became available within each node.\n\nEach node had 200 jobs allocated to it spread out over 40 CPUs resulting in the \\(\\sim\\) 5 distinct waves of job execution.\n\n\n\n\n\n\nTotal computation time for 3000 jobs was \\(\\sim\\) 25.5 days, but elapsed time was \\(\\sim\\) 1.75 hours\n\n\n\n\n\n2998 of 2999 (99.99 %) completed models produced identical output to OSG\n\n\n\n\nStart and stop time for jobs run on NOAA Hera using gnu parallel utility.\n\nNote this is how the earlier SS3 example was formulated."
  },
  {
    "objectID": "assets/presentation.html#benchmark-testing---summary",
    "href": "assets/presentation.html#benchmark-testing---summary",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Benchmark testing - Summary",
    "text": "Benchmark testing - Summary\n\n\nWe have working proof-of-concept and workflows for both OSG and Hera that can result in substantial time savings\n\n\n\nAnalyses are portable between computing solutions with minimal modifications to workflows\n\n\n\nOSG and Hera have different strengths, weaknesses, and constraints so analysts can choose which may be better suited for different tasks\n\n\n\nIs this ‘time-savings’ worth it?"
  },
  {
    "objectID": "assets/presentation.html#getting-started",
    "href": "assets/presentation.html#getting-started",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Getting started!",
    "text": "Getting started!\n\n\nOpenScienceGrid (OSG)\n\n\nReach out to OSPool staff and apply for access.\nRun some models!\n\n\nNOAA Hera\n\n\nApply for an RDHPCS account at the Account Information Management (AIM) website (CAC login required).\nRequest access to a RDHPCS project. First time users can request access to the htc4sa project to test out Hera before requesting their own project allocation.\nRun some models!"
  },
  {
    "objectID": "assets/presentation.html#closing-thoughts",
    "href": "assets/presentation.html#closing-thoughts",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Closing thoughts",
    "text": "Closing thoughts\n\n\n\nLimitations and bottle-necks\nWhat have we not discussed?\nWhat about the cloud?\nIntegrating with Open Science workflows"
  },
  {
    "objectID": "assets/presentation.html#acknowledgements",
    "href": "assets/presentation.html#acknowledgements",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThank you to Howard Townsend for helping get the RDHPCS htc4sa project off the ground.\n\nThank you also to Help Desk staff at both OSG OSPool and NOAA RDHPCS for helping troubleshoot and refine job array workflows using HTCondor and Slurm, respectively.\n\nOSG workflow development and benchmark testing was conducted using services provided by the OSG Consortium (OSG 2006, 2015; Pordes et al. 2007; Sfiligoi et al. 2009), which is supported by the National Science Foundation awards #2030508 and #1836650.\n\nSlide design influenced by Emil Hvitfeldt’s published examples."
  },
  {
    "objectID": "assets/presentation.html#contact-us",
    "href": "assets/presentation.html#contact-us",
    "title": "HTC/HPC for Stock Assessments",
    "section": "Contact us",
    "text": "Contact us\n\n\n\nNicholas Ducharme-Barth\nnicholas.ducharme-barth at noaa.gov\n\n\nMegumi Oshima\nmegumi.oshima at noaa.gov"
  },
  {
    "objectID": "assets/presentation.html#references",
    "href": "assets/presentation.html#references",
    "title": "HTC/HPC for Stock Assessments",
    "section": "References",
    "text": "References\n\n\nOSG. 2006. “OSPool.” OSG. https://doi.org/10.21231/906P-4D78.\n\n\n———. 2015. “Open Science Data Federation.” OSG. https://doi.org/10.21231/0KVZ-VE57.\n\n\nPordes, Ruth, Don Petravick, Bill Kramer, Doug Olson, Miron Livny, Alain Roy, Paul Avery, et al. 2007. “The Open Science Grid.” In J. Phys. Conf. Ser., 78:012057. 78th Series. https://doi.org/10.1088/1742-6596/78/1/012057.\n\n\nSfiligoi, Igor, Daniel C Bradley, Burt Holzman, Parag Mhashilkar, Sanjay Padhi, and Frank Wurthwein. 2009. “The Pilot Way to Grid Resources Using glideinWMS.” In 2009 WRI World Congress on Computer Science and Information Engineering, 2:428–32. 2nd Series. https://doi.org/10.1109/CSIE.2009.950."
  },
  {
    "objectID": "assets/osg_documentation.html",
    "href": "assets/osg_documentation.html",
    "title": "Working with Open Science Grid",
    "section": "",
    "text": "The Open Science Grid is an example of an HTC computing environment where jobs are run and scheduled using HTCondor. OSG is free to use for US-based government staff conducting research or education related work, and HTCondor is well suited for running many short, non-sequential, single core jobs. Helpful documentation can be found on their website and the following information is based on this documentation.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your OSG user name.\nReplace osg.project_name your OSG project name.\nReplace apXX to match your OSG access point (e.g. ap20 or ap21).",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#open-science-grid",
    "href": "assets/osg_documentation.html#open-science-grid",
    "title": "Working with Open Science Grid",
    "section": "",
    "text": "The Open Science Grid is an example of an HTC computing environment where jobs are run and scheduled using HTCondor. OSG is free to use for US-based government staff conducting research or education related work, and HTCondor is well suited for running many short, non-sequential, single core jobs. Helpful documentation can be found on their website and the following information is based on this documentation.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your OSG user name.\nReplace osg.project_name your OSG project name.\nReplace apXX to match your OSG access point (e.g. ap20 or ap21).",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#connecting-to-osg-vis-ssh",
    "href": "assets/osg_documentation.html#connecting-to-osg-vis-ssh",
    "title": "Working with Open Science Grid",
    "section": "2 Connecting to OSG vis ssh",
    "text": "2 Connecting to OSG vis ssh\nAccess to OSG and file transfer is done using a pair of Terminal/PowerShell windows. In your first powershell terminal, log in to your access point. You will be prompted to enter your passphrase (password). It will then ask for a verification code generated by the Google Authenticator app.\n\nssh User.Name@apXX.uc.osg-htc.org\n\nIf you see the following, you have connected successfully:\n\n\n                *** Unauthorized use is prohibited. ***\n\n      If you log on to this computer system, you acknowledge your\n  awareness of and concurrence with the OSG Acceptable Use Policy; see\n             https://www.osgconnect.net/aup or /etc/osg/AUP\n\n                              ***\n\n              Cite the OSPool in your publications!\n                https://osg-htc.org/acknowledging\n\n                              ***\n\n               OSPool Office Hours are twice a week:\n    Tues 4-5:30pm ET/1-2:30pm PT, Thurs 11:30am-1pm ET/8:30-10am PT\n             Zoom link: https://osg-htc.org/OfficeHoursZoom\n\n***********************************************************************",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#transferring-files-via-scp",
    "href": "assets/osg_documentation.html#transferring-files-via-scp",
    "title": "Working with Open Science Grid",
    "section": "3 Transferring files via scp",
    "text": "3 Transferring files via scp\n\n3.1 Move a file to OSG\nIf you want to transfer a file (for example, my_file.txt) from your local machine to OSG, navigate to the directory of my_file.txt on your local computer. Open a terminal and use scp and specify the source file name and destination path as shown:\n\n\n## format is scp &lt;source&gt; &lt;destination&gt; \nscp my_file.txt User_Name@apXX.uc.osg-htc.org:/home/User_Name/\n\nNote, you do not need to login to OSG first, you will be prompted to enter your passphrase and then a verification code before the file transfers. And each time you transfer a file you will need to re-enter the information.\n\n\n3.2 Move multiple files at once\nTo transfer many files more efficiently, you can compress your files into tarballs (.tar.gz file). For example, to upload all of the input files for running an array of linear model jobs at once, in R, run:\n\nsystem(paste0(\"powershell cd \",file.path(\"examples\", \"OSG\", \"array_lm\"),\";tar -czf inputs.tar.gz inputs\"))\n\nto create a tarball called inputs.tar.gz. To do this in a terminal, open a terminal window in the directory above the files to compress (e.g. examples/OSG/array_lm) and run:\n\ntar -czf inputs.tar.gz inputs\n\n\n\n3.3 Move files back to local machine\nTo transfer files from OSG back to your local machine you can use the same command as above but swap the source and destination paths. From the same terminal as before (or in the directory to put the file), use:\n\n\nscp User_Name@apXX.uc.osg-htc.org:/home/User_Name/my_file.txt ./\n\nwhere ./ is the current location on your local computer or you can specify the path relative to where you are (e.g. ./path/to/files).",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#transferring-large-files-to-osdf",
    "href": "assets/osg_documentation.html#transferring-large-files-to-osdf",
    "title": "Working with Open Science Grid",
    "section": "4 Transferring large files to OSDF",
    "text": "4 Transferring large files to OSDF\nFor large input and output files, including containers, it is recommended to use the Open Science Data Federation (OSDF). To see exactly which OSDF origins to use see this guidance on Where to Put Your Files. If you are working with a container, you can upload it and transfer it using the following commands.\nFrom a terminal in the directory where the container file (linux-r4ss-v4.sif) is stored on your local computer, run:\n\nscp linux-r4ss-v4.sif User.Name@apXX.uc.osg-htc.org:/home/User.Name\n\nAgain you will be prompted for your passphrase and RSA code each time. Then, in a second terminal that is logged into OSG, move the container to the OSDF location.\n\nmv linux-r4ss-v4.sif /ospool/apXX/data/User.Name\n\nTo test the container, in that same OSG terminal, run:\n\n\napptainer shell /ospool/apXX/data/User.Name/linux-r4ss-v4.sif\n\n#once it opens you can try running R and making sure the packages you need are there\nR\nlibrary(r4ss)\nq()",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/web-about-mo.html",
    "href": "assets/web-about-mo.html",
    "title": "Megumi Oshima",
    "section": "",
    "text": "Megumi Oshima has been working at the Pacific Islands Fisheries Science Center since 2021. She works mostly on domestic and territorial bottomfish stocks and is interested in Openscience and creating reproducible and transparent workflows. Before joining PIFSC, she was a graduate student at University of Southern Mississippi where she got her PhD in Coastal Sciences.\n\n\n Back to top",
    "crumbs": [
      "Contact us",
      "Megumi Oshima"
    ]
  },
  {
    "objectID": "assets/web-slides.html",
    "href": "assets/web-slides.html",
    "title": "Seminar slides",
    "section": "",
    "text": "Seminar slides from the NOAA National Stock Assessment Science Seminar Series presentation.\n    View slides in full screen\n       \n      \n    \n  \nA recording of the seminar can be found on the NOAA Library YouTube channel (here).\n\n\n\n Back to top",
    "crumbs": [
      "Seminar slides"
    ]
  },
  {
    "objectID": "assets/ss3-osg.html",
    "href": "assets/ss3-osg.html",
    "title": "Running an array of SS3 jobs on OSG",
    "section": "",
    "text": "Similar to the array_lm example, this example also sets up running an array job on OSG. As before, we will use a *.txt to indicate which directories we want to run jobs in as a part of our array.\nThere are a few main differences that serve to illustrate useful modifications to the workflow:\nThe OSG/ss3 example can be set-up either by cloning the repository git clone https://github.com/MOshima-PIFSC/NSASS-HTC-HPC-Computing.git, or stepping through the following code:",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on OSG"
    ]
  },
  {
    "objectID": "assets/ss3-osg.html#build-software-container",
    "href": "assets/ss3-osg.html#build-software-container",
    "title": "Running an array of SS3 jobs on OSG",
    "section": "1 Build software container",
    "text": "1 Build software container\nSoftware containers allow for portable, reproducible research by allowing researchers to set-up a software environment to their exact spefications and being able to run it on any Linux system. The Apptainer container system is widely used across HPC/HTC systems, and make it easy to build a container from a definition file. Running a job within a container means that you are able to replicate an identical software environment in any location with Apptainer installed, no matter the native operating system, software and installed packages. The Apptainer container can be built from any Linux machine with Apptainer installed, including the Open Science Grid (OSG) access points. Here we walk through the steps needed to build a Linux (Ubuntu 20.04) container containing Stock Synthesis (version 3.30.22.1), R (version 4.4.0) and the R packages r4ss, ss3diags, data.table, magrittr, and mvtnorm from a definition file, linux-r4ss-v4.def. In this case we will show the steps needed to build the container using the OSG access point as our Linux virtual machine (VM), though this may not be needed if working from an alternative Linux VM.\n\n\n\n\n\n\nCoding alert!\n\n\n\nNote: you will have to change apXX to match your OSG access point (e.g., ap20 or ap21).\n\n\nThe first step is to log onto your OSG access point via ssh using a Terminal/PowerShell window and make a directory to build your container in this case singularity1.\n\nssh User.Name@apXX.uc.osg-htc.org\nmkdir -p singularity/linux_r4ss\n\nUsing a second Terminal/PowerShell window, navigate to the directory that you cloned the NSASS-HTC-HPC-Computing repo into and upload the definition file (linux-r4ss-v4.def) to the directory you just created on OSG.\n\nscp apptainer/linux-r4ss-v4.def User.Name@apXX.uc.osg-htc.org:/home/User.Name/singularity/linux_r4ss\n\nBack in your first Terminal/PowerShell window manoeuvre into the directory, and build the container2. The second line of code is what builds the Singularity Image File (.sif) and takes two arguments: the name of the output .sif file and the input definition file (.def).\n\ncd singularity/linux_r4ss\napptainer build linux-r4ss-v4.sif linux-r4ss-v4.def\n\nMove the built .sif file to Open Science Data Federation (OSDF) so that it can be cached and made available to any availble HTCondor jobs. Note that if you update the build of the .sif file in any way you should also rename it (e.g., -v5) as caching may not update the file if it has the same name.\n\nmkdir -p /ospool/apXX/data/User.Name/singularity\ncp linux-r4ss-v4.sif /ospool/apXX/data/User.Name/singularity",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on OSG"
    ]
  },
  {
    "objectID": "assets/ss3-osg.html#setup-data-inputs-and-directories",
    "href": "assets/ss3-osg.html#setup-data-inputs-and-directories",
    "title": "Running an array of SS3 jobs on OSG",
    "section": "2 Setup data inputs and directories",
    "text": "2 Setup data inputs and directories\nGiven that our example is to run a 4-year retrospective analysis for each of the SS3 test models, the next step is downloading the SS3 test models from the nmfs-stock-synthesis/test-models Github repo. Once you’ve downloaded the test models, copy the models/ directory into a new example directory ss3/inputs/ within the NSASS-HTC-HPC-Computing/examples/OSG/ directory on your machine. If you cloned the NSASS-HTC-HPC-Computing repo, the SS3 test models will already be in the correct location.\nFor the sake of example the job array will be set-up to run each retrospective peel (e.g., -0 years, -1 year, … , -4 years of data) as individual jobs in the job array. We will store the results of each retrospective peel in its own directory. The directories on OSG will be listed in a text file, and we will use this text file to launch jobs on OSG (as a part of the job array) in each of the named directories.\nLet us define that text file using R.\n\nDefine a relative path, we are starting from the root directory of this project.\n\n\n\nShow code used to define relative paths.\nproj_dir = this.path::this.proj()\n\n\n\nWrite a text file containing the path names for where the directories will be on OSG relative to the examples/OSG/ss3/ folder.\n\n\n\nShow code used to define job directory structure.\ntest_models=list.dirs(paste0(proj_dir,\"/examples/OSG/ss3/inputs/models/\"),recursive=FALSE,full.names=FALSE)\nretro_peels=0:4\n\n# replace '-' with '_' in model names since we will use '-' as a delimiter\n    if(length(grep(\"-\",test_models,fixed=TRUE))&gt;0){\n        test_models_new = gsub(\"-\",\"_\",test_models)\n        rename_models_idx = grep(\"-\",test_models,fixed=TRUE)\n        for(i in seq_along(rename_models_idx)){\n            # create new dir\n            dir.create(paste0(proj_dir,\"/examples/OSG/ss3/inputs/models/\",test_models_new[rename_models_idx[i]]),recursive=TRUE)\n            # copy files\n            file.copy(paste0(proj_dir,\"/examples/OSG/ss3/inputs/models/\",test_models[rename_models_idx[i]],\"/\",list.files(paste0(proj_dir,\"/examples/OSG/ss3/inputs/models/\",test_models[rename_models_idx[i]]),full.names=FALSE,recursive=FALSE)),paste0(proj_dir,\"/examples/OSG/ss3/inputs/models/\",test_models_new[rename_models_idx[i]]))\n            # delete old dir\n            shell(paste0(\"powershell rm -r \",proj_dir,\"/examples/OSG/ss3/inputs/models/\",test_models[rename_models_idx[i]],\"/\"))\n        }\n        test_models = test_models_new\n    }\n    \n\n# define scenarios\nscenario_df = expand.grid(model=test_models,peel=retro_peels)\nscenario_df$run_id = 1:nrow(scenario_df)\nscenario_df = scenario_df[,c(3,1,2)]\nscenario_df$run_id = ifelse(scenario_df$run_id&lt;10,paste0(0,scenario_df$run_id),as.character(scenario_df$run_id))\n\n# write text file\n# define paths relative to the examples/OSG/ss3/ folder\nosg_dir_lines = paste0(\"output/\", apply(scenario_df,1,paste0,collapse=\"-\"), \"/\")\nwriteLines(osg_dir_lines, con=paste0(proj_dir, \"/examples/OSG/ss3/scripts/osg_job_directories.txt\"))",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on OSG"
    ]
  },
  {
    "objectID": "assets/ss3-osg.html#prepare-job-scripts",
    "href": "assets/ss3-osg.html#prepare-job-scripts",
    "title": "Running an array of SS3 jobs on OSG",
    "section": "3 Prepare job scripts",
    "text": "3 Prepare job scripts\nIn order to execute the HTC workflow, instructions are coordinated using four scripts:\n\nosg-prep.sh: This script prepares files for HTCondor job execution, and makes the directory structure specified by osg_job_directories.txt.\nosg.sub: This is the HTCondor job submission script that specifies job requirements and input/output files.\nosg-wrapper-r.sh: This wrapper script controls file input/output to and from the R script osg-ss3-example-calcs.r, executes the R script, conducts job timing and tidies up the job working directory.\nosg-ss3-example-calcs.r: This is the actual computation script which modifies the SS3 input files as needed, executes the appropriate SS3 model run and conducts any needed post-processing of the output within R.\n\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn osg.sub you will need to change the following before you upload and run the script:\n\nLine 15: change access point apXX and User.Name to match your user name and access point.\nLine 27: change project_name to match your project on OSG.\n\n\n\n\nFrom within R, compress the OSG/ss3/inputs/ and OSG/ss3/scripts/ directories as a tar.gz file upload.example-ss3.tar.gz. This simplifies the number of steps needed for file transfers.\n\n\nshell(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"OSG\", \"ss3\",\"inputs\"), \";tar -czf upload.example-ss3-models.tar.gz models/\"))\nshell(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"OSG\", \"ss3\"), \";tar -czf upload.example-ss3-scripts.tar.gz scripts/\"))\nshell(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"OSG\", \"ss3\"), \";tar -czf upload.example-ss3.tar.gz inputs/upload.example-ss3-models.tar.gz upload.example-ss3-scripts.tar.gz\"))",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on OSG"
    ]
  },
  {
    "objectID": "assets/ss3-osg.html#osg-workflow",
    "href": "assets/ss3-osg.html#osg-workflow",
    "title": "Running an array of SS3 jobs on OSG",
    "section": "4 OSG workflow",
    "text": "4 OSG workflow\n\nConnect to OSG\n\nAs mentioned, access to OSG and file transfer is done using a pair of Terminal/PowerShell windows, we will call them Terminal A and Terminal B. In Terminal A, log onto your access point and create a directory for this example.\n\nssh User.Name@ap21.uc.osg-htc.org\nmkdir -p examples/OSG/ss3/\n\n\nTransfer files\n\nOpen a second PowerShell terminal in the NSASS-HTC-HPC-Computing directory on your machine. This will be your local workstation, call it Terminal B. Use this terminal window to upload via scp the needed files (examples/OSG/ss3/upload.example-ss3.tar.gz) to OSG. The upload.example-ss3.tar.gz will be uploaded to the directory you just created examples/ss3.\n\nscp examples/OSG/ss3/upload.example-ss3.tar.gz User.Name@apXX.uc.osg-htc.org:/home/User.Name/examples/OSG/ss3/\n\n\nPrepare files and submit job on Hera\n\nIn Terminal A, un-tar files, change the permissions/line endings for prep scripts, execute the script and launch the condor job.\n\ncd examples/OSG/ss3/\ntar -xzf upload.example-ss3.tar.gz\ntar -xzf upload.example-ss3-scripts.tar.gz\nchmod 777 scripts/osg-prep.sh\ndos2unix scripts/osg-prep.sh\n./scripts/osg-prep.sh\ncondor_submit scripts/osg.sub\n\n\nDownload jobs\n\nOnce all jobs are completed (or the job has hit its time limit), use your Terminal B to download your jobs.\n\nscp -r User.Name@apXX.uc.osg-htc.org:/home/User.Name/examples/OSG/ss3/output examples/OSG/ss3/",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on OSG"
    ]
  },
  {
    "objectID": "assets/ss3-osg.html#process-results",
    "href": "assets/ss3-osg.html#process-results",
    "title": "Running an array of SS3 jobs on OSG",
    "section": "5 Process results",
    "text": "5 Process results\nAfter results are downloaded they can be processed in R to extract the model run times, time series of estimated biomass for each model run, and Mohn’s rho across retrospective peels for a given model ‘family’.\n\n\nShow output processing code\n# iterate over output files and extract quantities\nlibrary(data.table)\nlibrary(magrittr)\nlibrary(r4ss)\n\noutput_dirs = list.dirs(paste0(proj_dir,\"/examples/OSG/ss3/output/\"),recursive=FALSE,full.names=FALSE)\nssb_dt.list = comptime_dt.list = as.list(rep(NA,length(output_dirs)))\nss_output_list =  as.list(rep(NA,length(output_dirs)))\nnames(ss_output_list) = output_dirs\n\nfor(i in seq_along(output_dirs)){\n    tmp_model = strsplit(output_dirs[i],\"-\")[[1]][2]\n    tmp_peel = as.numeric(strsplit(output_dirs[i],\"-\")[[1]][3])\n    tmp_index = as.numeric(strsplit(output_dirs[i],\"-\")[[1]][1])\n\n    # check if the End.tar.gz file got created\n    if(file.exists(paste0(proj_dir,\"/examples/OSG/ss3/output/\",output_dirs[i],\"/End.tar.gz\")))\n    {\n        # get snapshot of original files in the directory\n        tmp_orig_files = list.files(paste0(proj_dir,\"/examples/OSG/ss3/output/\",output_dirs[i],\"/\"))\n\n        # un-tar if the End.tar.gz file gets made\n        shell(paste0(\"powershell cd \", paste0(proj_dir,\"/examples/OSG/ss3/output/\",output_dirs[i],\"/\"), \";tar -xzf End.tar.gz\"))\n\n        # check if runtime.txt was produced and extract output\n        if(file.exists(paste0(proj_dir,\"/examples/OSG/ss3/output/\",output_dirs[i],\"/runtime.txt\"))){\n            tmp_time = readLines(paste0(proj_dir,\"/examples/OSG/ss3/output/\",output_dirs[i],\"/runtime.txt\")) %&gt;%\n            gsub(\".*?([0-9]+).*\", \"\\\\1\", .) %&gt;%\n            as.numeric(.) %&gt;%\n            as.data.table(.) %&gt;%\n            setnames(.,\".\",\"time\")\n            comptime_dt.list[[i]] = data.table(id = output_dirs[i])    \n            comptime_dt.list[[i]]$index = tmp_index\n            comptime_dt.list[[i]]$model = tmp_model\n            comptime_dt.list[[i]]$peel = tmp_peel\n            comptime_dt.list[[i]]$OSG_start = as.POSIXct(tmp_time$time[1],origin=\"1970-01-01\")\n            comptime_dt.list[[i]]$OSG_end = as.POSIXct(tmp_time$time[2],origin=\"1970-01-01\")\n            comptime_dt.list[[i]]$OSG_runtime = tmp_time$time[3]/60\n\n            # clean-up\n            rm(list=c(\"tmp_time\"))\n        }\n\n        # if \"ss_report.RData\" is produced put it into the storage list\n        if(file.exists(paste0(proj_dir,\"/examples/OSG/ss3/output/\",output_dirs[i],\"/ss_report.RData\"))){\n            load(paste0(proj_dir,\"/examples/OSG/ss3/output/\",output_dirs[i],\"/ss_report.RData\"))\n            ss_output_list[[i]] = ss_report\n\n            ssb_dt.list[[i]] = ss_report$derived_quants %&gt;%\n                as.data.table(.) %&gt;%\n                .[Label %in% paste0(\"SSB_\", ss_report$startyr:ss_report$endyr)] %&gt;%\n                .[,id := output_dirs[i]] %&gt;%\n                .[,sbo:=Value/subset(ss_report$derived_quants,Label==\"SSB_Virgin\")$Value] %&gt;%\n                .[,yr:=sapply(Label,function(x)as.numeric(strsplit(x,\"_\")[[1]][2]))] %&gt;%\n                .[,.(id,yr,sbo)]\n            # clean-up\n                rm(list=c(\"ss_report\"))\n        }    \n\n        # clean-up\n        file.remove(paste0(proj_dir,\"/examples/OSG/ss3/output/\",output_dirs[i],\"/\",setdiff(list.files(paste0(proj_dir,\"/examples/OSG/ss3/output/\",output_dirs[i],\"/\")),tmp_orig_files)))\n        rm(list=c(\"tmp_orig_files\"))\n    } else {\n        comptime_dt.list[[i]] = data.table(id=output_dirs[i],index=tmp_index,model=tmp_model,peel=tmp_peel,OSG_start=NA,OSG_end=NA,OSG_runtime=NA)\n        ssb_dt.list[[i]] = data.table(id=output_dirs[i],yr=2023,sbo=NA)\n    }\n\n    # clean-up\n    rm(list=c(\"tmp_model\",\"tmp_peel\",\"tmp_index\"))\n}\n\ncomptime_dt = rbindlist(na.omit(comptime_dt.list))\nssb_dt = rbindlist(ssb_dt.list) %&gt;% merge(comptime_dt[,.(id,index,model,peel)],.,by=\"id\")\nss_output_list = na.omit(ss_output_list)\n\n# save\nfwrite(comptime_dt,file=paste0(proj_dir,\"/examples/OSG/ss3/output/comptime_dt.csv\"))\nfwrite(ssb_dt,file=paste0(proj_dir,\"/examples/OSG/ss3/output/ssb_dt.csv\"))\n\n# calculate Mohn's rho\nunique_models = unique(comptime_dt$model)\nretro_dt.list = as.list(rep(NA,length(unique_models)))\n\nfor(i in seq_along(unique_models)){\n    tmp_model = unique_models[i]\n\n    retro_dt.list[[i]] = data.table(model=tmp_model)\n    retro_dt.list[[i]]$type = c(\"SBO\")\n    retro_dt.list[[i]]$rho = NA\n\n    if(uniqueN(na.omit(ssb_dt[model==tmp_model])$peel)==5){\n        tmp_dt = ssb_dt[model==tmp_model]\n        base_dt = tmp_dt[peel==0]\n        year_vec = max(base_dt$yr) - 1:4\n        bias_vec = rep(NA,length(year_vec))\n        # calc Mohn's rho for runs where all models completed\n        for(j in 1:4){\n            bias_vec[j] = (ssb_dt[model==tmp_model&peel==j&yr==year_vec[j]]$sbo - base_dt[yr==year_vec[j]]$sbo)/base_dt[yr==year_vec[j]]$sbo\n        }\n        retro_dt.list[[i]]$rho = mean(bias_vec)\n        rm(list=c(\"tmp_dt\",\"base_dt\",\"year_vec\",\"bias_vec\"))\n    } \n    \n    rm(list=c(\"tmp_model\"))\n}\n\nretro_dt = rbindlist(retro_dt.list)\nfwrite(retro_dt,file=paste0(proj_dir,\"/examples/OSG/ss3/output/retro_dt.csv\"))\n\n\n\n5.1 Job runtime\nThe 90 jobs run on OSG completed 4.14 hours of calculations (2.76 minutes per job) in an elapsed time of \\(\\sim\\) 34 minutes (Figure 1).\n\n\nShow plotting code\nlibrary(ggplot2)\n\np = comptime_dt %&gt;%\n.[,.(id,OSG_start,OSG_end)] %&gt;%\nmelt(.,id.vars=\"id\") %&gt;%\n.[,variable:=ifelse(variable%in%c(\"OSG_start\"),\"start\",\"end\")] %&gt;%\ndcast(.,id~variable) %&gt;%\n.[order(start)] %&gt;%\nggplot() +\nxlab(\"Time (GMT)\") +\nylab(\"Job\") +\ngeom_segment(aes(x=start,xend=end,y=id,yend=id),color=\"#003087\",alpha=0.5,linewidth=2) +\ntheme(panel.background = element_rect(fill = \"transparent\", color = \"black\", linetype = \"solid\"),\n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),  \n            strip.background =element_rect(fill=\"transparent\"),\n            legend.key = element_rect(fill = \"transparent\"),\n            axis.text.y=element_blank(),\n            axis.ticks.y=element_blank())\np\n\n# save plot\nggsave(\n  \"OSG-ss3-elapsed.png\",\n  plot = p,\n  device = \"png\",\n  path = paste0(proj_dir,\"/assets/static/\"),\n  width = 8,\n  height = 4.5,\n  units = c(\"in\"),\n  dpi = 300,\n  bg = \"transparent\")\n\n\n\n\n\n\n\n\nFigure 1: Start and stop time for jobs run on OSG.",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on OSG"
    ]
  },
  {
    "objectID": "assets/ss3-osg.html#example-results",
    "href": "assets/ss3-osg.html#example-results",
    "title": "Running an array of SS3 jobs on OSG",
    "section": "6 Example results",
    "text": "6 Example results\n\n6.1 Retrospectives\nRetrospective plots of static biomass depletion for the SS3 test models are shown in Figure 2.\n\n\nShow plotting code\ntext_dt.list = as.list(rep(NA,uniqueN(ssb_dt$model)))\nfor(i in seq_along(text_dt.list)){\n    tmp_dt = ssb_dt[model==unique(ssb_dt$model)[i]]\n    tmp_min_yr = min(tmp_dt$yr)\n    text_dt.list[[i]] = data.table(model=unique(ssb_dt$model)[i],yr=tmp_min_yr,sbo=0.2,rho=round(retro_dt[model==unique(ssb_dt$model)[i]]$rho,digits=2))\n}\ntext_dt = rbindlist(text_dt.list)\n\n\np = ssb_dt %&gt;%\n            ggplot() +\n            facet_wrap(~model,scales=\"free_x\") +\n            xlab(\"Year\") +\n            ylab(expression(SB/SB[0])) +\n            ylim(0,NA) +\n            geom_hline(yintercept=0) +\n            geom_path(aes(x=yr,y=sbo,color=as.character(peel),group=id)) +\n            geom_text(data=text_dt,aes(x=yr,y=sbo,label=rho),size=3,hjust = 0) +\n            viridis::scale_color_viridis(\"Peel\",begin = 0.1,end = 0.8,direction = 1,option = \"H\",discrete=TRUE) +\n            viridis::scale_fill_viridis(\"Peel\",begin = 0.1,end = 0.8,direction = 1,option = \"H\",discrete=TRUE) +\n            theme(panel.background = element_rect(fill = \"transparent\", color = \"black\", linetype = \"solid\"),\n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),\n            strip.background =element_rect(fill=\"transparent\"),\n            legend.key = element_rect(fill = \"transparent\"))\np\n\n# save plot\nggsave(\n  \"OSG-ss3-retro.png\",\n  plot = p,\n  device = \"png\",\n  path = paste0(proj_dir,\"/assets/static/\"),\n  width = 8,\n  height = 4.5,\n  units = c(\"in\"),\n  dpi = 300,\n  bg = \"transparent\")\n\n\n\n\n\n\n\n\nFigure 2: Depletion estimates across retrospective peels from the Stock Synthesis testing model suite examples. Mohn’s rho values are printed in each panel.",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on OSG"
    ]
  },
  {
    "objectID": "assets/ss3-osg.html#footnotes",
    "href": "assets/ss3-osg.html#footnotes",
    "title": "Running an array of SS3 jobs on OSG",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis directory can be named anything that you like, in this case singularity is a legacy name from an earlier version of the code written before Singularity changed its name to Apptainer.↩︎\nThis may take ~10-15 minutes depending on how long it takes to install R packages.↩︎",
    "crumbs": [
      "Documentation",
      "Running an array of SS3 jobs on OSG"
    ]
  }
]