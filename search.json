[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This repository contains examples and documentation for accessing and applying existing research computing resources available to NOAA Fisheries staff. Example code and documentation were presented as a part of the National Stock Assessment Seminar series (slides).\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "assets/web-slides.html",
    "href": "assets/web-slides.html",
    "title": "Seminar slides",
    "section": "",
    "text": "Seminar slides from the NOAA National Stock Assessment Science Seminar Series presentation.\n    View slides in full screen\n       \n      \n    \n  \n\n\n\n Back to top",
    "crumbs": [
      "Seminar slides"
    ]
  },
  {
    "objectID": "assets/web-about-mo.html",
    "href": "assets/web-about-mo.html",
    "title": "Megumi Oshima",
    "section": "",
    "text": "Megumi Oshima has been working at the Pacific Islands Fisheries Science Center since 2021. She works mostly on domestic and territorial bottomfish stocks and is interested in Openscience and creating reproducible and transparent workflows. Before joining PIFSC, she was a graduate student at University of Southern Mississippi where she got her PhD in Coastal Sciences.\n\n\n Back to top",
    "crumbs": [
      "Contact us",
      "Megumi Oshima"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html",
    "href": "assets/osg_documentation.html",
    "title": "Working with Open Science Grid",
    "section": "",
    "text": "The Open Science Grid is an example of an HTC computing environment where jobs are run and scheduled using HTCondor. OSG is free to use for US-based government staff conducting research or education related work, and HTCondor is well suited for running many short, non-sequential, single core jobs. Helpful documentation can be found on their website and the following information is based on this documentation.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your OSG user name.\nReplace osg.project_name your OSG project name.",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#open-science-grid",
    "href": "assets/osg_documentation.html#open-science-grid",
    "title": "Working with Open Science Grid",
    "section": "",
    "text": "The Open Science Grid is an example of an HTC computing environment where jobs are run and scheduled using HTCondor. OSG is free to use for US-based government staff conducting research or education related work, and HTCondor is well suited for running many short, non-sequential, single core jobs. Helpful documentation can be found on their website and the following information is based on this documentation.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your OSG user name.\nReplace osg.project_name your OSG project name.",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#connecting-to-osg-vis-ssh",
    "href": "assets/osg_documentation.html#connecting-to-osg-vis-ssh",
    "title": "Working with Open Science Grid",
    "section": "2 Connecting to OSG vis ssh",
    "text": "2 Connecting to OSG vis ssh\nAccess to OSG and file transfer is done using a pair of Terminal/PowerShell windows. In your first powershell terminal, log onto your access point. You will be prompted to enter your passphrase (password). It will then ask for a verification code generated by the Google Authenticator app.\n\nssh User.Name@ap21.uc.osg-htc.org\n\nIf you see the following, you have connected successfully:\n\n\n                *** Unauthorized use is prohibited. ***\n\n      If you log on to this computer system, you acknowledge your\n  awareness of and concurrence with the OSG Acceptable Use Policy; see\n             https://www.osgconnect.net/aup or /etc/osg/AUP\n\n                              ***\n\n              Cite the OSPool in your publications!\n                https://osg-htc.org/acknowledging\n\n                              ***\n\n               OSPool Office Hours are twice a week:\n    Tues 4-5:30pm ET/1-2:30pm PT, Thurs 11:30am-1pm ET/8:30-10am PT\n             Zoom link: https://osg-htc.org/OfficeHoursZoom\n\n***********************************************************************",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#transferring-files-via-scp",
    "href": "assets/osg_documentation.html#transferring-files-via-scp",
    "title": "Working with Open Science Grid",
    "section": "3 Transferring files via scp",
    "text": "3 Transferring files via scp\n\n3.1 Move a file to OSG\nIf you want to transfer a file (for example, my_file.txt) from your local machine to OSG, navigate to the directory of my_file.txt on your local computer. Open a terminal and use scp and specify the source file name and destination path as shown:\n\n\n## format is scp &lt;source&gt; &lt;destination&gt; \nscp my_file.txt User_Name@ap21.uc.osg-htc.org:/home/User_Name/\n\nNote, you do not need to login to OSG first, you will be prompted to enter your passphrase and then a verification code before the file transfers. And each time you transfer a file you will need to re-enter the information.\n\n\n3.2 Move multiple files at once\nTo transfer many files more efficiently, you can compress your files into tarballs (.tar.gz file). For example, to upload all of the input files for running an array of linear model jobs at once, in R, run:\n\nsystem(paste0(\"powershell cd \",file.path(\"examples\", \"osg\", \"array_lm\"),\";tar -czf inputs.tar.gz inputs\"))\n\nto create a tarball called inputs.tar.gz. To do this in a terminal, open a terminal window in the directory above the files to compress (e.g. examples/osg/array_lm) and run:\n\ntar -czf inputs.tar.gz inputs\n\n\n\n3.3 Move files back to local machine\nTo transfer files from OSG back to your local machine you can use the same command as above but swaping the source and destination paths. From the same terminal as before (or in the directory to put the file), use:\n\n\nscp User_Name@ap21.uc.osg-htc.org:/home/User_Name/my_file.txt ./\n\nwhere ./ is the current location on your local computer or you can specify the path relative to where you are (e.g. ./path/to/files).",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/osg_documentation.html#transferring-large-files-to-osdf",
    "href": "assets/osg_documentation.html#transferring-large-files-to-osdf",
    "title": "Working with Open Science Grid",
    "section": "4 Transferring large files to OSDF",
    "text": "4 Transferring large files to OSDF\nFor large input and output files, including containers, it is recommended to use the Open Science Data Federation (OSDF). To see exactly which OSDF origins to use see this guidance on Where to Put Your Files. For this example, we will use the access point of ap21.uc.osg-htc.org. If you are working with a container, you can upload it to your files and transfer it using the following commands.\nFrom a terminal in the directory where the container files (linux.def and linux.sif) are stored on your local computer, run:\n\nscp linux.def User.Name@ap21.uc.osg-htc.org:/home/User.Name\nscp linux.sif User.Name@ap21.uc.osg-htc.org:/home/User.Name\n\nAgain you will be prompted for your passphrase and RSA code each time. Then, in a second terminal that is logged into OSG, move the container to the OSDF location.\n\nmv linux.sif /ospool/ap21/data/User.Name\n\nTo test the container, in that same OSG terminal, run:\n\n\napptainer shell /ospool/ap21/data/User.Name/linux.sif\n\n#once it opens you can try running R and making sure the packages you need are there\nR\nlibrary(r4ss)\nq()",
    "crumbs": [
      "Documentation",
      "Working with Open Science Grid"
    ]
  },
  {
    "objectID": "assets/presentation.html#what",
    "href": "assets/presentation.html#what",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "What?",
    "text": "What?\n\n\n\nResearch computing is the collection of computing, software, storage resources and services that allows for data analysis at scale.\n\n\n\n\nIn our particular case we are interested in leverging research computing to augment stock assessment worflows.\n\n\n\nRun more/bigger models in less time"
  },
  {
    "objectID": "assets/presentation.html#why",
    "href": "assets/presentation.html#why",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Why?",
    "text": "Why?\n\n\n\n\n\nImprove efficiency by running 10s - 1000s of models ‘simultaneously’.\n\n\n\n\n  \n\n2021 Southwest Pacific Ocean swordfish stock assessment\n\n\n\n\n9,300 model runs totalling ~46 months of computation time."
  },
  {
    "objectID": "assets/presentation.html#why-1",
    "href": "assets/presentation.html#why-1",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Why?",
    "text": "Why?\n\n\n\nEfficiency\n\n\n\n\n\n\nKnowledge acquisition\n\n\n\n\n\n\nAutomation, transparency, reproducibility & portability\n\n\n\n\n\n\nMulti-model inference\n\n\n\n\n\n\nSoftware containers\n\n\n\n\n\n\n\n\nBetter science"
  },
  {
    "objectID": "assets/presentation.html#how-1",
    "href": "assets/presentation.html#how-1",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "How?",
    "text": "How?\n\n\nHigh-throughput computing (HTC)\n\nHigh-performance computing (HPC)\n\n \n\nPhoto credit: NOAA\n\n\nOpenScienceGrid (OSG): OSPool\n\n\nNOAA Hera"
  },
  {
    "objectID": "assets/presentation.html#how-2",
    "href": "assets/presentation.html#how-2",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "How?",
    "text": "How?\n\n\nOpenScienceGrid (OSG)\n\n\n\nUses HTCondor distributed computing network (no shared file system between compute nodes) to implement HTC workflows\n\n\n\n\nFree to use for US based researchers affiliated with academic/government organization and using OSG for research/education efforts\n\n\n\n\nShould not be used to analyze protected data\n\n\n\n\nNOAA Hera\n\n\n\nUses Slurm to schedule HPC (or HTC) workflows\n\n\n\n\nShared file system between compute nodes\n\n\n\n\nNOAA resource so no restrictions on acceptable use/analyzing protected data if working on mission related tasks\n\n\n\n\nAllocation determines access\n\n\n\n\n\n\nBoth use software containers"
  },
  {
    "objectID": "assets/presentation.html#software-containers",
    "href": "assets/presentation.html#software-containers",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Software containers",
    "text": "Software containers\n\n\nMany may already be using containers such as GitHub Codespaces or Posit Workbench in existing cloud-based workflows\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplication: set up identical, custom software environments on OSG and Hera\n\n\n\n\nApplication: use to “version” analyses by “freezing” packages/libraries"
  },
  {
    "objectID": "assets/presentation.html#software-containers-1",
    "href": "assets/presentation.html#software-containers-1",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Software containers",
    "text": "Software containers\n\n\nApptainer\n\n\nSecure, portable and reproducible software container for Linux operating systems\n\n\n\n\nEasy to use\n\n\n\n\nDoesn’t require root privileges to build\n\n\n\n\nPlays nice with existing containers (e.g., Docker)"
  },
  {
    "objectID": "assets/presentation.html#apptainer",
    "href": "assets/presentation.html#apptainer",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Apptainer",
    "text": "Apptainer\nLet’s look at an example (linux-r4ss-v4.def):\n\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    TZ=Etc/UTC && \\\n    ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && \\\n    echo $TZ &gt; /etc/timezone\n    apt update -y\n    apt install -y \\\n        tzdata \\\n        curl \\\n        dos2unix\n\n    apt-get update -y\n    apt-get install -y \\\n            build-essential \\\n            cmake \\\n            g++ \\\n            libssl-dev \\\n            libssh2-1-dev \\\n            libcurl4-openssl-dev \\\n            libfontconfig1-dev \\\n            libxml2-dev \\\n            libgit2-dev \\\n            wget \\\n            tar \\\n            coreutils \\\n            gzip \\\n            findutils \\\n            sed \\\n            gdebi-core \\\n            locales \\\n            nano\n    \n    locale-gen en_US.UTF-8\n\n    export R_VERSION=4.4.0\n    curl -O https://cdn.rstudio.com/r/ubuntu-2004/pkgs/r-${R_VERSION}_1_amd64.deb\n    gdebi -n r-${R_VERSION}_1_amd64.deb\n\n    ln -s /opt/R/${R_VERSION}/bin/R /usr/local/bin/R\n    ln -s /opt/R/${R_VERSION}/bin/Rscript /usr/local/bin/Rscript\n\n    R -e \"install.packages('remotes', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"install.packages('data.table', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"install.packages('magrittr', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"install.packages('mvtnorm', dependencies=TRUE, repos='http://cran.rstudio.com/')\"\n    R -e \"remotes::install_github('r4ss/r4ss')\"\n    R -e \"remotes::install_github('PIFSCstockassessments/ss3diags')\"\n\n    NOW=`date`\n    echo 'export build_date=$NOW' &gt;&gt; $SINGULARITY_ENVIRONMENT\n\n    mkdir -p /ss_exe\n    curl -L -o /ss_exe/ss3_linux https://github.com/nmfs-ost/ss3-source-code/releases/download/v3.30.22.1/ss3_linux\n    chmod 755 /ss_exe/ss3_linux\n\n%environment\n    export PATH=/ss_exe:$PATH\n    \n%labels\n    Author nicholas.ducharme-barth@noaa.gov\n    Version v0.0.4\n\n%help\n    This is a Linux (Ubuntu 20.04) container containing Stock Synthesis (version 3.30.22.1), R (version 4.4.0) and the R packages r4ss, ss3diags, data.table, magrittr, and mvtnorm."
  },
  {
    "objectID": "assets/presentation.html#apptainer-1",
    "href": "assets/presentation.html#apptainer-1",
    "title": "Operationalizing available research computing resources for stock assessment",
    "section": "Apptainer",
    "text": "Apptainer\nLet’s look at an example (linux-r4ss-v4.def):\nBuild on Linux system with Apptainer installed.\n\napptainer build linux-r4ss-v4.sif linux-r4ss-v4.def\n\n\n\n\n\n\nNational Stock Assessment Science Seminar"
  },
  {
    "objectID": "assets/hera_documentation.html",
    "href": "assets/hera_documentation.html",
    "title": "Working with NOAA HPC Hera",
    "section": "",
    "text": "Hera is one of NOAA’s high performance computing resources available to NMFS scientists where jobs are run and scheduled with SLURM. In order to access the system, you must first have a RDHPCS user account and request access to a project. For all of the examples in this documentation, we will be working in the htc4sa project. For complete documentation about Hera and other RDHPCS resources, see the NOAA RDHPCS website.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your RDHPCS user name.\nReplace bastion with either boulder or princeton depending on which bastion you wish to login in through.\nReplace project_name with your RDHPCS project name.",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html#hera",
    "href": "assets/hera_documentation.html#hera",
    "title": "Working with NOAA HPC Hera",
    "section": "",
    "text": "Hera is one of NOAA’s high performance computing resources available to NMFS scientists where jobs are run and scheduled with SLURM. In order to access the system, you must first have a RDHPCS user account and request access to a project. For all of the examples in this documentation, we will be working in the htc4sa project. For complete documentation about Hera and other RDHPCS resources, see the NOAA RDHPCS website.\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn the following code you will need to, where necessary:\n\nReplace User.Name with your RDHPCS user name.\nReplace bastion with either boulder or princeton depending on which bastion you wish to login in through.\nReplace project_name with your RDHPCS project name.",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html#connecting-to-hera-via-ssh",
    "href": "assets/hera_documentation.html#connecting-to-hera-via-ssh",
    "title": "Working with NOAA HPC Hera",
    "section": "2 Connecting to Hera via ssh",
    "text": "2 Connecting to Hera via ssh\nOpen a terminal window (.e.g, command prompt or PowerShell), then open an ssh tunnel to Hera by using the following command:\n\nssh -m hmac-sha2-256-etm@openssh.com User.Name@hera-rsa.bastion.rdhpcs.noaa.gov -p22\n\nNote that the flag -p22 specifies opening port 22 and is optional. When prompted, put in your password followed by the RSA authentication code:\n\nXXXXXXXXRSACODE\n\nIf you see the following then you have connected successfully:\n\n________________________________________________________\n|                                                        |\n|                                                        |\n|  Welcome to the Hera High Performance Computing system |\n|                                                        |\n|        This system is located in Fairmont, WV          |\n|                                                        |\n|          Please Submit Helpdesk Requests to:           |\n|               rdhpcs.hera.help@noaa.gov                |\n|                                                        |\n|________________________________________________________|\n\nNote that you will also see the following terminal output upon connecting:\n\nLocal port XXXXX forwarded to remote host.\nRemote port YYYYY forwarded to local host.\n\nWhere XXXXX and YYYYY are your 4-5 digit port forwarding numbers. Make note of what your local port number (XXXXX) is since this will be used for scp file transfer using an ssh tunnel.",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/hera_documentation.html#transferring-files-tofrom-hera",
    "href": "assets/hera_documentation.html#transferring-files-tofrom-hera",
    "title": "Working with NOAA HPC Hera",
    "section": "3 Transferring files to/from Hera",
    "text": "3 Transferring files to/from Hera\n\n3.1 via scp using data transfer node (DTN)\nInformation in this section is largely based on this wiki (CAC login required). File transfer using a DTN is only possible from machines within the noaa.gov domain (VPN ok), and can only transfer files to the scratch directory. However, this is ok since it is recommended that input/output data files are stored in the scratch directory. Using a DTN is faster than using the ssh tunnel.\n\n\n\n\n\n\nNote\n\n\n\nThe scratch1/NMFS/project_name/ directory is a shared space (shared access and shared disk space) for all users of a project. Rather than dumping files into the root project directory (e.g., project_name/) it is better to place them in your own sub-directory. Since this sub-directory structure does not already exist you will need to log into Hera and create it:\n\nmkdir scratch1/NMFS/project_name/User.Name/\n\nThis will avoid over-writing other users’ work.\n\n\nFile transfer takes place via scp and you need to specify the source file (and path to the file if your terminal is not in that directory) and destination path as shown:\n\n## format is scp &lt;source&gt; &lt;destination&gt; \nscp /path/to/local/file User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/\n\nIf trying to transfer from a machine not within the NOAA domain (or on the VPN) you can use scp with an untrusted DTN:\n\nscp /path/to/local/file User.Name@udtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/data_untrusted/User.Name/\n\nNote that dtn-hera was changed to udtn-hera. Since files cannot stay in the data_untrusted directory we will need to log in to Hera and move it to the correct project directory on scratch. Once logged in, moving the files can be done with rsync (CAC login required), and then they can be deleted from data_untrusted by:\n\nrsync -axv /scratch1/data_untrusted/User.Name/file /scratch1/NMFS/project_name/User.Name/\nrm /scratch1/data_untrusted/User.Name/file\n\nFiles can be downloaded back to your local machine using scp from either the trusted (dtn) or untrusted (udtn) DTN:\n\nscp User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/file /path/to/local/file \n\n\n\n3.2 via scp using an ssh tunnel\nInformation in this section is largely based on this wiki (CAC login required). File transfer using an ssh tunnel is possible from all locations. Using the ssh tunnel for file transfer requires a two-step process with two active terminal windows (we suggest using PowerShell):\n\nIn the first terminal window, open an ssh connection to Hera with port forwarding:\n\n\nssh -m hmac-sha2-256-etm@openssh.com -LXXXXX:localhost:XXXXX User.Name@hera-rsa.bastion.rdhpcs.noaa.gov\n\nReplace XXXXX with your local port number.\n\nIn the second terminal window you can check to see if the tunnel was properly created:\n\n\nssh -p XXXXX User.Name@localhost\n\nif you get prompted for your password then success! Press CONTROL+C to ignore this prompt. Staying within this 2nd terminal window use scp to transfer your file:\n\nscp -P XXXXX /path/to/local/file User.Name@localhost:/home/User.Name/\n\nThis will copy your file from your local machine into your home directory on Hera. Simply append additional directory structure to /home/User.Name/ to copy it into a sub-directory that you have created on Hera (e.g., /home/User.Name/sub/dir/). To transfer files into your scratch directory, specify the proper destination path (e.g., /scratch1/NMFS/project_name/User.Name/).\nTo download a file from Hera using scp and the ssh tunnel use the following:\n\nscp -P XXXXX User.Name@localhost:/home/User.Name/file_name /path/to/local/file",
    "crumbs": [
      "Documentation",
      "Working with NOAA HPC Hera"
    ]
  },
  {
    "objectID": "assets/array_osg.html",
    "href": "assets/array_osg.html",
    "title": "Submitting an array job with OSG",
    "section": "",
    "text": "In this example we step through submitting an array job on OSG where we want to run the same job in a number of directories. In this case the job is running a simple R script that reads in the data.csv file stored in the directory, fits a linear model, and writes the paramter estimates to a par.csv. We specify which directories we want to run jobs in as a part of the job-array using a text file to specify the directory path names on OSG.\nThe osg/array_lm example can be set-up either by cloning the repository git clone https://github.com/MOshima-PIFSC/NSASS-HTC-HPC-Computing.git, or stepping through the following code:",
    "crumbs": [
      "Documentation",
      "Submitting an array job with OSG"
    ]
  },
  {
    "objectID": "assets/array_osg.html#setup-data-inputs-and-directories",
    "href": "assets/array_osg.html#setup-data-inputs-and-directories",
    "title": "Submitting an array job with OSG",
    "section": "1 Setup data inputs and directories",
    "text": "1 Setup data inputs and directories\n\nDefine a relative path, we are starting from the root directory of this project.\n\n\nproj_dir = this.path::this.proj()\nosg_project = \"\" ##TDOD Do we need this? \n\n\nDefine directory names for each run.\n\n\ndir_name = paste0(\"rep_0\", 0:9)\n\n\nIterate across directories, create them, and then write a simple .csv file into them containing data to fit a linear model.\n\n\nfor(i in seq_along(dir_name)){\n    \n    if(!file.exists(file.path(proj_dir, \"example\", \"OSG\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\")))\n    {\n        set.seed(i)\n        dir.create(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", dir_name[i]), recursive=TRUE)\n        tmp = data.frame(x=1:1000)\n        tmp$y = (i + (0.5*i)*tmp$x) + rnorm(1000,0,i)\n        write.csv(tmp, file = file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\"))\n    }\n}\n\n\nWrite an R script to read in the data, run a linear model, and report back the estimated parameters.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"run_lm_osg_array.r\"))){\n    script_lines = c(\"tmp=read.csv('data.csv')\", \n    \"fit=lm(y~x,data=tmp)\", \n    \"out = data.frame(par=unname(fit$coefficients))\", \n    \"write.csv(out,file='par.csv')\"\n    )\n    writeLines(script_lines, con = file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"run_lm_osg_array.r\"))\n}\n\n\nWrite a text file containing the full path names for where the directories will be on OSG.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"osg_job_directories.txt\"))){\n    dir_lines = paste0(\"array_osg/\", dir_name, \"/\")\n    writeLines(dir_lines, con = file.path(proj_dir, \"examples\", \"OSG\", \"array_lm\", \"inputs\", \"osg_job_directories.txt\"))\n}\n\n\nIn addtion to the input files, you will need to have 2 additional scripts: a wrapper script (wrapper.sh) and a submission script (submission.sub). To easily upload all the necessary files at once, compress the entire array_lm/ directory as a tar.gz file upload.array_lm.tar.gz.\n\n\nshell(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"OSG\"), \";tar -czf upload.array_lm.tar.gz array_lm \"))",
    "crumbs": [
      "Documentation",
      "Submitting an array job with OSG"
    ]
  },
  {
    "objectID": "assets/array_osg.html#osg-workflow",
    "href": "assets/array_osg.html#osg-workflow",
    "title": "Submitting an array job with OSG",
    "section": "2 OSG workflow",
    "text": "2 OSG workflow\nAs before, access to OSG and file transfer is done using a pair of Terminal/PowerShell windows. In your first window, log onto your access point and create directory for this example.\n\nssh nicholas.ducharmebarth@ap21.uc.osg-htc.org\nmkdir r_and_ss\n\nIn the second window, upload the files contained in example/r_and_ss/osg/ into the OSG directory that you just created. These files include:\n\nthe base SS files to be modified (as a compressed file) Start.tar.gz;\nthe text file osg_dir.txt that was created in the above R code chunk;\nthe R script r_and_ss.r for the job (manipulating SS input files, running SS, and bootstrapping the results to generate uncertainty around management quantities);\nthe HTCondor submit script r_and_ss.sub;\nthe bash wrapper script osg_wrapper_r_and_ss.sh executing the condor job (unpacks files, sets up job timing, executes R script, and packages results);\nand a bash script osg_prep.sh preparing the before-listed files for being run on HTCondor (change file permissions, make directory structure, and change dos2unix line endings).\n\n\nscp -r example/r_and_ss/osg/* nicholas.ducharmebarth@ap21.uc.osg-htc.org:/home/nicholas.ducharmebarth/r_and_ss\n\nIn the first window, change the permissions and line endings for osg_prep.sh. Since the OSG access point does not have the dos2unix utility installed we can quickly jump into the Singularity container that we created to access dos2unix, change the line endings and execute the script. Use condor_submit to submit your HTCondor job.",
    "crumbs": [
      "Documentation",
      "Submitting an array job with OSG"
    ]
  },
  {
    "objectID": "assets/web-about-nd.html",
    "href": "assets/web-about-nd.html",
    "title": "Nicholas Ducharme-Barth",
    "section": "",
    "text": "Nicholas Ducharme-Barth joined the Pacific Islands Fisheries Science Center in 2021. Previously, Nicholas worked at the Pacific Community (SPC) conducting pelagic stock assessments for the Western and Central Pacific Fisheries Commission (WCPFC). He received his B.S. in Mathematics from the College of William & Mary and his Ph. D. in Fisheries and Aquatic Sciences from the University of Florida.\n\n\n Back to top",
    "crumbs": [
      "Contact us",
      "Nicholas Ducharme-Barth"
    ]
  },
  {
    "objectID": "assets/array_hera.html",
    "href": "assets/array_hera.html",
    "title": "Submitting an array job with Hera",
    "section": "",
    "text": "In this example we step through submitting an array job on Hera where we want to run the same job in a number of directories. In this case the job is running a simple R script that reads in the data.csv file stored in the directory, fits a linear model, and writes the paramter estimates to a par.csv. We specify which directories we want to run jobs in as a part of the job-array using a text file to specify the directory path names on Hera.\nThe hera/array_lm example can be set-up either by cloning the repository git clone https://github.com/MOshima-PIFSC/NSASS-HTC-HPC-Computing.git, or stepping through the following code:",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  },
  {
    "objectID": "assets/array_hera.html#setup-data-inputs-and-directories",
    "href": "assets/array_hera.html#setup-data-inputs-and-directories",
    "title": "Submitting an array job with Hera",
    "section": "1 Setup data inputs and directories",
    "text": "1 Setup data inputs and directories\n\nDefine a relative path, we are starting from the root directory of this project.\n\n\nproj_dir = this.path::this.proj()\nhera_project = \"NMFS/project_name/User.Name/\"\n\n\nDefine directory names for each run.\n\n\ndir_name = paste0(\"rep_0\", 0:9)\n\n\nIterate across directories, create them, and then write a simple .csv file into them containing data to fit a linear model.\n\n\nfor(i in seq_along(dir_name)){\n    \n    if(!file.exists(file.path(proj_dir, \"example\", \"hera\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\")))\n    {\n        set.seed(i)\n        dir.create(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", dir_name[i]), recursive=TRUE)\n        tmp = data.frame(x=1:1000)\n        tmp$y = (i + (0.5*i)*tmp$x) + rnorm(1000,0,i)\n        write.csv(tmp, file = file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", dir_name[i], \"data.csv\"))\n    }\n}\n\n\nWrite an R script to read in the data, run a linear model, and report back the estimated parameters.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"run_lm_hera_array.r\"))){\n    script_lines = c(\"tmp=read.csv('data.csv')\", \n    \"fit=lm(y~x,data=tmp)\", \n    \"out = data.frame(par=unname(fit$coefficients))\", \n    \"write.csv(out,file='par.csv')\"\n    )\n    writeLines(script_lines, con = file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"run_lm_hera_array.r\"))\n}\n\n\nWrite a text file containing the full path names for where the directories will be on Hera.\n\n\nif(!file.exists(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"hera_job_directories.txt\"))){\n    dir_lines = paste0(\"/scratch1/\", hera_project, \"array_hera/\", dir_name, \"/\")\n    writeLines(dir_lines, con = file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", \"hera_job_directories.txt\"))\n}\n\n\nCompress the entire array_lm/ directory as a tar.gz file upload.array_lm.tar.gz. This simplifies the number of steps needed for file transfers.\n\n\nshell(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"hera\"), \";tar -czf upload.array_lm.tar.gz array_lm \"))",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  },
  {
    "objectID": "assets/array_hera.html#hera-workflow",
    "href": "assets/array_hera.html#hera-workflow",
    "title": "Submitting an array job with Hera",
    "section": "2 Hera workflow",
    "text": "2 Hera workflow\n\nConnect to Hera\n\nOpen a PowerShell terminal and connect to Hera. This terminal will be your remote workstation, call it Terminal A. You will be prompted for your RSA passcode, which is your password followed by the 8-digit code from the authenticator app.\n\nssh -m hmac-sha2-256-etm@openssh.com User.Name@hera-rsa.boulder.rdhpcs.noaa.gov -p22\n\n\nCreate directories\n\nIn Terminal A navigate to the project directory on scratch1 and create some directories. If using a shared directory such as htc4sa/, make a directory to save your work within this directory (.e.g., User.Name/). Change your working directory to this directory. We will upload our SLURM submit script into submit_scripts/ and write our SLURM log files to logs/.\n\n# navigate to project directory\ncd /scratch1/NMFS/project_name/\n# create new directory\nmkdir User.Name/\n# navigate into new directory\ncd User.Name/\n# create directory for SLURM scripts and logs\nmkdir submit_scripts/\nmkdir logs/\n\n\nTransfer files\n\nOpen a second PowerShell terminal in the NSASS-HTC-HPC-Computing directory on your machine. This will be your local workstation, call it Terminal B. Use this terminal window to upload via scp the needed files (examples/hera/upload.array_lm.tar.gz and examples/hera/array_lm/slurm_scripts/submit_array_lm.sh) to Hera. The upload.array_lm.tar.gz will be uploaded to your directory within the project directory on scratch1 and the submit script submit_array_lm.sh will be uploaded to the submit_scripts directory. Make sure your VPN is active when attempting to upload using the DTN. You will be prompted for your RSA passcode after each scp command.\n\nscp examples/hera/upload.array_lm.tar.gz User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name\n\n\n\n\n\n\n\nTroubleshooting Tip\n\n\n\nIf you are getting an error Corrupted MAC on input when uploading files, add -o MACs=hmac-sha2-512 between scp and the name of the file to upload.\n\n\n\n\n\n\n\n\nCoding alert!\n\n\n\nIn submit_array_lm.sh you will need to change the following before you upload and run the script:\n\nLine 5: change account project_name to the name of your project. If you are using the NOAA htc4sa project, it would be htc4sa.\nLine 9: /scratch1/NMFS/project_name/User.Name/logs/ to the location you created for logs/ above.\nLine 10: Change to your email address.\nLine 24: Make sure that this line points to the location on Hera that you uploaded hera_job_directories.txt to. hera_job_directories.txt is located in the array_lm/inputs directory that was uploaded as a part of upload.array_lm.tar.gz.\nLine 37: Make sure that this line points to the location on Hera that you uploaded run_lm_hera_array.r to. run_lm_hera_array.r is located in the array_lm/inputs directory that was uploaded as a part of upload.array_lm.tar.gz.\n\n\n\n\nscp examples/hera/array_lm/slurm_scripts/submit_array_lm.sh User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/submit_scripts/\n\n\nUn-tar files\n\nBack in Terminal A untar the uploaded directory.\n\ntar -xzf upload.array_lm.tar.gz\n\n\nPrep files to be read on Hera\n\nMake sure files that will be read/executed have unix line endings:\n\ndos2unix inputs/run_lm_hera_array.r inputs/hera_job_directories.txt slurm_scripts/submit_array_lm.sh\n\n\nSubmit job\n\nNow you are ready to submit the SLURM submission script submit_array_lm.sh.\n\nsbatch slurm_scripts/submit_array_lm.sh\n\nAs part of this job script, it cleans the directories specified in hera_job_directories.txt of the input file data.csv. The output is a compressed tar.gz containing the output produced by run_lm_hera_array.r (par.csv) and runtime.txt which logs the job start, end, and runtime. You can check your job status using squeue but this job should complete in a few seconds.\n\nsqueue -u User.Name\n\n\nDownload results\n\nBefore sending back the results you need to compress array_lm.\n\ntar -czf download.array_lm.tar.gz array_lm\n\nMoving back to Terminal B you can download the results, but first you create a directory for it to be downloaded into. This can be done in R:\n\ndir.create(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\"), recursive=TRUE, showWarnings = FALSE)\n\nNow you can use scp in Terminal B to download download.array_lm.tar.gz into examples/hera/array_lm/output\n\nscp User.Name@dtn-hera.fairmont.rdhpcs.noaa.gov:/scratch1/NMFS/project_name/User.Name/download.array_lm.tar.gz examples/hera/array_lm/output/\n\nMove into examples/hera/array_lm/output/ and untar downloaded results.\n\n# navigate to output directory\ncd examples/hera/array_lm/output/\n# untar results\ntar -xzf download.array_lm.tar.gz",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  },
  {
    "objectID": "assets/array_hera.html#process-the-output",
    "href": "assets/array_hera.html#process-the-output",
    "title": "Submitting an array job with Hera",
    "section": "3 Process the output",
    "text": "3 Process the output\nIn R, iterate through the sub-directories of the input and output data to extract the results of the linear model fits, and the model run time information.\n\n\nShow code\nlibrary(data.table)\nlibrary(magrittr)\n\ninput_data.list = as.list(rep(NA,10))\noutput_data.list = as.list(rep(NA,10))\nruntime_data.list = as.list(rep(NA,10))\n\n##TODO: change paths here to match directory names\nfor(i in seq_along(dir_name)){\n    # get input data\n        input_data.list[[i]] = fread(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"inputs\", dir_name[i],\"data.csv\")) %&gt;%\n            .[,.(x,y)] %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            .[,.(model,x,y)]\n    \n    # untar results\n        system(paste0(\"powershell cd \", file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\", dir_name[i],\"/\"), \";tar -xzf output.tar.gz\"))\n\n    # get output\n        output_data.list[[i]] = fread(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\", dir_name[i],\"par.csv\")) %&gt;%\n            .[,.(par)] %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            .[,.(model,par)] %&gt;%\n            melt(.,id.vars=\"model\") %&gt;%\n            .[,variable:=c(\"intercept\",\"slope\")] %&gt;%\n            dcast(.,model ~ variable) %&gt;%\n            merge(.,input_data.list[[i]][,.(model,x)],by=\"model\") %&gt;%\n            .[,pred_y := intercept+slope*x] %&gt;%\n            .[,.(model,x,pred_y)]\n    # get time\n        runtime_data.list[[i]] = readLines(file.path(proj_dir, \"examples\", \"hera\", \"array_lm\", \"output\", dir_name[i],\"runtime.txt\")) %&gt;%\n            gsub(\".*?([0-9]+).*\", \"\\\\1\", .) %&gt;%\n            as.numeric(.) %&gt;%\n            as.data.table(.) %&gt;%\n            setnames(.,\".\",\"time\") %&gt;%\n            .[,model := factor(as.character(i),levels=as.character(1:10))] %&gt;%\n            melt(.,id.vars=\"model\") %&gt;%\n            .[,variable:=c(\"start\",\"end\",\"runtime\")] %&gt;%\n            dcast(.,model ~ variable) %&gt;%\n            .[,.(model,start,end,runtime)]\n}\n\ninput_data = rbindlist(input_data.list)\noutput_data = rbindlist(output_data.list)\nruntime_data = rbindlist(runtime_data.list)\n\n\nThe jobs started execution at 2023-05-31 00:49:56 and all finished by 2023-05-31 00:49:57 for an elapsed runtime of 1 seconds and a total computation time of 5 seconds. Use of Hera resulted in a job completing 5\\(\\times\\) faster. Figure 1 shows the simulated data and estimated linear fits for each model run in the job-array.\n\n\nShow code\nlibrary(ggplot2)\ninput_data %&gt;%\nggplot() +\ngeom_point(aes(x=x,y=y,fill=model),alpha=0.05,size=5,shape=21) +\ngeom_line(data=output_data,aes(x=x,y=pred_y,color=model),linewidth=2)\n\n\n\n\n\n\n\n\nFigure 1: Linear model fits from the 10 models run.",
    "crumbs": [
      "Documentation",
      "Submitting an array job with Hera"
    ]
  }
]